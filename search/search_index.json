{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcdc DocETL: A System for Complex Document Processing","text":"<p>DocETL is a tool for creating and executing LLM-powered data processing pipelines. It offers a low-code, declarative YAML interface to define complex data operations on complex data.</p> <p>When to Use DocETL</p> <p>DocETL is the ideal choice when you're looking to maximize correctness and output quality for complex tasks over a collection of documents or unstructured datasets. You should consider using DocETL if:</p> <ul> <li>You have complex tasks that you want to represent via map-reduce (e.g., map over your documents, then group by the result of your map call &amp; reduce)</li> <li>You're unsure how to best write your pipeline or sequence of operations to maximize LLM accuracy</li> <li>You're working with long documents that don't fit into a single prompt or are too lengthy for effective LLM reasoning</li> <li>You have validation criteria and want tasks to automatically retry when the validation fails</li> </ul>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Rich Suite of Operators: Tailored for complex data processing, including specialized operators like \"resolve\" for entity resolution and \"gather\" for maintaining context when splitting documents.</li> <li>Low-Code Interface: Define your pipeline and prompts easily using YAML. You have 100% control over the prompts.</li> <li>Flexible Processing: Handle various document types and processing tasks across domains like law, medicine, and social sciences.</li> <li>Accuracy Optimization: Our optimizer leverages LLM agents to experiment with different logically-equivalent rewrites of your pipeline and automatically selects the most accurate version. This includes finding limits of how many documents to process in a single reduce operation before the accuracy plateaus.</li> </ul>"},{"location":"#getting-started","title":"\u26a1 Getting Started","text":"<p>To get started with DocETL:</p> <ol> <li>Install the package (see installation for detailed instructions)</li> <li>Define your pipeline in a YAML file</li> <li>Run your pipeline using the DocETL command-line interface</li> </ol>"},{"location":"#project-origin","title":"\ud83c\udfdb\ufe0f Project Origin","text":"<p>DocETL was created by members of the EPIC Data Lab and Data Systems and Foundations group at UC Berkeley. The EPIC (Effective Programming, Interaction, and Computation with Data) Lab focuses on developing low-code and no-code interfaces for data work, powered by next-generation predictive programming techniques. DocETL is one of the projects that emerged from our research efforts to streamline complex document processing tasks.</p> <p>For more information about the labs and other projects, visit the EPIC Lab webpage and the Data Systems and Foundations webpage.</p>"},{"location":"best-practices/","title":"Best Practices for DocETL","text":"<p>This guide outlines best practices for using DocETL effectively, focusing on the most important aspects of pipeline creation, execution, and optimization.</p> <p>Supported Models</p> <p>DocETL supports many models through LiteLLM:</p> <ul> <li>OpenAI models (e.g., GPT-4, GPT-3.5-turbo)</li> <li>Anthropic models (e.g., Claude 2, Claude Instant)</li> <li>Google VertexAI models (e.g., chat-bison, text-bison)</li> <li>Cohere models</li> <li>Replicate models</li> <li>Azure OpenAI models</li> <li>Hugging Face models</li> <li>AWS Bedrock models (e.g., Claude, AI21, Cohere)</li> <li>Gemini models (e.g., gemini-1.5-pro)</li> <li>Ollama models (e.g., llama2)</li> </ul> <p>For a complete and up-to-date list of supported models, please refer to the LiteLLM documentation. You can use the model name just like the litellm documentation (e.g., <code>openai/gpt-4o-mini</code> or <code>gemini/gemini-1.5-flash-002</code>).</p> <p>While DocETL supports various models, it has been primarily tested with OpenAI's language models. Using OpenAI is currently recommended for the best experience and most reliable results, especially for operations that depend on structured outputs. We have also tried gemini-1.5-flash-002 and found it to be pretty good for a much cheaper price.</p>"},{"location":"best-practices/#pipeline-design","title":"Pipeline Design","text":"<ol> <li>Start Simple: Begin with a basic pipeline and gradually add complexity as needed.</li> </ol> <p>Example: Start with a simple extraction operation before adding resolution and summarization.</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract and list all medications mentioned in the transcript:\n      {{ input.src }}\n</code></pre> <ol> <li>Modular Design: Break down complex tasks into smaller, manageable operations.</li> </ol> <p>Example: The medical transcripts pipeline in the tutorial demonstrates this by separating medication extraction, resolution, and summarization into distinct operations.</p> <ol> <li>Optimize Incrementally: Optimize one operation at a time to ensure stability and verify improvements.</li> </ol> <p>Example: After implementing the basic pipeline, you might optimize the <code>extract_medications</code> operation first:</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    optimize: true\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract and list all medications mentioned in the transcript:\n      {{ input.src }}\n</code></pre>"},{"location":"best-practices/#schema-and-prompt-design","title":"Schema and Prompt Design","text":"<ol> <li>Keep Schemas Simple: Use simple output schemas whenever possible. Complex nested structures can be difficult for LLMs to produce consistently.</li> </ol> <p>Good Example (Simple Schema):</p> <pre><code>output:\n  schema:\n    medication: list[str] # Note that this is different from the example in the tutorial.\n</code></pre> <p>Avoid (Complex Nested Structure):</p> <pre><code>output:\n  schema:\n    medications: \"list[{name: str, dosage: {amount: float, unit: str, frequency: str}}]\"\n</code></pre> <ol> <li>Clear and Concise Prompts: Write clear, concise prompts for LLM operations, providing relevant context from input data. Instruct quantities (e.g., 2-3 insights, one summary) to guide the LLM.</li> </ol> <p>Example: The <code>summarize_prescriptions</code> operation in the tutorial demonstrates a clear prompt with specific instructions:</p> <pre><code>prompt: |\n  Here are some transcripts of conversations between a doctor and a patient:\n\n  {% for value in inputs %}\n  Transcript {{ loop.index }}:\n  {{ value.src }}\n  {% endfor %}\n\n  For the medication {{ reduce_key }}, please provide the following information based on all the transcripts above:\n\n  1. Side Effects: Summarize all mentioned side effects of {{ reduce_key }}. List 2-3 main side effects.\n  2. Therapeutic Uses: Explain the medical conditions or symptoms for which {{ reduce_key }} was prescribed or recommended. Provide 1-2 primary uses.\n\n  Ensure your summary:\n  - Is based solely on information from the provided transcripts\n  - Focuses only on {{ reduce_key }}, not other medications\n  - Includes relevant details from all transcripts\n  - Is clear and concise\n  - Includes quotes from the transcripts\n</code></pre> <ol> <li>Take advantage of Jinja Templating: Use Jinja templating to dynamically generate prompts and provide context to the LLM. Feel free to use if statements, loops, and other Jinja features to customize prompts.</li> </ol> <p>Example: Using Jinja conditionals and loops in a prompt (note that age is a made-up field for this example):</p> <pre><code>prompt: |\n  Analyze the following medical transcript:\n  {{ input.src }}\n\n  {% if input.patient_age %}\n  Note that the patient is {{ input.patient_age }} years old.\n  {% endif %}\n\n  Please extract the following information:\n  {% for item in [\"medications\", \"symptoms\", \"diagnoses\"] %}\n  - List all {{ item }} mentioned in the transcript\n  {% endfor %}\n</code></pre> <ol> <li>Validate Outputs: Use the <code>validate</code> field to ensure the quality and correctness of processed data. This consists of Python statements that validate the output and optionally retry the LLM if one or more statements fail. To learn more about validation, see the validation documentation.</li> </ol> <p>Example: Adding validation to the <code>extract_medications</code> operation:</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract and list all medications mentioned in the transcript:\n      {{ input.src }}\n    validate: |\n      len(output.medication) &gt; 0\n      all(isinstance(med, str) for med in output.medication)\n      all(len(med) &gt; 1 for med in output.medication)\n</code></pre>"},{"location":"best-practices/#handling-large-documents-and-entity-resolution","title":"Handling Large Documents and Entity Resolution","text":"<ol> <li> <p>Chunk Large Inputs: For documents exceeding token limits, consider using the optimizer to automatically chunk inputs.</p> </li> <li> <p>Use Resolve Operations: Implement resolve operations before reduce operations when dealing with similar entities. Take care to write the compare prompts well to guide the LLM--often the optimizer-synthesized prompts are too generic.</p> </li> </ol> <p>Example: A more specific <code>resolve_medications</code> operation:</p> <pre><code>- name: resolve_medications\n  type: resolve\n  blocking_keys:\n    - medication\n  blocking_threshold: 0.6162\n  comparison_prompt: |\n    Compare the following two medication entries:\n    Entry 1: {{ input1.medication }}\n    Entry 2: {{ input2.medication }}\n\n    Are these medications the same or closely related? Consider the following:\n    1. Are they different brand names for the same active ingredient?\n    2. Are they in the same drug class with similar effects?\n    3. Are they commonly used as alternatives for the same condition?\n\n    Respond with YES if they are the same or closely related, and NO if they are distinct medications.\n</code></pre>"},{"location":"best-practices/#optimization-and-execution","title":"Optimization and Execution","text":"<ol> <li>Use the Optimizer: Leverage DocETL's optimizer for complex pipelines or when dealing with large documents.</li> </ol> <p>Example: Run the optimizer on your pipeline:</p> <pre><code>docetl build pipeline.yaml\n</code></pre> <ol> <li>Leverage Caching: Take advantage of DocETL's caching mechanism to avoid redundant computations. DocETL caches by default.</li> </ol> <p>To clear the cache:</p> <pre><code>docetl clear-cache\n</code></pre> <ol> <li>Monitor Resource Usage: Keep an eye on API costs and processing time, especially when optimizing. Use <code>gpt-4o-mini</code> for optimization (the default is <code>gpt-4o</code>) to save costs. Learn more about how to do this in the optimizer docs.</li> </ol>"},{"location":"best-practices/#additional-notes","title":"Additional Notes","text":"<ul> <li>Sampling Operations: If you want to run an operation on a random sample of your data, you can set the <code>sample</code> parameter for that operation.</li> </ul> <p>Example:</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    sample: 100\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Extract and list all medications mentioned in the transcript:\n      {{ input.src }}\n</code></pre> <ul> <li>Intermediate Output: If you provide an intermediate directory in your configuration, the outputs of each operation will be saved to this directory. This allows you to inspect the results of individual steps in the pipeline and can be useful for debugging or analyzing the pipeline's progress.</li> </ul> <p>Example:</p> <pre><code>pipeline:\n  output:\n    type: file\n    path: medication_summaries.json\n    intermediate_dir: intermediate_results\n</code></pre> <p>By following these comprehensive best practices and examples, you can create more efficient, reliable, and maintainable DocETL pipelines for your data processing tasks. Remember to iterate on your pipeline design, continuously refine your prompts, and leverage DocETL's optimization features to get the best results.</p>"},{"location":"installation/","title":"Installation","text":"<p>DocETL can be easily installed using pip, Python's package installer, or from source. Follow these steps to get DocETL up and running on your system:</p>"},{"location":"installation/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<p>Before installing DocETL, ensure you have Python 3.10 or later installed on your system. You can check your Python version by running:</p>"},{"location":"installation/#installation-via-pip","title":"\ud83d\udce6 Installation via pip","text":"<ol> <li>Install DocETL using pip:</li> </ol> <pre><code>pip install docetl\n</code></pre> <p>If you want to use the parsing tools, you need to install the <code>parsing</code> extra:</p> <pre><code>pip install docetl[parsing]\n</code></pre> <p>This command will install DocETL along with its dependencies as specified in the pyproject.toml file. To verify that DocETL has been installed correctly, you can run the following command in your terminal:</p> <pre><code>docetl version\n</code></pre>"},{"location":"installation/#installation-from-source","title":"\ud83d\udd27 Installation from Source","text":"<p>To install DocETL from source, follow these steps:</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/ucbepic/docetl.git\ncd docetl\n</code></pre> <ol> <li>Install Poetry (if not already installed):</li> </ol> <pre><code>pip install poetry\n</code></pre> <ol> <li>Install the project dependencies and DocETL:</li> </ol> <pre><code>poetry install\n</code></pre> <p>If you want to use the parsing tools, you need to install the <code>parsing</code> extra:</p> <pre><code>poetry install --extras \"parsing\"\n</code></pre> <p>This will create a virtual environment and install all the required dependencies.</p> <ol> <li>Set up your OpenAI API key:</li> </ol> <p>Create a .env file in the project root and add your OpenAI API key:</p> <pre><code>OPENAI_API_KEY=your_api_key_here\n</code></pre> <p>Alternatively, you can set the OPENAI_API_KEY environment variable in your shell.</p> <ol> <li>Run the basic test suite to ensure everything is working (this costs less than $0.01 with OpenAI):</li> </ol> <pre><code>make tests-basic\n</code></pre>"},{"location":"installation/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":"<p>If you encounter any issues during installation, please ensure that:</p> <ul> <li>Your Python version is 3.10 or later</li> <li>You have the latest version of pip installed</li> <li>Your system meets all the requirements specified in the pyproject.toml file</li> </ul> <p>For further assistance, please refer to the project's GitHub repository or reach out on the Discord server.</p>"},{"location":"python-api/","title":"Python API","text":"<p>The DocETL Python API provides a programmatic way to define, optimize, and run document processing pipelines. This approach offers an alternative to the YAML configuration method, allowing for more dynamic and flexible pipeline construction.</p>"},{"location":"python-api/#overview","title":"Overview","text":"<p>The Python API consists of several classes:</p> <ul> <li>Pipeline: The main class for defining and running a complete document processing pipeline.</li> <li>Dataset: Represents a dataset with a type and path.</li> <li>Various operation classes (e.g., MapOp, ReduceOp, FilterOp) for different types of data processing steps.</li> <li>PipelineStep: Represents a step in the pipeline with input and operations.</li> <li>PipelineOutput: Defines the output configuration for the pipeline.</li> </ul>"},{"location":"python-api/#example-usage","title":"Example Usage","text":"<p>Here's an example of how to use the Python API to create and run a simple document processing pipeline:</p> <pre><code>from docetl.api import Pipeline, Dataset, MapOp, ReduceOp, PipelineStep, PipelineOutput\n\n# Define datasets\ndatasets = {\n    \"my_dataset\": Dataset(type=\"file\", path=\"input.json\", parsing=[{\"input_key\": \"file_path\", \"function\": \"txt_to_string\", \"output_key\": \"content\"}]),\n}\n\n# Note that the parsing is applied to the `file_path` key in each item of the dataset,\n# and the result is stored in the `content` key.\n\n# Define operations\noperations = [\n    MapOp(\n        name=\"process\",\n        type=\"map\",\n        prompt=\"Determine what type of document this is: {{ input.content }}\",\n        output={\"schema\": {\"document_type\": \"string\"}}\n    ),\n    ReduceOp(\n        name=\"summarize\",\n        type=\"reduce\",\n        reduce_key=\"document_type\",\n        prompt=\"Summarize the processed contents: {% for item in inputs %}{{ item.content }} {% endfor %}\",\n        output={\"schema\": {\"summary\": \"string\"}}\n    )\n]\n\n# Define pipeline steps\nsteps = [\n    PipelineStep(name=\"process_step\", input=\"my_dataset\", operations=[\"process\"]),\n    PipelineStep(name=\"summarize_step\", input=\"process_step\", operations=[\"summarize\"])\n]\n\n# Define pipeline output\noutput = PipelineOutput(type=\"file\", path=\"output.json\")\n\n# Create the pipeline\npipeline = Pipeline(\n    name=\"example_pipeline\",\n    datasets=datasets,\n    operations=operations,\n    steps=steps,\n    output=output,\n    default_model=\"gpt-4o-mini\"\n)\n\n# Optimize the pipeline\noptimized_pipeline = pipeline.optimize()\n\n# Run the optimized pipeline\nresult = optimized_pipeline.run() # Saves the result to the output path\n\nprint(f\"Pipeline execution completed. Total cost: ${result:.2f}\")\n</code></pre> <p>This example demonstrates how to create a simple pipeline that processes input documents and then summarizes the processed content. The pipeline is optimized before execution to improve performance.</p>"},{"location":"python-api/#api-reference","title":"API Reference","text":"<p>For a complete reference of all available classes and their methods, please refer to the Python API Reference.</p> <p>The API Reference provides detailed information about each class, including:</p> <ul> <li>Available parameters</li> <li>Method signatures</li> <li>Return types</li> <li>Usage examples</li> </ul> <p>By using the Python API, you can create more complex and dynamic pipelines that can adapt to your specific document processing needs.</p>"},{"location":"tutorial/","title":"Tutorial: Analyzing Medical Transcripts with DocETL","text":"<p>This tutorial will guide you through the process of using DocETL to analyze medical transcripts and extract medication information. We'll create a pipeline that identifies medications, resolves similar names, and generates summaries of side effects and therapeutic uses.</p>"},{"location":"tutorial/#installation","title":"Installation","text":"<p>First, let's install DocETL. Follow the instructions in the installation guide to set up DocETL on your system.</p>"},{"location":"tutorial/#setting-up-api-keys","title":"Setting up API Keys","text":"<p>DocETL uses LiteLLM under the hood, which supports various LLM providers. For this tutorial, we'll use OpenAI, as DocETL tests and existing pipelines are run with OpenAI.</p> <p>Setting up your API Key</p> <p>Set your OpenAI API key as an environment variable:</p> <pre><code>export OPENAI_API_KEY=your_api_key_here\n</code></pre> <p>Alternatively, you can create a <code>.env</code> file in your project directory and add the following line:</p> <pre><code>OPENAI_API_KEY=your_api_key_here\n</code></pre> <p>OpenAI Dependency</p> <p>DocETL has been primarily tested with OpenAI's language models and relies heavily on their structured output capabilities. While we aim to support other providers in the future, using OpenAI is currently recommended for the best experience and most reliable results.</p> <p>If you choose to use a different provider, be aware that you may encounter unexpected behavior or reduced functionality, especially with operations that depend on structured outputs. We use tool calling to extract structured outputs from the LLM's response, so make sure your provider supports tool calling.</p> <p>If using a Gemini model, you can use the <code>gemini</code> prefix for the model name. For example, <code>gemini/gemini-1.5-flash-002</code>. (This has worked pretty well for us so far, and is so cheap!)</p> <p>If using Ollama (e.g., llama 3.2), make sure your output schemas are not too complex, since these models are not as good as OpenAI for structured outputs! For example, use parallel map operations to reduce the number of output attributes per prompt.</p>"},{"location":"tutorial/#preparing-the-data","title":"Preparing the Data","text":"<p>Organize your medical transcript data in a JSON file as a list of objects. Each object should have a \"src\" key containing the transcript text. You can download the example dataset here.</p> <p>Sample Data Structure</p> <pre><code>[\n    {\n        \"src\": \"Doctor: Hello, Mrs. Johnson. How have you been feeling since starting the new medication, Lisinopril?\\nPatient: Well, doctor, I've noticed my blood pressure has improved, but I've been experiencing some dry cough...\",\n    },\n    {\n        \"src\": \"Doctor: Good morning, Mr. Smith. I see you're here for a follow-up on your Metformin prescription.\\nPatient: Yes, doctor. I've been taking it regularly, but I'm concerned about some side effects I've been experiencing...\",\n    }\n]\n</code></pre> <p>Save this file as <code>medical_transcripts.json</code> in your project directory.</p>"},{"location":"tutorial/#creating-the-pipeline","title":"Creating the Pipeline","text":"<p>Now, let's create a DocETL pipeline to analyze this data. We'll use a series of operations to extract and process the medication information:</p> <ol> <li>Medication Extraction: Analyze each transcript to identify and list all mentioned medications.</li> <li>Unnesting: The extracted medication list is flattened, such that each medication (and associated data) is a separate document. This operator is akin to the pandas <code>explode</code> operation.</li> <li>Medication Resolution: Similar medication names are resolved to standardize the entries. This step helps in consolidating different variations or brand names of the same medication. For example, step 1 might extract \"Ibuprofen\" and \"Motrin 800mg\" as separate medications, and step 3 might resolve them to a single \"Ibuprofen\" entry.</li> <li>Summary Generation: For each unique medication, generate a summary of side effects and therapeutic uses based on information from all relevant transcripts.</li> </ol> <p>Create a file named <code>pipeline.yaml</code> with the following structure:</p> <p>Pipeline Structure</p> <pre><code>datasets:\n  transcripts:\n    path: medical_transcripts.json\n    type: file\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Analyze the following transcript of a conversation between a doctor and a patient:\n      {{ input.src }}\n      Extract and list all medications mentioned in the transcript.\n      If no medications are mentioned, return an empty list.\n\n  - name: unnest_medications\n    type: unnest\n    unnest_key: medication\n\n  - name: resolve_medications\n    type: resolve\n    blocking_keys:\n      - medication\n    blocking_threshold: 0.6162\n    comparison_prompt: |\n      Compare the following two medication entries:\n      Entry 1: {{ input1.medication }}\n      Entry 2: {{ input2.medication }}\n      Are these medications likely to be the same or closely related?\n    embedding_model: text-embedding-3-small\n    output:\n      schema:\n        medication: str\n    resolution_prompt: |\n      Given the following matched medication entries:\n      {% for entry in inputs %}\n      Entry {{ loop.index }}: {{ entry.medication }}\n      {% endfor %}\n      Determine the best resolved medication name for this group of entries. The resolved\n      name should be a standardized, widely recognized medication name that best represents\n      all matched entries.\n\n  - name: summarize_prescriptions\n    type: reduce\n    reduce_key:\n      - medication\n    output:\n      schema:\n        side_effects: str\n        uses: str\n    prompt: |\n      Here are some transcripts of conversations between a doctor and a patient:\n\n      {% for value in inputs %}\n      Transcript {{ loop.index }}:\n      {{ value.src }}\n      {% endfor %}\n\n      For the medication {{ reduce_key }}, please provide the following information based on all the transcripts above:\n\n      1. Side Effects: Summarize all mentioned side effects of {{ reduce_key }}.\n      2. Therapeutic Uses: Explain the medical conditions or symptoms for which {{ reduce_key }} was prescribed or recommended.\n\n      Ensure your summary:\n      - Is based solely on information from the provided transcripts\n      - Focuses only on {{ reduce_key }}, not other medications\n      - Includes relevant details from all transcripts\n      - Is clear and concise\n      - Includes quotes from the transcripts\n\npipeline:\n  steps:\n    - name: medical_info_extraction\n      input: transcripts\n      operations:\n        - extract_medications\n        - unnest_medications\n        - resolve_medications\n        - summarize_prescriptions\n  output:\n    type: file\n    path: medication_summaries.json\n    intermediate_dir: intermediate_results\n</code></pre>"},{"location":"tutorial/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Pipeline Performance</p> <p>When running this pipeline on a sample dataset, we observed the following performance metrics using <code>gpt-4o-mini</code> as defined in the pipeline:</p> <ul> <li>Total cost: $0.10</li> <li>Total execution time: 49.13 seconds</li> </ul> <p>If you want to run it on a smaller sample, set the <code>sample</code> parameter for the map operation. For example, <code>sample: 10</code> will run the pipeline on a random sample of 10 transcripts:</p> <pre><code>operations:\n  - name: extract_medications\n    type: map\n    sample: 10\n    ...\n</code></pre> <p>To execute the pipeline, run the following command in your terminal:</p> <pre><code>docetl run pipeline.yaml\n</code></pre> <p>This will process the medical transcripts, extract medication information, resolve similar medication names, and generate summaries of side effects and therapeutic uses for each medication. The results will be saved in <code>medication_summaries.json</code>.</p>"},{"location":"tutorial/#further-questions","title":"Further Questions","text":"What if I want to focus on a specific type of medication or medical condition? <p>You can modify the prompts in the <code>extract_medications</code> and <code>summarize_prescriptions</code> operations to focus on specific types of medications or medical conditions. For example, you could update the <code>extract_medications</code> prompt to only list medications related to cardiovascular diseases.</p> How can I improve the accuracy of medication name resolution? <p>The <code>resolve_medications</code> operation uses a blocking threshold and comparison prompt to identify similar medication names. Learn more about how to configure this operation in the resolve operation documentation. To automatically find the optimal blocking threshold for your data, you can invoke the optimizer, as described in the optimization documentation.</p> Can I process other types of medical documents with this pipeline? <p>Yes, you can adapt this pipeline to process other types of medical documents by modifying the input data format and adjusting the prompts in each operation. For example, you could use it to analyze discharge summaries, clinical notes, or research papers by updating the extraction and summarization prompts accordingly.</p> <p>If you're unsure about the optimal configuration for your specific use case, you can use DocETL's optimizer, which can be invoked using <code>docetl build</code> instead of <code>docetl run</code>. Learn more about the optimizer in the optimization documentation.</p>"},{"location":"api-reference/cli/","title":"docetl.cli","text":""},{"location":"api-reference/cli/#docetl.cli.run","title":"<code>docetl.cli.run(yaml_file=typer.Argument(..., help='Path to the YAML file containing the pipeline configuration'), max_threads=typer.Option(None, help='Maximum number of threads to use for running operations'))</code>","text":"<p>Run the configuration specified in the YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Path</code> <p>Path to the YAML file containing the pipeline configuration.</p> <code>Argument(..., help='Path to the YAML file containing the pipeline configuration')</code> <code>max_threads</code> <code>Optional[int]</code> <p>Maximum number of threads to use for running operations.</p> <code>Option(None, help='Maximum number of threads to use for running operations')</code> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef run(\n    yaml_file: Path = typer.Argument(\n        ..., help=\"Path to the YAML file containing the pipeline configuration\"\n    ),\n    max_threads: Optional[int] = typer.Option(\n        None, help=\"Maximum number of threads to use for running operations\"\n    ),\n):\n    \"\"\"\n    Run the configuration specified in the YAML file.\n\n    Args:\n        yaml_file (Path): Path to the YAML file containing the pipeline configuration.\n        max_threads (Optional[int]): Maximum number of threads to use for running operations.\n    \"\"\"\n    # Get the current working directory (where the user called the command)\n    cwd = os.getcwd()\n\n    # Load .env file from the current working directory\n    env_file = os.path.join(cwd, \".env\")\n    if os.path.exists(env_file):\n        load_dotenv(env_file)\n\n    runner = DSLRunner.from_yaml(str(yaml_file), max_threads=max_threads)\n    runner.run()\n</code></pre>"},{"location":"api-reference/cli/#docetl.cli.build","title":"<code>docetl.cli.build(yaml_file=typer.Argument(..., help='Path to the YAML file containing the pipeline configuration'), max_threads=typer.Option(None, help='Maximum number of threads to use for running operations'), model=typer.Option('gpt-4o', help='Model to use for optimization'), resume=typer.Option(False, help='Resume optimization from a previous build that may have failed'), timeout=typer.Option(60, help='Timeout for optimization operations in seconds'))</code>","text":"<p>Build and optimize the configuration specified in the YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Path</code> <p>Path to the YAML file containing the pipeline configuration.</p> <code>Argument(..., help='Path to the YAML file containing the pipeline configuration')</code> <code>max_threads</code> <code>Optional[int]</code> <p>Maximum number of threads to use for running operations.</p> <code>Option(None, help='Maximum number of threads to use for running operations')</code> <code>model</code> <code>str</code> <p>Model to use for optimization. Defaults to \"gpt-4o\".</p> <code>Option('gpt-4o', help='Model to use for optimization')</code> <code>resume</code> <code>bool</code> <p>Whether to resume optimization from a previous run. Defaults to False.</p> <code>Option(False, help='Resume optimization from a previous build that may have failed')</code> <code>timeout</code> <code>int</code> <p>Timeout for optimization operations in seconds. Defaults to 60.</p> <code>Option(60, help='Timeout for optimization operations in seconds')</code> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef build(\n    yaml_file: Path = typer.Argument(\n        ..., help=\"Path to the YAML file containing the pipeline configuration\"\n    ),\n    max_threads: Optional[int] = typer.Option(\n        None, help=\"Maximum number of threads to use for running operations\"\n    ),\n    model: str = typer.Option(\"gpt-4o\", help=\"Model to use for optimization\"),\n    resume: bool = typer.Option(\n        False, help=\"Resume optimization from a previous build that may have failed\"\n    ),\n    timeout: int = typer.Option(\n        60, help=\"Timeout for optimization operations in seconds\"\n    ),\n):\n    \"\"\"\n    Build and optimize the configuration specified in the YAML file.\n\n    Args:\n        yaml_file (Path): Path to the YAML file containing the pipeline configuration.\n        max_threads (Optional[int]): Maximum number of threads to use for running operations.\n        model (str): Model to use for optimization. Defaults to \"gpt-4o\".\n        resume (bool): Whether to resume optimization from a previous run. Defaults to False.\n        timeout (int): Timeout for optimization operations in seconds. Defaults to 60.\n    \"\"\"\n    # Get the current working directory (where the user called the command)\n    cwd = os.getcwd()\n\n    # Load .env file from the current working directory\n    env_file = os.path.join(cwd, \".env\")\n    if os.path.exists(env_file):\n        load_dotenv(env_file)\n\n    optimizer = Optimizer.from_yaml(\n        str(yaml_file),\n        max_threads=max_threads,\n        model=model,\n        timeout=timeout,\n        resume=resume,\n    )\n    optimizer.optimize()\n    optimizer.save_optimized_config()\n</code></pre>"},{"location":"api-reference/cli/#docetl.cli.clear_cache","title":"<code>docetl.cli.clear_cache()</code>","text":"<p>Clear the LLM cache stored on disk.</p> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef clear_cache():\n    \"\"\"\n    Clear the LLM cache stored on disk.\n    \"\"\"\n    cc()\n</code></pre>"},{"location":"api-reference/docetl/","title":"docetl","text":""},{"location":"api-reference/docetl/#docetl.DSLRunner","title":"<code>docetl.DSLRunner</code>","text":"<p>               Bases: <code>ConfigWrapper</code></p> <p>A class for executing Domain-Specific Language (DSL) configurations.</p> <p>This class is responsible for loading, validating, and executing DSL configurations defined in YAML files. It manages datasets, executes pipeline steps, and tracks the cost of operations.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>The loaded configuration from the YAML file.</p> <code>default_model</code> <code>str</code> <p>The default language model to use for operations.</p> <code>max_threads</code> <code>int</code> <p>Maximum number of threads for parallel processing.</p> <code>console</code> <code>Console</code> <p>Rich console for output formatting.</p> <code>datasets</code> <code>Dict</code> <p>Storage for loaded datasets.</p> Source code in <code>docetl/runner.py</code> <pre><code>class DSLRunner(ConfigWrapper):\n    \"\"\"\n    A class for executing Domain-Specific Language (DSL) configurations.\n\n    This class is responsible for loading, validating, and executing DSL configurations\n    defined in YAML files. It manages datasets, executes pipeline steps, and tracks\n    the cost of operations.\n\n    Attributes:\n        config (Dict): The loaded configuration from the YAML file.\n        default_model (str): The default language model to use for operations.\n        max_threads (int): Maximum number of threads for parallel processing.\n        console (Console): Rich console for output formatting.\n        datasets (Dict): Storage for loaded datasets.\n    \"\"\"\n\n    def __init__(self, config: Dict, max_threads: int = None):\n        \"\"\"\n        Initialize the DSLRunner with a YAML configuration file.\n\n        Args:\n            max_threads (int, optional): Maximum number of threads to use. Defaults to None.\n        \"\"\"\n        ConfigWrapper.__init__(self, config, max_threads)\n        self.datasets = {}\n\n        self.intermediate_dir = (\n            self.config.get(\"pipeline\", {}).get(\"output\", {}).get(\"intermediate_dir\")\n        )\n\n        # Create parsing tool map\n        self.parsing_tool_map = create_parsing_tool_map(\n            self.config.get(\"parsing_tools\", None)\n        )\n\n        # Check if output path is correctly formatted as JSON\n        output_path = self.config.get(\"pipeline\", {}).get(\"output\", {}).get(\"path\")\n        if output_path:\n            if not (\n                output_path.lower().endswith(\".json\")\n                or output_path.lower().endswith(\".csv\")\n            ):\n                raise ValueError(\n                    f\"Output path '{output_path}' is not a JSON or CSV file. Please provide a path ending with '.json' or '.csv'.\"\n                )\n        else:\n            raise ValueError(\n                \"No output path specified in the configuration. Please provide an output path ending with '.json' or '.csv' in the configuration.\"\n            )\n\n        self.syntax_check()\n\n        op_map = {op[\"name\"]: op for op in self.config[\"operations\"]}\n\n        # Hash each pipeline step/operation\n        # for each step op, hash the code of each op up until and (including that op)\n        self.step_op_hashes = defaultdict(dict)\n        for step in self.config[\"pipeline\"][\"steps\"]:\n            for idx, op in enumerate(step[\"operations\"]):\n                op_name = op if isinstance(op, str) else list(op.keys())[0]\n\n                all_ops_until_and_including_current = [\n                    op_map[prev_op] for prev_op in step[\"operations\"][:idx]\n                ] + [op_map[op_name]]\n                # If there's no model in the op, add the default model\n                for op in all_ops_until_and_including_current:\n                    if \"model\" not in op:\n                        op[\"model\"] = self.default_model\n\n                all_ops_str = json.dumps(all_ops_until_and_including_current)\n                self.step_op_hashes[step[\"name\"]][op_name] = hashlib.sha256(\n                    all_ops_str.encode()\n                ).hexdigest()\n\n    def syntax_check(self):\n        \"\"\"\n        Perform a syntax check on all operations defined in the configuration.\n\n        This method validates each operation by attempting to instantiate it.\n        If any operation fails to instantiate, a ValueError is raised.\n\n        Raises:\n            ValueError: If any operation fails the syntax check.\n        \"\"\"\n        self.console.rule(\"[yellow]Syntax Check[/yellow]\")\n        self.console.print(\n            \"[yellow]Performing syntax check on all operations...[/yellow]\"\n        )\n\n        for operation_config in self.config[\"operations\"]:\n            operation = operation_config[\"name\"]\n            operation_type = operation_config[\"type\"]\n\n            try:\n                operation_class = get_operation(operation_type)\n                operation_class(\n                    self,\n                    operation_config,\n                    self.default_model,\n                    self.max_threads,\n                    self.console,\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Syntax check failed for operation '{operation}': {str(e)}\"\n                )\n\n        self.console.print(\"[green]Syntax check passed for all operations.[/green]\")\n\n    def find_operation(self, op_name: str) -&gt; Dict:\n        for operation_config in self.config[\"operations\"]:\n            if operation_config[\"name\"] == op_name:\n                return operation_config\n        raise ValueError(f\"Operation '{op_name}' not found in configuration.\")\n\n    def run(self) -&gt; float:\n        \"\"\"\n        Execute the entire pipeline defined in the configuration.\n\n        This method loads datasets, executes each step in the pipeline, saves the output,\n        and returns the total cost of execution.\n\n        Returns:\n            float: The total cost of executing the pipeline.\n        \"\"\"\n        self.console.rule(\"[bold blue]Pipeline Execution[/bold blue]\")\n        start_time = time.time()\n        self.load_datasets()\n        total_cost = 0\n        for step in self.config[\"pipeline\"][\"steps\"]:\n            step_name = step[\"name\"]\n            input_data = (\n                self.datasets[step[\"input\"]].load() if \"input\" in step else None\n            )\n            output_data, step_cost = self.execute_step(step, input_data)\n            self.datasets[step_name] = Dataset(self, \"memory\", output_data)\n            flush_cache(self.console)\n            total_cost += step_cost\n            self.console.log(\n                f\"Step [cyan]{step_name}[/cyan] completed. Cost: [green]${step_cost:.2f}[/green]\"\n            )\n\n        self.save_output(\n            self.datasets[self.config[\"pipeline\"][\"steps\"][-1][\"name\"]].load()\n        )\n\n        # Save the self.step_op_hashes to a file if self.intermediate_dir exists\n        if self.intermediate_dir:\n            with open(\n                os.path.join(self.intermediate_dir, \".docetl_intermediate_config.json\"),\n                \"w\",\n            ) as f:\n                json.dump(self.step_op_hashes, f)\n\n        self.console.rule(\"[bold green]Execution Summary[/bold green]\")\n        self.console.print(f\"[bold green]Total cost: [green]${total_cost:.2f}[/green]\")\n        self.console.print(\n            f\"[bold green]Total time: [green]{time.time() - start_time:.2f} seconds[/green]\"\n        )\n\n        return total_cost\n\n    def load_datasets(self):\n        \"\"\"\n        Load all datasets defined in the configuration.\n\n        This method creates Dataset objects for each dataset in the configuration.\n\n        Raises:\n            ValueError: If an unsupported dataset type is encountered.\n        \"\"\"\n        self.console.rule(\"[cyan]Loading Datasets[/cyan]\")\n        for name, dataset_config in self.config[\"datasets\"].items():\n            if dataset_config[\"type\"] == \"file\":\n                self.datasets[name] = Dataset(\n                    self,\n                    \"file\",\n                    dataset_config[\"path\"],\n                    source=\"local\",\n                    parsing=dataset_config.get(\"parsing\", []),\n                    user_defined_parsing_tool_map=self.parsing_tool_map,\n                )\n                self.console.print(f\"Loaded dataset: [bold]{name}[/bold]\")\n            else:\n                raise ValueError(f\"Unsupported dataset type: {dataset_config['type']}\")\n\n    def save_output(self, data: List[Dict]):\n        \"\"\"\n        Save the final output of the pipeline.\n\n        Args:\n            data (List[Dict]): The data to be saved.\n\n        Raises:\n            ValueError: If an unsupported output type is specified in the configuration.\n        \"\"\"\n        self.console.rule(\"[cyan]Saving Output[/cyan]\")\n        output_config = self.config[\"pipeline\"][\"output\"]\n        if output_config[\"type\"] == \"file\":\n            if output_config[\"path\"].lower().endswith(\".json\"):\n                with open(output_config[\"path\"], \"w\") as file:\n                    json.dump(data, file, indent=2)\n            else:  # CSV\n                import csv\n\n                with open(output_config[\"path\"], \"w\", newline=\"\") as file:\n                    writer = csv.DictWriter(file, fieldnames=data[0].keys())\n                    writer.writeheader()\n                    writer.writerows(data)\n            self.console.print(\n                f\"[green italic]\ud83d\udcbe Output saved to {output_config['path']}[/green italic]\"\n            )\n        else:\n            raise ValueError(f\"Unsupported output type: {output_config['type']}\")\n\n    def execute_step(\n        self, step: Dict, input_data: Optional[List[Dict]]\n    ) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Execute a single step in the pipeline.\n\n        This method runs all operations defined for a step, updating the progress\n        and calculating the cost.\n\n        Args:\n            step (Dict): The step configuration.\n            input_data (Optional[List[Dict]]): Input data for the step.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the output data and the total cost of the step.\n        \"\"\"\n        self.console.rule(f\"[bold blue]Executing Step: {step['name']}[/bold blue]\")\n        total_cost = 0\n        for operation in step[\"operations\"]:\n            if isinstance(operation, dict):\n                operation_name = list(operation.keys())[0]\n                operation_config = self.find_operation(operation_name)\n            else:\n                operation_name = operation\n                operation_config = {}\n\n            # Load from checkpoint if it exists\n            attempted_input_data = self._load_from_checkpoint_if_exists(\n                step[\"name\"], operation_name\n            )\n            if attempted_input_data is not None:\n                input_data = attempted_input_data\n                self.console.print(\n                    f\"[green]\u2713 [italic]Loaded saved data for operation '{operation_name}' in step '{step['name']}'[/italic][/green]\"\n                )\n                continue\n\n            op_object = self.find_operation(operation_name).copy()\n            op_object.update(operation_config)\n\n            # If sample is set, sample the input data\n            if op_object.get(\"sample\"):\n                input_data = self.datasets[step[\"input\"]].sample(op_object[\"sample\"])\n\n            with self.console.status(\"[bold]Running Operation:[/bold]\") as status:\n                status.update(f\"Type: [cyan]{op_object['type']}[/cyan]\")\n                status.update(f\"Name: [cyan]{op_object.get('name', 'Unnamed')}[/cyan]\")\n                self.status = status\n\n                operation_class = get_operation(op_object[\"type\"])\n                operation_instance = operation_class(\n                    self,\n                    op_object,\n                    self.default_model,\n                    self.max_threads,\n                    self.console,\n                    self.status,\n                )\n                if op_object[\"type\"] == \"equijoin\":\n                    left_data = self.datasets[op_object[\"left\"]].load()\n                    right_data = self.datasets[op_object[\"right\"]].load()\n                    input_data, cost = operation_instance.execute(left_data, right_data)\n                else:\n                    input_data, cost = operation_instance.execute(input_data)\n                total_cost += cost\n                self.console.log(\n                    f\"\\tOperation [cyan]{operation_name}[/cyan] completed. Cost: [green]${cost:.2f}[/green]\"\n                )\n\n            # Checkpoint after each operation\n            if self.intermediate_dir:\n                self._save_checkpoint(step[\"name\"], operation_name, input_data)\n\n        return input_data, total_cost\n\n    def _load_from_checkpoint_if_exists(\n        self, step_name: str, operation_name: str\n    ) -&gt; Optional[List[Dict]]:\n        if self.intermediate_dir is None:\n            return None\n\n        intermediate_config_path = os.path.join(\n            self.intermediate_dir, \".docetl_intermediate_config.json\"\n        )\n        if not os.path.exists(intermediate_config_path):\n            return None\n\n        # See if the checkpoint config is the same as the current step op hash\n        with open(intermediate_config_path, \"r\") as f:\n            intermediate_config = json.load(f)\n\n        if (\n            intermediate_config.get(step_name, {}).get(operation_name, \"\")\n            != self.step_op_hashes[step_name][operation_name]\n        ):\n            return None\n\n        checkpoint_path = os.path.join(\n            self.intermediate_dir, step_name, f\"{operation_name}.json\"\n        )\n        # check if checkpoint exists\n        if os.path.exists(checkpoint_path):\n            if f\"{step_name}_{operation_name}\" not in self.datasets:\n                self.datasets[f\"{step_name}_{operation_name}\"] = Dataset(\n                    self, \"file\", checkpoint_path, \"local\"\n                )\n            return self.datasets[f\"{step_name}_{operation_name}\"].load()\n        return None\n\n    def _save_checkpoint(self, step_name: str, operation_name: str, data: List[Dict]):\n        \"\"\"\n        Save a checkpoint of the current data after an operation.\n\n        This method creates a JSON file containing the current state of the data\n        after an operation has been executed. The checkpoint is saved in a directory\n        structure that reflects the step and operation names.\n\n        Args:\n            step_name (str): The name of the current step in the pipeline.\n            operation_name (str): The name of the operation that was just executed.\n            data (List[Dict]): The current state of the data to be checkpointed.\n\n        Note:\n            The checkpoint is saved only if a checkpoint directory has been specified\n            when initializing the DSLRunner.\n        \"\"\"\n        checkpoint_path = os.path.join(\n            self.intermediate_dir, step_name, f\"{operation_name}.json\"\n        )\n        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n        with open(checkpoint_path, \"w\") as f:\n            json.dump(data, f)\n\n        self.console.print(\n            f\"[green]\u2713 [italic]Intermediate saved for operation '{operation_name}' in step '{step_name}' at {checkpoint_path}[/italic][/green]\"\n        )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.__init__","title":"<code>__init__(config, max_threads=None)</code>","text":"<p>Initialize the DSLRunner with a YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>max_threads</code> <code>int</code> <p>Maximum number of threads to use. Defaults to None.</p> <code>None</code> Source code in <code>docetl/runner.py</code> <pre><code>def __init__(self, config: Dict, max_threads: int = None):\n    \"\"\"\n    Initialize the DSLRunner with a YAML configuration file.\n\n    Args:\n        max_threads (int, optional): Maximum number of threads to use. Defaults to None.\n    \"\"\"\n    ConfigWrapper.__init__(self, config, max_threads)\n    self.datasets = {}\n\n    self.intermediate_dir = (\n        self.config.get(\"pipeline\", {}).get(\"output\", {}).get(\"intermediate_dir\")\n    )\n\n    # Create parsing tool map\n    self.parsing_tool_map = create_parsing_tool_map(\n        self.config.get(\"parsing_tools\", None)\n    )\n\n    # Check if output path is correctly formatted as JSON\n    output_path = self.config.get(\"pipeline\", {}).get(\"output\", {}).get(\"path\")\n    if output_path:\n        if not (\n            output_path.lower().endswith(\".json\")\n            or output_path.lower().endswith(\".csv\")\n        ):\n            raise ValueError(\n                f\"Output path '{output_path}' is not a JSON or CSV file. Please provide a path ending with '.json' or '.csv'.\"\n            )\n    else:\n        raise ValueError(\n            \"No output path specified in the configuration. Please provide an output path ending with '.json' or '.csv' in the configuration.\"\n        )\n\n    self.syntax_check()\n\n    op_map = {op[\"name\"]: op for op in self.config[\"operations\"]}\n\n    # Hash each pipeline step/operation\n    # for each step op, hash the code of each op up until and (including that op)\n    self.step_op_hashes = defaultdict(dict)\n    for step in self.config[\"pipeline\"][\"steps\"]:\n        for idx, op in enumerate(step[\"operations\"]):\n            op_name = op if isinstance(op, str) else list(op.keys())[0]\n\n            all_ops_until_and_including_current = [\n                op_map[prev_op] for prev_op in step[\"operations\"][:idx]\n            ] + [op_map[op_name]]\n            # If there's no model in the op, add the default model\n            for op in all_ops_until_and_including_current:\n                if \"model\" not in op:\n                    op[\"model\"] = self.default_model\n\n            all_ops_str = json.dumps(all_ops_until_and_including_current)\n            self.step_op_hashes[step[\"name\"]][op_name] = hashlib.sha256(\n                all_ops_str.encode()\n            ).hexdigest()\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.execute_step","title":"<code>execute_step(step, input_data)</code>","text":"<p>Execute a single step in the pipeline.</p> <p>This method runs all operations defined for a step, updating the progress and calculating the cost.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>Dict</code> <p>The step configuration.</p> required <code>input_data</code> <code>Optional[List[Dict]]</code> <p>Input data for the step.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Dict], float]</code> <p>Tuple[List[Dict], float]: A tuple containing the output data and the total cost of the step.</p> Source code in <code>docetl/runner.py</code> <pre><code>def execute_step(\n    self, step: Dict, input_data: Optional[List[Dict]]\n) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Execute a single step in the pipeline.\n\n    This method runs all operations defined for a step, updating the progress\n    and calculating the cost.\n\n    Args:\n        step (Dict): The step configuration.\n        input_data (Optional[List[Dict]]): Input data for the step.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the output data and the total cost of the step.\n    \"\"\"\n    self.console.rule(f\"[bold blue]Executing Step: {step['name']}[/bold blue]\")\n    total_cost = 0\n    for operation in step[\"operations\"]:\n        if isinstance(operation, dict):\n            operation_name = list(operation.keys())[0]\n            operation_config = self.find_operation(operation_name)\n        else:\n            operation_name = operation\n            operation_config = {}\n\n        # Load from checkpoint if it exists\n        attempted_input_data = self._load_from_checkpoint_if_exists(\n            step[\"name\"], operation_name\n        )\n        if attempted_input_data is not None:\n            input_data = attempted_input_data\n            self.console.print(\n                f\"[green]\u2713 [italic]Loaded saved data for operation '{operation_name}' in step '{step['name']}'[/italic][/green]\"\n            )\n            continue\n\n        op_object = self.find_operation(operation_name).copy()\n        op_object.update(operation_config)\n\n        # If sample is set, sample the input data\n        if op_object.get(\"sample\"):\n            input_data = self.datasets[step[\"input\"]].sample(op_object[\"sample\"])\n\n        with self.console.status(\"[bold]Running Operation:[/bold]\") as status:\n            status.update(f\"Type: [cyan]{op_object['type']}[/cyan]\")\n            status.update(f\"Name: [cyan]{op_object.get('name', 'Unnamed')}[/cyan]\")\n            self.status = status\n\n            operation_class = get_operation(op_object[\"type\"])\n            operation_instance = operation_class(\n                self,\n                op_object,\n                self.default_model,\n                self.max_threads,\n                self.console,\n                self.status,\n            )\n            if op_object[\"type\"] == \"equijoin\":\n                left_data = self.datasets[op_object[\"left\"]].load()\n                right_data = self.datasets[op_object[\"right\"]].load()\n                input_data, cost = operation_instance.execute(left_data, right_data)\n            else:\n                input_data, cost = operation_instance.execute(input_data)\n            total_cost += cost\n            self.console.log(\n                f\"\\tOperation [cyan]{operation_name}[/cyan] completed. Cost: [green]${cost:.2f}[/green]\"\n            )\n\n        # Checkpoint after each operation\n        if self.intermediate_dir:\n            self._save_checkpoint(step[\"name\"], operation_name, input_data)\n\n    return input_data, total_cost\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.load_datasets","title":"<code>load_datasets()</code>","text":"<p>Load all datasets defined in the configuration.</p> <p>This method creates Dataset objects for each dataset in the configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported dataset type is encountered.</p> Source code in <code>docetl/runner.py</code> <pre><code>def load_datasets(self):\n    \"\"\"\n    Load all datasets defined in the configuration.\n\n    This method creates Dataset objects for each dataset in the configuration.\n\n    Raises:\n        ValueError: If an unsupported dataset type is encountered.\n    \"\"\"\n    self.console.rule(\"[cyan]Loading Datasets[/cyan]\")\n    for name, dataset_config in self.config[\"datasets\"].items():\n        if dataset_config[\"type\"] == \"file\":\n            self.datasets[name] = Dataset(\n                self,\n                \"file\",\n                dataset_config[\"path\"],\n                source=\"local\",\n                parsing=dataset_config.get(\"parsing\", []),\n                user_defined_parsing_tool_map=self.parsing_tool_map,\n            )\n            self.console.print(f\"Loaded dataset: [bold]{name}[/bold]\")\n        else:\n            raise ValueError(f\"Unsupported dataset type: {dataset_config['type']}\")\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.run","title":"<code>run()</code>","text":"<p>Execute the entire pipeline defined in the configuration.</p> <p>This method loads datasets, executes each step in the pipeline, saves the output, and returns the total cost of execution.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The total cost of executing the pipeline.</p> Source code in <code>docetl/runner.py</code> <pre><code>def run(self) -&gt; float:\n    \"\"\"\n    Execute the entire pipeline defined in the configuration.\n\n    This method loads datasets, executes each step in the pipeline, saves the output,\n    and returns the total cost of execution.\n\n    Returns:\n        float: The total cost of executing the pipeline.\n    \"\"\"\n    self.console.rule(\"[bold blue]Pipeline Execution[/bold blue]\")\n    start_time = time.time()\n    self.load_datasets()\n    total_cost = 0\n    for step in self.config[\"pipeline\"][\"steps\"]:\n        step_name = step[\"name\"]\n        input_data = (\n            self.datasets[step[\"input\"]].load() if \"input\" in step else None\n        )\n        output_data, step_cost = self.execute_step(step, input_data)\n        self.datasets[step_name] = Dataset(self, \"memory\", output_data)\n        flush_cache(self.console)\n        total_cost += step_cost\n        self.console.log(\n            f\"Step [cyan]{step_name}[/cyan] completed. Cost: [green]${step_cost:.2f}[/green]\"\n        )\n\n    self.save_output(\n        self.datasets[self.config[\"pipeline\"][\"steps\"][-1][\"name\"]].load()\n    )\n\n    # Save the self.step_op_hashes to a file if self.intermediate_dir exists\n    if self.intermediate_dir:\n        with open(\n            os.path.join(self.intermediate_dir, \".docetl_intermediate_config.json\"),\n            \"w\",\n        ) as f:\n            json.dump(self.step_op_hashes, f)\n\n    self.console.rule(\"[bold green]Execution Summary[/bold green]\")\n    self.console.print(f\"[bold green]Total cost: [green]${total_cost:.2f}[/green]\")\n    self.console.print(\n        f\"[bold green]Total time: [green]{time.time() - start_time:.2f} seconds[/green]\"\n    )\n\n    return total_cost\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.save_output","title":"<code>save_output(data)</code>","text":"<p>Save the final output of the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict]</code> <p>The data to be saved.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported output type is specified in the configuration.</p> Source code in <code>docetl/runner.py</code> <pre><code>def save_output(self, data: List[Dict]):\n    \"\"\"\n    Save the final output of the pipeline.\n\n    Args:\n        data (List[Dict]): The data to be saved.\n\n    Raises:\n        ValueError: If an unsupported output type is specified in the configuration.\n    \"\"\"\n    self.console.rule(\"[cyan]Saving Output[/cyan]\")\n    output_config = self.config[\"pipeline\"][\"output\"]\n    if output_config[\"type\"] == \"file\":\n        if output_config[\"path\"].lower().endswith(\".json\"):\n            with open(output_config[\"path\"], \"w\") as file:\n                json.dump(data, file, indent=2)\n        else:  # CSV\n            import csv\n\n            with open(output_config[\"path\"], \"w\", newline=\"\") as file:\n                writer = csv.DictWriter(file, fieldnames=data[0].keys())\n                writer.writeheader()\n                writer.writerows(data)\n        self.console.print(\n            f\"[green italic]\ud83d\udcbe Output saved to {output_config['path']}[/green italic]\"\n        )\n    else:\n        raise ValueError(f\"Unsupported output type: {output_config['type']}\")\n</code></pre>"},{"location":"api-reference/docetl/#docetl.DSLRunner.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Perform a syntax check on all operations defined in the configuration.</p> <p>This method validates each operation by attempting to instantiate it. If any operation fails to instantiate, a ValueError is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any operation fails the syntax check.</p> Source code in <code>docetl/runner.py</code> <pre><code>def syntax_check(self):\n    \"\"\"\n    Perform a syntax check on all operations defined in the configuration.\n\n    This method validates each operation by attempting to instantiate it.\n    If any operation fails to instantiate, a ValueError is raised.\n\n    Raises:\n        ValueError: If any operation fails the syntax check.\n    \"\"\"\n    self.console.rule(\"[yellow]Syntax Check[/yellow]\")\n    self.console.print(\n        \"[yellow]Performing syntax check on all operations...[/yellow]\"\n    )\n\n    for operation_config in self.config[\"operations\"]:\n        operation = operation_config[\"name\"]\n        operation_type = operation_config[\"type\"]\n\n        try:\n            operation_class = get_operation(operation_type)\n            operation_class(\n                self,\n                operation_config,\n                self.default_model,\n                self.max_threads,\n                self.console,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Syntax check failed for operation '{operation}': {str(e)}\"\n            )\n\n    self.console.print(\"[green]Syntax check passed for all operations.[/green]\")\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer","title":"<code>docetl.Optimizer</code>","text":"<p>               Bases: <code>ConfigWrapper</code></p> Source code in <code>docetl/builder.py</code> <pre><code>class Optimizer(ConfigWrapper):\n    @classmethod\n    def from_yaml(cls, yaml_file: str, **kwargs):\n        # check that file ends with .yaml or .yml\n        if not yaml_file.endswith(\".yaml\") and not yaml_file.endswith(\".yml\"):\n            raise ValueError(\n                \"Invalid file type. Please provide a YAML file ending with '.yaml' or '.yml'.\"\n            )\n\n        base_name = yaml_file.rsplit(\".\", 1)[0]\n        suffix = yaml_file.split(\"/\")[-1].split(\".\")[0]\n        return super(Optimizer, cls).from_yaml(\n            yaml_file, base_name=base_name, yaml_file_suffix=suffix, **kwargs\n        )\n\n    def __init__(\n        self,\n        config: Dict,\n        base_name: str,\n        yaml_file_suffix: str,\n        max_threads: Optional[int] = None,\n        model: str = \"gpt-4o\",\n        resume: bool = False,\n        timeout: int = 60,\n    ):\n        \"\"\"\n        Initialize the Optimizer class.\n\n        This method sets up the optimizer with the given configuration file and parameters.\n        It loads the configuration, initializes the console for output, sets up the LLM client,\n        and prepares various attributes for optimization.\n\n        Args:\n            yaml_file (str): Path to the YAML configuration file.\n            max_threads (Optional[int]): Maximum number of threads to use for parallel processing.\n                If None, it will be set to (number of CPUs * 4).\n            model (str): The name of the language model to use. Defaults to \"gpt-4o\".\n            resume (bool): Whether to resume optimization from a previous run. Defaults to False.\n            timeout (int): Timeout in seconds for operations. Defaults to 60.\n\n        Attributes:\n            yaml_file_path (str): Stores the path to the YAML file.\n            config (Dict): Stores the loaded configuration from the YAML file.\n            console (Console): Rich console for formatted output.\n            optimized_config (Dict): A copy of the original config to be optimized.\n            llm_client (LLMClient): Client for interacting with the language model.\n            max_threads (int): Maximum number of threads for parallel processing.\n            operations_cost (float): Tracks the total cost of operations.\n            timeout (int): Timeout for operations in seconds.\n            selectivities (defaultdict): Stores selectivity information for operations.\n                Selectivity is the ratio of output size to input size for an operation.\n                It's used to estimate how much data will flow through the pipeline after\n                each operation, which helps in optimizing subsequent operations and\n                determining appropriate sample sizes. For example, a selectivity of 0.5\n                means an operation halves the size of its input data.\n            datasets (Dict): Stores loaded datasets.\n\n        The method also calls print_optimizer_config() to display the initial configuration.\n        \"\"\"\n        ConfigWrapper.__init__(self, config, max_threads)\n        self.optimized_config = copy.deepcopy(self.config)\n        self.llm_client = LLMClient(model)\n        self.operations_cost = 0\n        self.timeout = timeout\n        self.selectivities = defaultdict(dict)\n        self.samples_taken = defaultdict(dict)\n        self.resume = resume\n\n        # create parsing tool map\n        self.parsing_tool_map = create_parsing_tool_map(\n            self.config.get(\"parsing_tools\", None)\n        )\n\n        home_dir = os.path.expanduser(\"~\")\n        cache_dir = os.path.join(home_dir, f\".docetl/cache/{yaml_file_suffix}\")\n        os.makedirs(cache_dir, exist_ok=True)\n        self.datasets = DatasetOnDisk(dir=cache_dir, console=self.console)\n        self.optimized_ops_path = f\"{cache_dir}/optimized_ops\"\n        self.optimized_config_path = f\"{base_name}_opt.yaml\"\n\n        # Update sample size map\n        self.sample_size_map = SAMPLE_SIZE_MAP\n        if self.config.get(\"optimizer_config\", {}).get(\"sample_sizes\", {}):\n            self.sample_size_map.update(self.config[\"optimizer_config\"][\"sample_sizes\"])\n\n        self.step_op_to_optimized_ops = {}\n\n        self.print_optimizer_config()\n\n    def find_operation(self, op_name: str, config: Optional[Dict] = None) -&gt; Dict:\n        if not config:\n            config = self.config\n        for operation_config in config[\"operations\"]:\n            if operation_config[\"name\"] == op_name:\n                return operation_config\n        raise ValueError(f\"Operation '{op_name}' not found in configuration.\")\n\n    def syntax_check(self):\n        \"\"\"\n        Perform a syntax check on all operations defined in the configuration.\n\n        This method validates each operation by attempting to instantiate it.\n        If any operation fails to instantiate, a ValueError is raised.\n\n        Raises:\n            ValueError: If any operation fails the syntax check.\n        \"\"\"\n        for operation_config in self.config[\"operations\"]:\n            operation = operation_config[\"name\"]\n            operation_type = operation_config[\"type\"]\n\n            try:\n                operation_class = get_operation(operation_type)\n                operation_class(\n                    self,\n                    operation_config,\n                    self.config.get(\"default_model\", \"gpt-4o-mini\"),\n                    self.max_threads,\n                    console=self.console,\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Syntax check failed for operation '{operation}': {str(e)}\"\n                )\n\n        self.console.log(\"[green]Syntax check passed for all operations.[/green]\")\n\n    def print_optimizer_config(self):\n        \"\"\"\n        Print the current configuration of the optimizer.\n\n        This method uses the Rich console to display a formatted output of the optimizer's\n        configuration. It includes details such as the YAML file path, sample sizes for\n        different operation types, maximum number of threads, the language model being used,\n        and the timeout setting.\n\n        The output is color-coded and formatted for easy readability, with a header and\n        separator lines to clearly delineate the configuration information.\n        \"\"\"\n        self.console.rule(\"[bold cyan]Optimizer Configuration[/bold cyan]\")\n        self.console.log(f\"[yellow]Sample Size:[/yellow] {self.sample_size_map}\")\n        self.console.log(f\"[yellow]Max Threads:[/yellow] {self.max_threads}\")\n        self.console.log(f\"[yellow]Model:[/yellow] {self.llm_client.model}\")\n        self.console.log(f\"[yellow]Timeout:[/yellow] {self.timeout} seconds\")\n\n    def compute_sample_size(\n        self,\n        step_name: str,\n        step_ops: List[str],\n        op_config: Dict[str, Any],\n    ) -&gt; int:\n        \"\"\"\n        Compute the sample size necessary for optimizing given operation based on upstream operations.\n\n        This method calculates an appropriate sample size for an operation, taking into\n        account the selectivities of upstream operations in the same step. It uses a\n        predefined sample size map (SAMPLE_SIZE_MAP) as a starting point.\n\n        For example, if we have a 'map' operation with a default sample size of 10,\n        and one upstream operation with a selectivity of 0.5, the computed sample size for the upstream operation would be:\n        10 / 0.5 = 20\n\n        This ensures that after applying the selectivity of the upstream operation,\n        we still have a representative sample size for the current operation.\n\n        Args:\n            step_name (str): The name of the current step in the pipeline.\n            step_ops (List[str]): A list of all operations in the current step.\n            op_config (Dict[str, Any]): The configuration dictionary for the current operation.\n\n        Returns:\n            int: The computed sample size for the operation.\n\n        The method works as follows:\n        1. If there are no upstream operations, it returns the default sample size for the operation type.\n        2. Otherwise, it starts with the default sample size and adjusts it based on the selectivities\n           of upstream operations.\n        3. It iterates through upstream operations in reverse order, dividing the sample size by\n           each operation's selectivity.\n        4. The final result is rounded to the nearest integer.\n\n        Raises:\n            ValueError: If the selectivity for any upstream operation is not found.\n\n        Note:\n            - The method assumes that selectivities for all upstream operations have been\n              previously computed and stored in self.selectivities.\n            - The sample size is always at least 1, even after all adjustments.\n        \"\"\"\n        # If an equijoin, load the default. Equijoins are always first\n        if op_config.get(\"type\") == \"equijoin\":\n            return SAMPLE_SIZE_MAP.get(op_config.get(\"type\"))\n\n        # If there are no upstream operations, use the default sample_size\n        upstream_ops = []\n        for step_op in step_ops:\n            if step_op != op_config.get(\"name\"):\n                if step_op in self.step_op_to_optimized_ops:\n                    upstream_ops.extend(self.step_op_to_optimized_ops[step_op])\n                else:\n                    upstream_ops.append(step_op)\n            else:\n                break\n\n        if len(upstream_ops) == 0:\n            return self.sample_size_map.get(op_config.get(\"type\"), float(\"inf\"))\n\n        # Otherwise, compute the sample size based on the upstream operations\n        sample_size = self.sample_size_map.get(op_config.get(\"type\"), 100)\n        for op in reversed(upstream_ops):\n            # Use the selectivity of the upstream operation to compute the sample size\n            if op not in self.selectivities[step_name]:\n                raise ValueError(\n                    f\"Selectivity for operation {op} not found in selectivities. Other ops are {self.selectivities[step_name]}\"\n                )\n\n            sample_size = sample_size / self.selectivities[step_name].get(op)\n\n        return int(math.ceil(sample_size))\n\n    def _insert_empty_resolve_operations(self):\n        \"\"\"\n        Determines whether to insert resolve operations in the pipeline.\n\n        This method iterates through each step in the pipeline and checks if there's a reduce\n        operation that follows a map operation with no resolver in between. If such a case is\n        found, it synthesizes an empty resolver operation and inserts it into the pipeline.\n\n        The method modifies the pipeline configuration in-place.\n\n        Returns:\n            None\n\n        Side effects:\n        - Modifies self.config[\"pipeline\"][\"steps\"] by potentially inserting new resolve operations.\n        - Adds new resolve operations to self.config[\"operations\"] if necessary.\n        \"\"\"\n        for i, step in enumerate(self.config[\"pipeline\"][\"steps\"]):\n            operations = step.get(\"operations\", [])\n            has_map = False\n            has_reduce = False\n            has_resolve = False\n            map_op = None\n            reduce_op = None\n\n            for op in operations:\n                if isinstance(op, dict):\n                    op = list(op.keys())[0]\n                op_config = self.find_operation(op)\n                op_type = op_config[\"type\"]\n                if op_type == \"map\":\n                    has_map = True\n                    map_op = op\n                elif op_type == \"reduce\" and op_config.get(\"synthesize_resolve\", True):\n                    has_reduce = True\n                    reduce_op = op\n                elif op_type == \"resolve\":\n                    has_resolve = True\n\n            if has_map and has_reduce and not has_resolve:\n                # Synthesize an empty resolver\n                self.console.log(\n                    \"[yellow]Synthesizing empty resolver operation:[/yellow]\"\n                )\n                self.console.log(\n                    f\"  \u2022 [cyan]Reduce operation:[/cyan] [bold]{reduce_op}[/bold]\"\n                )\n                self.console.log(f\"  \u2022 [cyan]Step:[/cyan] [bold]{step['name']}[/bold]\")\n\n                new_resolve_op = f\"synthesized_resolve_{i}\"\n                reduce_key = self.find_operation(reduce_op).get(\"reduce_key\")\n                if isinstance(reduce_key, str):\n                    reduce_key = [reduce_key]\n                self.config[\"operations\"].append(\n                    {\n                        \"name\": new_resolve_op,\n                        \"type\": \"resolve\",\n                        \"empty\": True,\n                        \"optimize\": True,\n                        \"embedding_model\": \"text-embedding-3-small\",\n                        \"resolution_model\": self.config.get(\n                            \"default_model\", \"gpt-4o-mini\"\n                        ),\n                        \"comparison_model\": self.config.get(\n                            \"default_model\", \"gpt-4o-mini\"\n                        ),\n                        \"_intermediates\": {\n                            \"map_prompt\": self.find_operation(map_op).get(\"prompt\"),\n                            \"reduce_key\": reduce_key,\n                        },\n                    }\n                )\n\n                # Insert the new resolve operation before the reduce operation\n                reduce_index = next(\n                    i\n                    for i, op in enumerate(operations)\n                    if self.find_operation(op).get(\"type\") == \"reduce\"\n                )\n                operations.insert(reduce_index, new_resolve_op)\n\n                has_resolve = True\n\n            self.config[\"pipeline\"][\"steps\"][i][\"operations\"] = operations\n\n        # Update the pipeline configuration\n        self.config[\"pipeline\"][\"steps\"] = self.config[\"pipeline\"][\"steps\"]\n\n    def _add_map_prompts_to_reduce_operations(self):\n        \"\"\"\n        Add relevant map prompts to reduce operations based on their reduce keys.\n\n        This method iterates through all map operations to create a dictionary mapping\n        output schema keys to map prompts. It then loops through reduce operations,\n        adding the relevant map prompts based on the reduce keys and output schema.\n\n        Side effects:\n        - Modifies reduce operations in self.config[\"operations\"] by adding map prompts.\n        \"\"\"\n        # Create a dictionary mapping output schema keys to map prompts\n        output_key_to_prompt = {}\n        for op_config in self.config[\"operations\"]:\n            if op_config.get(\"type\") == \"map\":\n                output_schema = op_config.get(\"output\", {}).get(\"schema\", {})\n                prompt = op_config.get(\"prompt\", \"\")\n                for key in output_schema.keys():\n                    output_key_to_prompt[key] = prompt\n\n        # Add relevant map prompts to reduce operations\n        for op_config in self.config[\"operations\"]:\n            if op_config.get(\"type\") == \"reduce\":\n                reduce_keys = op_config.get(\"reduce_key\", [])\n                if isinstance(reduce_keys, str):\n                    reduce_keys = [reduce_keys]\n\n                relevant_prompts = []\n                for key in reduce_keys:\n                    if key in output_key_to_prompt:\n                        relevant_prompts.append(output_key_to_prompt[key])\n\n                if relevant_prompts:\n                    op_config[\"_intermediates\"] = op_config.get(\"_intermediates\", {})\n                    op_config[\"_intermediates\"][\"last_map_prompt\"] = relevant_prompts[\n                        -1\n                    ]\n\n    def _load_optimized_ops(self):\n        \"\"\"\n        Load the optimized operations from disk.\n        \"\"\"\n        if os.path.exists(self.optimized_ops_path):\n            for filename in os.listdir(self.optimized_ops_path):\n                if filename.endswith(\".json\"):\n                    original_op_name = filename[:-5]  # Remove '.json' from the filename\n                    with open(\n                        os.path.join(self.optimized_ops_path, filename), \"r\"\n                    ) as f:\n                        optimized_ops = json.load(f)\n\n                    # Update the config with the optimized operations\n                    if original_op_name in [\n                        op[\"name\"] for op in self.config[\"operations\"]\n                    ]:\n                        # Update the config with the optimized operations\n                        # First, remove all operations that are already in the config with the same name\n                        self.config[\"operations\"] = [\n                            op\n                            for op in self.config[\"operations\"]\n                            if op[\"name\"] != original_op_name\n                        ]\n\n                        for op in optimized_ops:\n                            op[\"optimize\"] = False\n                            self.config[\"operations\"].append(op)\n\n                        # Update the step operations\n                        for step in self.config[\"pipeline\"][\"steps\"]:\n                            if original_op_name in step[\"operations\"]:\n                                index = step[\"operations\"].index(original_op_name)\n                                step[\"operations\"] = (\n                                    step[\"operations\"][:index]\n                                    + [op[\"name\"] for op in optimized_ops]\n                                    + step[\"operations\"][index + 1 :]\n                                )\n\n                    self.console.log(\n                        f\"Loaded optimized operations for {original_op_name}\"\n                    )\n\n            self.console.log(\"[green]Finished loading optimized operations[/green]\")\n\n            # Print out the operations for each step\n            self.console.log(\"[bold blue]Operations for each step:[/bold blue]\")\n            for step in self.config[\"pipeline\"][\"steps\"]:\n                step_name = step.get(\"name\")\n                operations = step.get(\"operations\", [])\n                self.console.log(f\"[cyan]Step: {step_name}[/cyan]\")\n                for op in operations:\n                    if isinstance(op, dict):\n                        # Handle the case where the operation is a dictionary (e.g., for equijoin)\n                        op_name = list(op.keys())[0]\n                        op_details = op[op_name]\n                        self.console.log(f\"  - {op_name}: {op_details}\")\n                    else:\n                        self.console.log(f\"  - {op}\")\n                self.console.log(\"\")  # Add a blank line between steps\n        else:\n            self.console.log(\"[yellow]No optimized operations found[/yellow]\")\n\n    def optimize(self):\n        \"\"\"\n        Optimize the entire pipeline defined in the configuration.\n\n        This method is the main entry point for the optimization process. It iterates through\n        each step in the pipeline, optimizing from upstream to downstream, and constructs an\n        optimized version of the configuration.\n\n        The optimization process includes:\n        1. Iterating through each step in the pipeline, from upstream to downstream.\n        2. Optimizing each step using the _optimize_step method.\n        3. Updating the optimized configuration with the new operations and steps.\n        4. Saving the optimized configuration to a file.\n        5. Logging the total costs (agent cost, operations cost, and total cost).\n\n        Returns:\n            None\n\n        Side effects:\n        - Modifies self.optimized_config with the optimized pipeline and operations.\n        - Updates self.datasets with the results of each step.\n        - Calls _save_optimized_config to save the optimized configuration to a file.\n        - Logs cost information to the console.\n\n        Raises:\n            ValueError: If a step in the pipeline does not have a name.\n\n        Note:\n        - This method assumes that all necessary data and configurations are already\n          loaded and initialized in the Optimizer instance.\n        - The optimization process is performed step by step, from upstream to downstream,\n          with each step potentially depending on the results of previous steps.\n        \"\"\"\n        self.console.rule(\"[bold cyan]Beginning Pipeline Optimization[/bold cyan]\")\n\n        self.syntax_check()\n\n        self._insert_empty_resolve_operations()\n\n        # If resume is True, load the optimized operations from disk\n        if self.resume:\n            self._load_optimized_ops()\n\n        for step in self.config[\"pipeline\"][\"steps\"]:\n            step_name = step.get(\"name\")\n            if not step_name:\n                raise ValueError(\n                    \"Step does not have a name. Each step must have a unique name.\"\n                )\n\n            optimized_step, step_operations, input_data = self._optimize_step(step)\n            old_op_names = [\n                op\n                for op in step[\"operations\"]\n                if op not in optimized_step[\"operations\"]\n            ]\n\n            # Remove all old_op_names from self.optimized_config[\"operations\"]\n            self.optimized_config[\"operations\"] = [\n                op\n                for op in self.optimized_config[\"operations\"]\n                if op[\"name\"] not in old_op_names\n            ]\n\n            for op in optimized_step[\"operations\"]:\n                changed_op = False\n                for i, op_config in enumerate(self.optimized_config[\"operations\"]):\n                    if op_config[\"name\"] == op:\n                        self.optimized_config[\"operations\"][i] = step_operations[op]\n                        changed_op = True\n                if not changed_op:\n                    self.optimized_config[\"operations\"].append(step_operations[op])\n\n            self.optimized_config[\"pipeline\"][\"steps\"] = [\n                step\n                for step in self.optimized_config[\"pipeline\"][\"steps\"]\n                if step[\"name\"] != step_name\n            ] + [optimized_step]\n\n            self.step_op_to_optimized_ops[step_name] = optimized_step[\"operations\"]\n\n            step_hash = (\n                hashlib.md5(\n                    json.dumps(\n                        {\n                            \"step\": [\n                                s\n                                for s in self.optimized_config[\"pipeline\"][\"steps\"]\n                                if s[\"name\"] == step_name\n                            ][0],\n                            \"operations\": [\n                                self.find_operation(op, self.optimized_config)\n                                for op in optimized_step[\"operations\"]\n                            ],\n                        }\n                    ).encode()\n                ).hexdigest()\n                + \".json\"\n            )\n            # If the dataset already exists, skip the step\n            if step_hash in self.datasets:\n                continue\n\n            flush_cache(self.console)\n\n            if step_name in self.config.get(\"optimizer_config\", {}).get(\n                \"run_full_step\", []\n            ):\n                # Run the entire step\n                input_data = self._run_partial_step(\n                    step,\n                    step_operations,\n                    float(\"inf\"),  # TODO: FIX THIS\n                )\n                self.datasets[step_hash] = copy.deepcopy(input_data)\n            else:\n                self.datasets[step_hash] = copy.deepcopy(input_data)\n\n        self.console.log(\n            f\"[bold]Total agent cost: ${self.llm_client.total_cost:.2f}[/bold]\"\n        )\n        self.console.log(\n            f\"[bold]Total operations cost: ${self.operations_cost:.2f}[/bold]\"\n        )\n        self.console.log(\n            f\"[bold]Total cost: ${self.llm_client.total_cost + self.operations_cost:.2f}[/bold]\"\n        )\n\n    def _run_partial_step(\n        self,\n        step: Dict[str, Any],\n        ops_to_run: List[str],\n        sample_size: int,\n        optimized_operations: Dict[str, Dict[str, Any]],\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Execute a partial step of the pipeline on a sample of the input data.\n\n        This internal method runs a subset of operations for a given step on a sample\n        of the input data. It's used as part of the optimization process to evaluate\n        and optimize individual operations within a step.\n\n        Args:\n            step (Dict[str, Any]): The step configuration dictionary.\n            ops_to_run (List[str]): List of operation names to execute in this partial step.\n            sample_size (int): The number of items to include in the input sample.\n            optimized_operations (Dict[str, Dict[str, Any]]): Dictionary of optimized operations.\n\n        Returns:\n            List[Dict[str, Any]]: The output data after running the specified operations.\n\n        The method performs the following steps:\n        1. Retrieves a sample of the input data using _get_sample_data.\n        2. For equijoin operations, it loads both left and right datasets.\n        3. Iterates through the specified operations, running each on the input sample.\n        4. Returns the final output after all specified operations have been applied.\n\n        Note:\n        - The method handles both regular steps and equijoin steps differently.\n\n        Raises:\n            Any exceptions raised by _get_sample_data or _run_operation methods.\n        \"\"\"\n        # Take the input data and run the operations in ops_to_run\n        # Return the output data\n        input_sample = self._get_sample_data(step.get(\"input\"), None, sample_size)\n\n        if step.get(\"input\") is None:\n            join_op_name = list(step.get(\"operations\")[0].keys())[0]\n            # this is an equijoin step, load left and right datasets\n            left_data = self._get_sample_data(\n                step.get(\"operations\")[0][join_op_name].get(\"left\"), None, sample_size\n            )\n            right_data = self._get_sample_data(\n                step.get(\"operations\")[0][join_op_name].get(\"right\"), None, sample_size\n            )\n            input_sample = {\"left\": left_data, \"right\": right_data}\n\n        for op in ops_to_run:\n            op_object = optimized_operations[op]\n            if \"name\" not in op_object:\n                op_object[\"name\"] = op\n\n            input_sample = self._run_operation(op_object, input_sample)\n        return input_sample\n\n    def _optimize_step(\n        self, step: Dict[str, Any]\n    ) -&gt; Tuple[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"\n        Optimize a single step in the pipeline.\n\n        This method takes a step configuration and optimizes each operation within it.\n        It handles different types of operations, including those that require optimization\n        and those that don't.\n\n        Args:\n            step (Dict[str, Any]): The configuration dictionary for the step to be optimized.\n\n        Returns:\n            Tuple[Dict[str, Any], List[Dict[str, Any]], List[Dict[str, Any]]]:\n                - The optimized step configuration.\n                - A list of optimized operations.\n                - The output data after running all operations in the step.\n\n        The method performs the following for each operation in the step:\n        1. Extracts the operation configuration.\n        2. Computes the appropriate sample size for the operation.\n        3. Runs the operation on a sample of the input data.\n        4. If the operation is optimizable and of a supported type, it calls the appropriate\n           optimization method (e.g., _optimize_map, _optimize_reduce).\n        5. If not optimizable or not supported, it runs the operation as-is.\n        6. Calculates and stores the selectivity of each operation.\n        7. Updates the list of optimized operations and their configurations.\n\n        The method uses rich console to provide status updates during the optimization process.\n\n        Note:\n        - This method is a key part of the overall optimization process, focusing on\n          individual steps in the pipeline.\n        - It relies on several helper methods like _run_partial_step, compute_sample_size,\n          and various _optimize_* methods for specific operation types.\n        - When optimizing an operation in the step, all previous operations are run on the\n          sample size needed for the current operation. This ensures that the input to the\n          operation being optimized is representative of what it would receive in the full pipeline.\n\n        Raises:\n            ValueError: If an unsupported operation type is encountered.\n        \"\"\"\n        optimized_operations = {}\n        optimized_operation_names = []\n        replacement_operations = {}  # List from old op name to new ops\n\n        for op_idx, operation in enumerate(step[\"operations\"]):\n            if isinstance(operation, dict):\n                operation_name = list(operation.keys())[0]\n                operation_config = operation[operation_name]\n            else:\n                operation_name = operation\n                operation_config = {}\n\n            op_object = self.find_operation(operation_name).copy()\n            op_object.update(operation_config)\n            op_object[\"name\"] = operation_name\n\n            # Run the pipeline\n            step_ops = []\n            for step_op in step.get(\"operations\"):\n                if step_op in replacement_operations:\n                    step_ops.extend(replacement_operations[step_op])\n                else:\n                    step_ops.append(step_op)\n\n            # TODO: incorporate this into the optimizer to not run the most downstream operations\n            downstream_ops_exist = op_idx &lt; len(step[\"operations\"]) - 1\n\n            sample_size = self.compute_sample_size(\n                step.get(\"name\"), step_ops, op_object\n            )\n            input_data = self._run_partial_step(\n                step, optimized_operation_names, sample_size, optimized_operations\n            )\n\n            if (\n                not op_object.get(\"optimize\", False)  # Default don't optimize\n                or op_object.get(\"type\") not in SUPPORTED_OPS\n            ):\n                # If optimize is False or operation type is not supported, just use the operation without optimization\n                output_data = self._run_operation(op_object, input_data)\n                optimized_operations[operation_name] = op_object\n                optimized_operation_names.append(operation_name)\n\n                selectivity = len(output_data) / len(input_data)\n\n                self.selectivities[step.get(\"name\")][operation_name] = selectivity\n                self.samples_taken[step.get(\"name\")][operation_name] = sample_size\n            else:\n                # Use rich console status to indicate optimization of the operation\n                with self.console.status(\n                    f\"[bold blue]Optimizing operation: {operation_name} (Type: {op_object['type']})[/bold blue]\"\n                ) as status:\n                    self.status = status\n\n                    # Print the number of elements in input_data\n                    self.console.rule(\n                        f\"[yellow]Optimizing operation {operation_name} (Type: {op_object['type']})[/yellow]\"\n                    )\n                    if op_object.get(\"type\") == \"equijoin\":\n                        self.console.log(\n                            f\"[yellow]  Sample size (left): {len(input_data['left'])}[/yellow]\"\n                        )\n                        self.console.log(\n                            f\"[yellow]  Sample size (right): {len(input_data['right'])}[/yellow]\"\n                        )\n                    else:\n                        self.console.log(\n                            f\"[yellow]  Sample size: {len(input_data)}[/yellow]\"\n                        )\n\n                    # Run optimization\n                    for retry in range(\n                        self.config.get(\"optimizer_config\", {}).get(\n                            \"num_retries\", NUM_OPTIMIZER_RETRIES\n                        )\n                    ):\n                        try:\n                            if op_object.get(\"type\") == \"map\":\n                                optimized_ops = self._optimize_map(\n                                    op_object, input_data\n                                )\n                            elif op_object.get(\"type\") == \"filter\":\n                                optimized_ops = self._optimize_map(\n                                    op_object, input_data, is_filter=True\n                                )\n                            elif op_object.get(\"type\") == \"reduce\":\n                                optimized_ops = self._optimize_reduce(\n                                    op_object, input_data, status\n                                )\n                            elif op_object.get(\"type\") == \"resolve\":\n                                optimized_ops = self._optimize_resolve(\n                                    op_object, input_data\n                                )\n                            elif op_object.get(\"type\") == \"equijoin\":\n                                (\n                                    optimized_ops,\n                                    input_data,\n                                    new_left_name,\n                                    new_right_name,\n                                ) = self._optimize_equijoin(\n                                    op_object,\n                                    operation[\"left\"],\n                                    operation[\"right\"],\n                                    input_data[\"left\"],\n                                    input_data[\"right\"],\n                                    status,\n                                )\n                            else:\n                                raise ValueError(\n                                    f\"Unsupported operation type: {op_object['type']}\"\n                                )\n                            break  # If successful, break out of the retry loop\n                        except Exception as e:\n                            if (\n                                retry\n                                == self.config.get(\"optimizer_config\", {}).get(\n                                    \"num_retries\", NUM_OPTIMIZER_RETRIES\n                                )\n                                - 1\n                            ):\n                                raise  # If this was the last retry, re-raise the exception\n                            self.console.log(\n                                f\"Optimization attempt {retry + 1} failed. Retrying...\"\n                            )\n\n                    if self.status:\n                        self.status.update(\n                            f\"[bold blue]Running optimized operation to estimate selectivities: {operation_name}[/bold blue]\"\n                        )\n\n                    for op in optimized_ops:\n                        op_name = op[\"name\"]\n                        optimized_operations[op_name] = op\n                        if op.get(\"type\") == \"equijoin\":\n                            optimized_operation_names.append(\n                                {\n                                    op_name: {\n                                        \"left\": new_left_name,\n                                        \"right\": new_right_name,\n                                    }\n                                }\n                            )\n                        else:\n                            optimized_operation_names.append(op_name)\n\n                        old_input_data_size = len(input_data)\n                        input_data = self._run_operation(op, input_data)\n                        new_input_data_size = len(input_data)\n                        selectivity = new_input_data_size / old_input_data_size\n                        self.selectivities[step.get(\"name\")][op_name] = selectivity\n                        self.samples_taken[step.get(\"name\")][op_name] = sample_size\n\n                    # Set replacement_operations\n                    replacement_operations[op_object[\"name\"]] = [\n                        o[\"name\"] for o in optimized_ops\n                    ]\n\n                    # Print new operator configs\n                    self.console.log(\"[bold green]New op configurations:[/bold green]\")\n                    for op_name, op_config in optimized_operations.items():\n                        if op_name in [o[\"name\"] for o in optimized_ops]:\n                            self.console.log(\n                                f\"[cyan]{op_name}:[/cyan] {json.dumps(op_config, indent=2)}\"\n                            )\n\n                    # Save the optimized operations to disk\n                    os.makedirs(self.optimized_ops_path, exist_ok=True)\n\n                    for original_op, replacement_ops in replacement_operations.items():\n                        optimized_ops_list = [\n                            (\n                                optimized_operations[op_name]\n                                if isinstance(op_name, str)\n                                else {\n                                    list(op_name.keys())[0]: optimized_operations[\n                                        list(op_name.keys())[0]\n                                    ]\n                                }\n                            )\n                            for op_name in replacement_ops\n                        ]\n\n                        # Save to disk\n                        optimized_op_file = os.path.join(\n                            self.optimized_ops_path, f\"{original_op}.json\"\n                        )\n                        with open(optimized_op_file, \"w\") as f:\n                            json.dump(optimized_ops_list, f, indent=2)\n\n                    self.console.log(\n                        f\"[green]Saved optimized operations to {self.optimized_ops_path}[/green]\"\n                    )\n                    self.status = None\n                    output_data = input_data\n\n        optimized_step = step.copy()\n        optimized_step[\"operations\"] = optimized_operation_names\n        return optimized_step, optimized_operations, output_data\n\n    def _get_sample_data(\n        self, dataset_name: str, op_config: Optional[Dict[str, Any]], sample_size: int\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Retrieve a sample of data from a specified dataset.\n\n        This method loads data from either a previously processed dataset or from a file,\n        and returns a sample of the data based on the given sample size and operation configuration.\n\n        Args:\n            dataset_name (str): The name of the dataset to sample from.\n            op_config (Optional[Dict[str, Any]]): The configuration of the operation to be performed.\n                                                  This is used to determine if special sampling is needed.\n            sample_size (int): The desired size of the sample. If set to float('inf'), all data is returned.\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the sampled data.\n\n        Raises:\n            ValueError: If the dataset is not found or if the dataset type is unsupported.\n        \"\"\"\n        if dataset_name is None:\n            return []\n\n        if any(\n            s[\"name\"] == dataset_name\n            for s in self.optimized_config[\"pipeline\"][\"steps\"]\n        ):\n            step = [\n                s\n                for s in self.optimized_config[\"pipeline\"][\"steps\"]\n                if s[\"name\"] == dataset_name\n            ][0]\n            name_hash = (\n                hashlib.md5(\n                    json.dumps(\n                        {\n                            \"step\": step,\n                            \"operations\": [\n                                self.find_operation(op) for op in step[\"operations\"]\n                            ],\n                        }\n                    ).encode()\n                ).hexdigest()\n                + \".json\"\n            )\n        else:\n            name_hash = None\n\n        if name_hash and name_hash in self.datasets:\n            data = self.datasets[name_hash]\n        else:\n            dataset_config = self.config[\"datasets\"].get(dataset_name)\n            if dataset_config is None:\n                raise ValueError(\n                    f\"Dataset '{dataset_name}' not found in config or previous steps.\"\n                )\n            dataset = Dataset(\n                runner=self,\n                type=dataset_config[\"type\"],\n                path_or_data=dataset_config[\"path\"],\n                parsing=dataset_config.get(\"parsing\", []),\n                user_defined_parsing_tool_map=self.parsing_tool_map,\n            )\n            data = dataset.load()\n\n        if sample_size == float(\"inf\"):\n            return data\n\n        if op_config:\n            if op_config.get(\"type\") == \"reduce\":\n                return self._get_reduce_sample(\n                    data, op_config.get(\"reduce_key\"), sample_size\n                )\n\n        # Take the random 500 examples or all if less than 500\n        initial_data = random.sample(data, min(500, len(data)))\n\n        # Calculate counts for each example\n        char_counts = [len(str(item)) for item in initial_data]\n        total_counts = sum(char_counts)\n\n        # Calculate weights based on word counts\n        weights = [count / total_counts for count in char_counts]\n\n        # Perform weighted random sampling\n        return random.choices(\n            initial_data, weights=weights, k=min(sample_size, len(initial_data))\n        )\n\n    def _get_reduce_sample(\n        self, data: List[Dict[str, Any]], reduce_key: str, sample_size: int\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get a representative sample for a reduce operation.\n\n        This method creates a sample that preserves the distribution of groups in the data,\n        focusing on the top 5 largest groups. It also generates and prints a histogram of group sizes.\n\n        Args:\n            data (List[Dict[str, Any]]): The full dataset to sample from.\n            reduce_key (str): The key used for grouping in the reduce operation.\n            sample_size (int): The desired size of the sample.\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries representing the sampled data.\n        \"\"\"\n        # Group data by reduce key\n        grouped_data = defaultdict(list)\n        for item in data:\n            grouped_data[item[reduce_key]].append(item)\n\n        # Sort groups by size in descending order\n        sorted_groups = sorted(\n            grouped_data.items(), key=lambda x: len(x[1]), reverse=True\n        )\n\n        sample = []\n\n        # Take the top 5 groups\n        top_5_groups = sorted_groups[:5]\n\n        # Calculate the total count of items in the top 5 groups\n        total_count = sum(len(items) for _, items in top_5_groups)\n\n        sample = []\n        for _, items in top_5_groups:\n            # Calculate the proportion of items to sample from this group\n            group_proportion = len(items) / total_count\n            group_sample_size = int(sample_size * group_proportion)\n\n            # Sample from the group\n            group_sample = random.sample(items, min(group_sample_size, len(items)))\n            sample.extend(group_sample)\n\n        # If we haven't reached the desired sample size, add more items randomly\n        if len(sample) &lt; sample_size:\n            remaining_items = [\n                item\n                for _, items in top_5_groups\n                for item in items\n                if item not in sample\n            ]\n            additional_sample = random.sample(\n                remaining_items,\n                min(sample_size - len(sample), len(remaining_items)),\n            )\n            sample.extend(additional_sample)\n\n        # Add items randomly from non-top groups to meet the sample size\n        if len(sample) &lt; sample_size:\n            remaining_items = [\n                item\n                for _, items in grouped_data.items()\n                for item in items\n                if item not in sample\n            ]\n            additional_sample = random.sample(\n                remaining_items,\n                min(sample_size - len(sample), len(remaining_items)),\n            )\n            sample.extend(additional_sample)\n\n        # Create a histogram of group sizes\n        group_sizes = [len(items) for _, items in grouped_data.items()]\n        size_counts = Counter(group_sizes)\n\n        # Sort the sizes for a more readable output\n        sorted_sizes = sorted(size_counts.items())\n\n        # Print the histogram\n        self.console.log(\"\\n[bold]Histogram of Group Sizes:[/bold]\")\n        max_bar_width, max_count = 2, max(size_counts.values())\n        for size, count in sorted_sizes[:5]:\n            normalized_count = int(count / max_count * max_bar_width)\n            bar = \"\u2588\" * normalized_count\n            self.console.log(f\"{size:3d}: {bar} ({count})\")\n        self.console.log(\"\\n\")\n\n        return sample\n\n    def _optimize_reduce(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        status: Status,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Optimize a reduce operation.\n\n        This method creates a ReduceOptimizer instance and uses it to optimize the reduce operation.\n\n        Args:\n            op_config (Dict[str, Any]): The configuration of the reduce operation.\n            input_data (List[Dict[str, Any]]): The input data for the reduce operation.\n            status (Status): The status object to update with the progress of the optimization.\n\n        Returns:\n            List[Dict[str, Any]]: The optimized operation configuration.\n        \"\"\"\n        reduce_optimizer = ReduceOptimizer(\n            self,\n            self.config,\n            self.console,\n            self.llm_client,\n            self.max_threads,\n            self._run_operation,\n            status=status,\n        )\n        optimized_ops, _, cost = reduce_optimizer.optimize(op_config, input_data)\n        self.operations_cost += cost\n        return optimized_ops\n\n    def _optimize_equijoin(\n        self,\n        op_config: Dict[str, Any],\n        left_name: str,\n        right_name: str,\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n        status: Status,\n    ) -&gt; Tuple[List[Dict[str, Any]], Dict[str, List[Dict[str, Any]]], str, str]:\n        \"\"\"\n        Optimize an equijoin operation.\n\n        This method creates a JoinOptimizer instance and uses it to optimize the equijoin operation.\n        It updates the operation cost and runs the optimized operation.\n        If the LLM suggests a map transformation, it will optimize the map operation as its own step, and then go back to optimize the equijoin operation.\n\n        Args:\n            op_config (Dict[str, Any]): The configuration of the equijoin operation.\n            left_name (str): The name of the left dataset.\n            right_name (str): The name of the right dataset.\n            left_data (List[Dict[str, Any]]): The left dataset for the join.\n            right_data (List[Dict[str, Any]]): The right dataset for the join.\n\n        Returns:\n            Tuple[List[Dict[str, Any]], Dict[str, List[Dict[str, Any]]], str, str]: The optimized operation configuration, the new left and right datasets, and the new left and right names.\n        \"\"\"\n        max_iterations = 2\n        new_left_name = left_name\n        new_right_name = right_name\n        for _ in range(max_iterations):\n            join_optimizer = JoinOptimizer(\n                self,\n                self.config,\n                op_config,\n                self.console,\n                self.llm_client,\n                self.max_threads,\n                target_recall=self.config.get(\"optimizer_config\", {})\n                .get(\"equijoin\", {})\n                .get(\"target_recall\", 0.95),\n                estimated_selectivity=self.config.get(\"optimizer_config\", {})\n                .get(\"equijoin\", {})\n                .get(\"estimated_selectivity\", None),\n                status=status,\n            )\n            optimized_config, cost, agent_results = join_optimizer.optimize_equijoin(\n                left_data, right_data\n            )\n            self.operations_cost += cost\n            # Update the operation config with the optimized values\n            op_config.update(optimized_config)\n\n            if not agent_results.get(\"optimize_map\", False):\n                break  # Exit the loop if no more map optimizations are necessary\n\n            # Update the status to indicate we're optimizing a map operation\n            output_key = agent_results[\"output_key\"]\n            if self.status:\n                self.status.update(\n                    f\"Optimizing map operation for {output_key} extraction to help with the equijoin\"\n                )\n            map_prompt = agent_results[\"map_prompt\"]\n            dataset_to_transform = (\n                left_data\n                if agent_results[\"dataset_to_transform\"] == \"left\"\n                else right_data\n            )\n\n            # Create a new step for the map operation\n            map_operation = {\n                \"name\": f\"synthesized_{output_key}_extraction\",\n                \"type\": \"map\",\n                \"prompt\": map_prompt,\n                \"model\": self.config.get(\"default_model\", \"gpt-4o-mini\"),\n                \"output\": {\"schema\": {output_key: \"string\"}},\n                \"optimize\": False,\n            }\n\n            # Optimize the map operation\n            if map_operation[\"optimize\"]:\n                dataset_to_transform_sample = random.sample(\n                    dataset_to_transform, self.sample_size_map.get(\"map\")\n                )\n                optimized_map_operations = self._optimize_map(\n                    map_operation, dataset_to_transform_sample\n                )\n            else:\n                optimized_map_operations = [map_operation]\n\n            new_step = {\n                \"name\": f\"synthesized_{output_key}_extraction\",\n                \"input\": (\n                    left_name\n                    if agent_results[\"dataset_to_transform\"] == \"left\"\n                    else right_name\n                ),\n                \"operations\": [mo[\"name\"] for mo in optimized_map_operations],\n            }\n            if agent_results[\"dataset_to_transform\"] == \"left\":\n                new_left_name = new_step[\"name\"]\n            else:\n                new_right_name = new_step[\"name\"]\n\n            for optimized_map_op in optimized_map_operations:\n                self.optimized_config[\"operations\"].append(optimized_map_op)\n\n            self.optimized_config[\"pipeline\"][\"steps\"].append(new_step)\n\n            # Now run the optimized map operation on the entire dataset_to_transform\n            for op in optimized_map_operations:\n                dataset_to_transform = self._run_operation(op, dataset_to_transform)\n\n            # Update the appropriate dataset for the next iteration\n            if agent_results[\"dataset_to_transform\"] == \"left\":\n                left_data = dataset_to_transform\n            else:\n                right_data = dataset_to_transform\n\n            if self.status:\n                self.status.update(\n                    f\"Optimizing equijoin operation with {output_key} extraction\"\n                )\n\n        # Pop off \"left\" and \"right\" from the op_config\n        op_config.pop(\"left\")\n        op_config.pop(\"right\")\n        return (\n            [op_config],\n            {\"left\": left_data, \"right\": right_data},\n            new_left_name,\n            new_right_name,\n        )\n\n    def _optimize_map(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        is_filter: bool = False,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Optimize a map operation.\n\n        This method creates a MapOptimizer instance and uses it to optimize the map operation.\n\n        Args:\n            op_config (Dict[str, Any]): The configuration of the map operation.\n            input_data (List[Dict[str, Any]]): The input data for the map operation.\n            is_filter (bool, optional): If True, the operation is a filter operation. Defaults to False.\n\n        Returns:\n            List[Dict[str, Any]]: The optimized operation configuration.\n        \"\"\"\n        map_optimizer = MapOptimizer(\n            self,\n            self.config,\n            self.console,\n            self.llm_client,\n            self.max_threads,\n            self._run_operation,\n            timeout=self.timeout,\n            is_filter=is_filter,\n        )\n        optimized_ops, _, cost = map_optimizer.optimize(op_config, input_data)\n        self.operations_cost += cost\n        return optimized_ops\n\n    def _optimize_resolve(\n        self, op_config: Dict[str, Any], input_data: List[Dict[str, Any]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Optimize a resolve operation.\n\n        This method creates a JoinOptimizer instance and uses it to optimize the resolve operation.\n        It updates the operation cost and runs the optimized operation.\n\n        Args:\n            op_config (Dict[str, Any]): The configuration of the resolve operation.\n            input_data (List[Dict[str, Any]]): The input data for the resolve operation.\n\n        Returns:\n            List[Dict[str, Any]]: The optimized operation configuration.\n        \"\"\"\n        optimized_config, cost = JoinOptimizer(\n            self,\n            self.config,\n            op_config,\n            self.console,\n            self.llm_client,\n            self.max_threads,\n            target_recall=self.config.get(\"optimizer_config\", {})\n            .get(\"resolve\", {})\n            .get(\"target_recall\", 0.95),\n        ).optimize_resolve(input_data)\n\n        if optimized_config.get(\"empty\", False):\n            # Remove this operation from the pipeline and just return input data\n            return [], input_data\n\n        self.operations_cost += cost\n\n        # Update the operation config with the optimized values\n        op_config.update(optimized_config)\n\n        return [op_config]\n\n    def _run_operation(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        return_instance: bool = False,\n        is_build: bool = False,\n    ) -&gt; Union[List[Dict[str, Any]], Tuple[List[Dict[str, Any]], BaseOperation]]:\n        \"\"\"\n        Run a single operation based on its configuration.\n\n        This method creates an instance of the appropriate operation class and executes it.\n        It also updates the total operation cost.\n\n        Args:\n            op_config (Dict[str, Any]): The configuration of the operation to run.\n            input_data (List[Dict[str, Any]]): The input data for the operation.\n            return_instance (bool, optional): If True, return the operation instance along with the output data.\n\n        Returns:\n            Union[List[Dict[str, Any]], Tuple[List[Dict[str, Any]], BaseOperation]]:\n            If return_instance is False, returns the output data.\n            If return_instance is True, returns a tuple of the output data and the operation instance.\n        \"\"\"\n        operation_class = get_operation(op_config[\"type\"])\n\n        oc_kwargs = {\n            \"runner\": self,\n            \"config\": op_config,\n            \"default_model\": self.config[\"default_model\"],\n            \"max_threads\": self.max_threads,\n            \"console\": self.console,\n            \"status\": self.status,\n        }\n        operation_instance = operation_class(**oc_kwargs)\n        if op_config[\"type\"] == \"equijoin\":\n            left_data = input_data[\"left\"]\n            right_data = input_data[\"right\"]\n            output_data, cost = operation_instance.execute(left_data, right_data)\n        elif op_config[\"type\"] == \"filter\":\n            output_data, cost = operation_instance.execute(input_data, is_build)\n        else:\n            output_data, cost = operation_instance.execute(input_data)\n        self.operations_cost += cost\n        if return_instance:\n            return output_data, operation_instance\n        else:\n            return output_data\n\n    # Recursively resolve all anchors and aliases\n    @staticmethod\n    def resolve_anchors(data):\n        \"\"\"\n        Recursively resolve all anchors and aliases in a nested data structure.\n\n        This static method traverses through dictionaries and lists, resolving any YAML anchors and aliases.\n\n        Args:\n            data: The data structure to resolve. Can be a dictionary, list, or any other type.\n\n        Returns:\n            The resolved data structure with all anchors and aliases replaced by their actual values.\n        \"\"\"\n        if isinstance(data, dict):\n            return {k: Optimizer.resolve_anchors(v) for k, v in data.items()}\n        elif isinstance(data, list):\n            return [Optimizer.resolve_anchors(item) for item in data]\n        else:\n            return data\n\n    def clean_optimized_config(self):\n        \"\"\"\n        Remove _intermediates from each operation in the optimized config\n        \"\"\"\n        # Create a copy of the optimized config to modify\n        config_to_save = self.optimized_config.copy()\n\n        resolved_config = Optimizer.resolve_anchors(config_to_save)\n\n        # Remove _intermediates from each operation in resolved_config\n        if \"operations\" in resolved_config:\n            for op_config in resolved_config[\"operations\"]:\n                if \"_intermediates\" in op_config:\n                    del op_config[\"_intermediates\"]\n\n        return resolved_config\n\n    def save_optimized_config(self):\n        \"\"\"\n        Save the optimized configuration to a YAML file.\n\n        This method creates a copy of the optimized configuration, resolves all anchors and aliases,\n        and saves it to a new YAML file. The new file name is based on the original file name with '_opt' appended.\n        \"\"\"\n        resolved_config = self.clean_optimized_config()\n\n        with open(self.optimized_config_path, \"w\") as f:\n            yaml.safe_dump(resolved_config, f, default_flow_style=False, width=80)\n            self.console.log(\n                f\"[green italic]\ud83d\udcbe Optimized config saved to {self.optimized_config_path}[/green italic]\"\n            )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.__init__","title":"<code>__init__(config, base_name, yaml_file_suffix, max_threads=None, model='gpt-4o', resume=False, timeout=60)</code>","text":"<p>Initialize the Optimizer class.</p> <p>This method sets up the optimizer with the given configuration file and parameters. It loads the configuration, initializes the console for output, sets up the LLM client, and prepares various attributes for optimization.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <code>max_threads</code> <code>Optional[int]</code> <p>Maximum number of threads to use for parallel processing. If None, it will be set to (number of CPUs * 4).</p> <code>None</code> <code>model</code> <code>str</code> <p>The name of the language model to use. Defaults to \"gpt-4o\".</p> <code>'gpt-4o'</code> <code>resume</code> <code>bool</code> <p>Whether to resume optimization from a previous run. Defaults to False.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds for operations. Defaults to 60.</p> <code>60</code> <p>Attributes:</p> Name Type Description <code>yaml_file_path</code> <code>str</code> <p>Stores the path to the YAML file.</p> <code>config</code> <code>Dict</code> <p>Stores the loaded configuration from the YAML file.</p> <code>console</code> <code>Console</code> <p>Rich console for formatted output.</p> <code>optimized_config</code> <code>Dict</code> <p>A copy of the original config to be optimized.</p> <code>llm_client</code> <code>LLMClient</code> <p>Client for interacting with the language model.</p> <code>max_threads</code> <code>int</code> <p>Maximum number of threads for parallel processing.</p> <code>operations_cost</code> <code>float</code> <p>Tracks the total cost of operations.</p> <code>timeout</code> <code>int</code> <p>Timeout for operations in seconds.</p> <code>selectivities</code> <code>defaultdict</code> <p>Stores selectivity information for operations. Selectivity is the ratio of output size to input size for an operation. It's used to estimate how much data will flow through the pipeline after each operation, which helps in optimizing subsequent operations and determining appropriate sample sizes. For example, a selectivity of 0.5 means an operation halves the size of its input data.</p> <code>datasets</code> <code>Dict</code> <p>Stores loaded datasets.</p> <p>The method also calls print_optimizer_config() to display the initial configuration.</p> Source code in <code>docetl/builder.py</code> <pre><code>def __init__(\n    self,\n    config: Dict,\n    base_name: str,\n    yaml_file_suffix: str,\n    max_threads: Optional[int] = None,\n    model: str = \"gpt-4o\",\n    resume: bool = False,\n    timeout: int = 60,\n):\n    \"\"\"\n    Initialize the Optimizer class.\n\n    This method sets up the optimizer with the given configuration file and parameters.\n    It loads the configuration, initializes the console for output, sets up the LLM client,\n    and prepares various attributes for optimization.\n\n    Args:\n        yaml_file (str): Path to the YAML configuration file.\n        max_threads (Optional[int]): Maximum number of threads to use for parallel processing.\n            If None, it will be set to (number of CPUs * 4).\n        model (str): The name of the language model to use. Defaults to \"gpt-4o\".\n        resume (bool): Whether to resume optimization from a previous run. Defaults to False.\n        timeout (int): Timeout in seconds for operations. Defaults to 60.\n\n    Attributes:\n        yaml_file_path (str): Stores the path to the YAML file.\n        config (Dict): Stores the loaded configuration from the YAML file.\n        console (Console): Rich console for formatted output.\n        optimized_config (Dict): A copy of the original config to be optimized.\n        llm_client (LLMClient): Client for interacting with the language model.\n        max_threads (int): Maximum number of threads for parallel processing.\n        operations_cost (float): Tracks the total cost of operations.\n        timeout (int): Timeout for operations in seconds.\n        selectivities (defaultdict): Stores selectivity information for operations.\n            Selectivity is the ratio of output size to input size for an operation.\n            It's used to estimate how much data will flow through the pipeline after\n            each operation, which helps in optimizing subsequent operations and\n            determining appropriate sample sizes. For example, a selectivity of 0.5\n            means an operation halves the size of its input data.\n        datasets (Dict): Stores loaded datasets.\n\n    The method also calls print_optimizer_config() to display the initial configuration.\n    \"\"\"\n    ConfigWrapper.__init__(self, config, max_threads)\n    self.optimized_config = copy.deepcopy(self.config)\n    self.llm_client = LLMClient(model)\n    self.operations_cost = 0\n    self.timeout = timeout\n    self.selectivities = defaultdict(dict)\n    self.samples_taken = defaultdict(dict)\n    self.resume = resume\n\n    # create parsing tool map\n    self.parsing_tool_map = create_parsing_tool_map(\n        self.config.get(\"parsing_tools\", None)\n    )\n\n    home_dir = os.path.expanduser(\"~\")\n    cache_dir = os.path.join(home_dir, f\".docetl/cache/{yaml_file_suffix}\")\n    os.makedirs(cache_dir, exist_ok=True)\n    self.datasets = DatasetOnDisk(dir=cache_dir, console=self.console)\n    self.optimized_ops_path = f\"{cache_dir}/optimized_ops\"\n    self.optimized_config_path = f\"{base_name}_opt.yaml\"\n\n    # Update sample size map\n    self.sample_size_map = SAMPLE_SIZE_MAP\n    if self.config.get(\"optimizer_config\", {}).get(\"sample_sizes\", {}):\n        self.sample_size_map.update(self.config[\"optimizer_config\"][\"sample_sizes\"])\n\n    self.step_op_to_optimized_ops = {}\n\n    self.print_optimizer_config()\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.clean_optimized_config","title":"<code>clean_optimized_config()</code>","text":"<p>Remove _intermediates from each operation in the optimized config</p> Source code in <code>docetl/builder.py</code> <pre><code>def clean_optimized_config(self):\n    \"\"\"\n    Remove _intermediates from each operation in the optimized config\n    \"\"\"\n    # Create a copy of the optimized config to modify\n    config_to_save = self.optimized_config.copy()\n\n    resolved_config = Optimizer.resolve_anchors(config_to_save)\n\n    # Remove _intermediates from each operation in resolved_config\n    if \"operations\" in resolved_config:\n        for op_config in resolved_config[\"operations\"]:\n            if \"_intermediates\" in op_config:\n                del op_config[\"_intermediates\"]\n\n    return resolved_config\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.compute_sample_size","title":"<code>compute_sample_size(step_name, step_ops, op_config)</code>","text":"<p>Compute the sample size necessary for optimizing given operation based on upstream operations.</p> <p>This method calculates an appropriate sample size for an operation, taking into account the selectivities of upstream operations in the same step. It uses a predefined sample size map (SAMPLE_SIZE_MAP) as a starting point.</p> <p>For example, if we have a 'map' operation with a default sample size of 10, and one upstream operation with a selectivity of 0.5, the computed sample size for the upstream operation would be: 10 / 0.5 = 20</p> <p>This ensures that after applying the selectivity of the upstream operation, we still have a representative sample size for the current operation.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>The name of the current step in the pipeline.</p> required <code>step_ops</code> <code>List[str]</code> <p>A list of all operations in the current step.</p> required <code>op_config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary for the current operation.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The computed sample size for the operation.</p> <p>The method works as follows: 1. If there are no upstream operations, it returns the default sample size for the operation type. 2. Otherwise, it starts with the default sample size and adjusts it based on the selectivities    of upstream operations. 3. It iterates through upstream operations in reverse order, dividing the sample size by    each operation's selectivity. 4. The final result is rounded to the nearest integer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the selectivity for any upstream operation is not found.</p> Note <ul> <li>The method assumes that selectivities for all upstream operations have been   previously computed and stored in self.selectivities.</li> <li>The sample size is always at least 1, even after all adjustments.</li> </ul> Source code in <code>docetl/builder.py</code> <pre><code>def compute_sample_size(\n    self,\n    step_name: str,\n    step_ops: List[str],\n    op_config: Dict[str, Any],\n) -&gt; int:\n    \"\"\"\n    Compute the sample size necessary for optimizing given operation based on upstream operations.\n\n    This method calculates an appropriate sample size for an operation, taking into\n    account the selectivities of upstream operations in the same step. It uses a\n    predefined sample size map (SAMPLE_SIZE_MAP) as a starting point.\n\n    For example, if we have a 'map' operation with a default sample size of 10,\n    and one upstream operation with a selectivity of 0.5, the computed sample size for the upstream operation would be:\n    10 / 0.5 = 20\n\n    This ensures that after applying the selectivity of the upstream operation,\n    we still have a representative sample size for the current operation.\n\n    Args:\n        step_name (str): The name of the current step in the pipeline.\n        step_ops (List[str]): A list of all operations in the current step.\n        op_config (Dict[str, Any]): The configuration dictionary for the current operation.\n\n    Returns:\n        int: The computed sample size for the operation.\n\n    The method works as follows:\n    1. If there are no upstream operations, it returns the default sample size for the operation type.\n    2. Otherwise, it starts with the default sample size and adjusts it based on the selectivities\n       of upstream operations.\n    3. It iterates through upstream operations in reverse order, dividing the sample size by\n       each operation's selectivity.\n    4. The final result is rounded to the nearest integer.\n\n    Raises:\n        ValueError: If the selectivity for any upstream operation is not found.\n\n    Note:\n        - The method assumes that selectivities for all upstream operations have been\n          previously computed and stored in self.selectivities.\n        - The sample size is always at least 1, even after all adjustments.\n    \"\"\"\n    # If an equijoin, load the default. Equijoins are always first\n    if op_config.get(\"type\") == \"equijoin\":\n        return SAMPLE_SIZE_MAP.get(op_config.get(\"type\"))\n\n    # If there are no upstream operations, use the default sample_size\n    upstream_ops = []\n    for step_op in step_ops:\n        if step_op != op_config.get(\"name\"):\n            if step_op in self.step_op_to_optimized_ops:\n                upstream_ops.extend(self.step_op_to_optimized_ops[step_op])\n            else:\n                upstream_ops.append(step_op)\n        else:\n            break\n\n    if len(upstream_ops) == 0:\n        return self.sample_size_map.get(op_config.get(\"type\"), float(\"inf\"))\n\n    # Otherwise, compute the sample size based on the upstream operations\n    sample_size = self.sample_size_map.get(op_config.get(\"type\"), 100)\n    for op in reversed(upstream_ops):\n        # Use the selectivity of the upstream operation to compute the sample size\n        if op not in self.selectivities[step_name]:\n            raise ValueError(\n                f\"Selectivity for operation {op} not found in selectivities. Other ops are {self.selectivities[step_name]}\"\n            )\n\n        sample_size = sample_size / self.selectivities[step_name].get(op)\n\n    return int(math.ceil(sample_size))\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.optimize","title":"<code>optimize()</code>","text":"<p>Optimize the entire pipeline defined in the configuration.</p> <p>This method is the main entry point for the optimization process. It iterates through each step in the pipeline, optimizing from upstream to downstream, and constructs an optimized version of the configuration.</p> <p>The optimization process includes: 1. Iterating through each step in the pipeline, from upstream to downstream. 2. Optimizing each step using the _optimize_step method. 3. Updating the optimized configuration with the new operations and steps. 4. Saving the optimized configuration to a file. 5. Logging the total costs (agent cost, operations cost, and total cost).</p> <p>Returns:</p> Type Description <p>None</p> <p>Side effects: - Modifies self.optimized_config with the optimized pipeline and operations. - Updates self.datasets with the results of each step. - Calls _save_optimized_config to save the optimized configuration to a file. - Logs cost information to the console.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a step in the pipeline does not have a name.</p> <p>Note: - This method assumes that all necessary data and configurations are already   loaded and initialized in the Optimizer instance. - The optimization process is performed step by step, from upstream to downstream,   with each step potentially depending on the results of previous steps.</p> Source code in <code>docetl/builder.py</code> <pre><code>def optimize(self):\n    \"\"\"\n    Optimize the entire pipeline defined in the configuration.\n\n    This method is the main entry point for the optimization process. It iterates through\n    each step in the pipeline, optimizing from upstream to downstream, and constructs an\n    optimized version of the configuration.\n\n    The optimization process includes:\n    1. Iterating through each step in the pipeline, from upstream to downstream.\n    2. Optimizing each step using the _optimize_step method.\n    3. Updating the optimized configuration with the new operations and steps.\n    4. Saving the optimized configuration to a file.\n    5. Logging the total costs (agent cost, operations cost, and total cost).\n\n    Returns:\n        None\n\n    Side effects:\n    - Modifies self.optimized_config with the optimized pipeline and operations.\n    - Updates self.datasets with the results of each step.\n    - Calls _save_optimized_config to save the optimized configuration to a file.\n    - Logs cost information to the console.\n\n    Raises:\n        ValueError: If a step in the pipeline does not have a name.\n\n    Note:\n    - This method assumes that all necessary data and configurations are already\n      loaded and initialized in the Optimizer instance.\n    - The optimization process is performed step by step, from upstream to downstream,\n      with each step potentially depending on the results of previous steps.\n    \"\"\"\n    self.console.rule(\"[bold cyan]Beginning Pipeline Optimization[/bold cyan]\")\n\n    self.syntax_check()\n\n    self._insert_empty_resolve_operations()\n\n    # If resume is True, load the optimized operations from disk\n    if self.resume:\n        self._load_optimized_ops()\n\n    for step in self.config[\"pipeline\"][\"steps\"]:\n        step_name = step.get(\"name\")\n        if not step_name:\n            raise ValueError(\n                \"Step does not have a name. Each step must have a unique name.\"\n            )\n\n        optimized_step, step_operations, input_data = self._optimize_step(step)\n        old_op_names = [\n            op\n            for op in step[\"operations\"]\n            if op not in optimized_step[\"operations\"]\n        ]\n\n        # Remove all old_op_names from self.optimized_config[\"operations\"]\n        self.optimized_config[\"operations\"] = [\n            op\n            for op in self.optimized_config[\"operations\"]\n            if op[\"name\"] not in old_op_names\n        ]\n\n        for op in optimized_step[\"operations\"]:\n            changed_op = False\n            for i, op_config in enumerate(self.optimized_config[\"operations\"]):\n                if op_config[\"name\"] == op:\n                    self.optimized_config[\"operations\"][i] = step_operations[op]\n                    changed_op = True\n            if not changed_op:\n                self.optimized_config[\"operations\"].append(step_operations[op])\n\n        self.optimized_config[\"pipeline\"][\"steps\"] = [\n            step\n            for step in self.optimized_config[\"pipeline\"][\"steps\"]\n            if step[\"name\"] != step_name\n        ] + [optimized_step]\n\n        self.step_op_to_optimized_ops[step_name] = optimized_step[\"operations\"]\n\n        step_hash = (\n            hashlib.md5(\n                json.dumps(\n                    {\n                        \"step\": [\n                            s\n                            for s in self.optimized_config[\"pipeline\"][\"steps\"]\n                            if s[\"name\"] == step_name\n                        ][0],\n                        \"operations\": [\n                            self.find_operation(op, self.optimized_config)\n                            for op in optimized_step[\"operations\"]\n                        ],\n                    }\n                ).encode()\n            ).hexdigest()\n            + \".json\"\n        )\n        # If the dataset already exists, skip the step\n        if step_hash in self.datasets:\n            continue\n\n        flush_cache(self.console)\n\n        if step_name in self.config.get(\"optimizer_config\", {}).get(\n            \"run_full_step\", []\n        ):\n            # Run the entire step\n            input_data = self._run_partial_step(\n                step,\n                step_operations,\n                float(\"inf\"),  # TODO: FIX THIS\n            )\n            self.datasets[step_hash] = copy.deepcopy(input_data)\n        else:\n            self.datasets[step_hash] = copy.deepcopy(input_data)\n\n    self.console.log(\n        f\"[bold]Total agent cost: ${self.llm_client.total_cost:.2f}[/bold]\"\n    )\n    self.console.log(\n        f\"[bold]Total operations cost: ${self.operations_cost:.2f}[/bold]\"\n    )\n    self.console.log(\n        f\"[bold]Total cost: ${self.llm_client.total_cost + self.operations_cost:.2f}[/bold]\"\n    )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.print_optimizer_config","title":"<code>print_optimizer_config()</code>","text":"<p>Print the current configuration of the optimizer.</p> <p>This method uses the Rich console to display a formatted output of the optimizer's configuration. It includes details such as the YAML file path, sample sizes for different operation types, maximum number of threads, the language model being used, and the timeout setting.</p> <p>The output is color-coded and formatted for easy readability, with a header and separator lines to clearly delineate the configuration information.</p> Source code in <code>docetl/builder.py</code> <pre><code>def print_optimizer_config(self):\n    \"\"\"\n    Print the current configuration of the optimizer.\n\n    This method uses the Rich console to display a formatted output of the optimizer's\n    configuration. It includes details such as the YAML file path, sample sizes for\n    different operation types, maximum number of threads, the language model being used,\n    and the timeout setting.\n\n    The output is color-coded and formatted for easy readability, with a header and\n    separator lines to clearly delineate the configuration information.\n    \"\"\"\n    self.console.rule(\"[bold cyan]Optimizer Configuration[/bold cyan]\")\n    self.console.log(f\"[yellow]Sample Size:[/yellow] {self.sample_size_map}\")\n    self.console.log(f\"[yellow]Max Threads:[/yellow] {self.max_threads}\")\n    self.console.log(f\"[yellow]Model:[/yellow] {self.llm_client.model}\")\n    self.console.log(f\"[yellow]Timeout:[/yellow] {self.timeout} seconds\")\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.resolve_anchors","title":"<code>resolve_anchors(data)</code>  <code>staticmethod</code>","text":"<p>Recursively resolve all anchors and aliases in a nested data structure.</p> <p>This static method traverses through dictionaries and lists, resolving any YAML anchors and aliases.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>The data structure to resolve. Can be a dictionary, list, or any other type.</p> required <p>Returns:</p> Type Description <p>The resolved data structure with all anchors and aliases replaced by their actual values.</p> Source code in <code>docetl/builder.py</code> <pre><code>@staticmethod\ndef resolve_anchors(data):\n    \"\"\"\n    Recursively resolve all anchors and aliases in a nested data structure.\n\n    This static method traverses through dictionaries and lists, resolving any YAML anchors and aliases.\n\n    Args:\n        data: The data structure to resolve. Can be a dictionary, list, or any other type.\n\n    Returns:\n        The resolved data structure with all anchors and aliases replaced by their actual values.\n    \"\"\"\n    if isinstance(data, dict):\n        return {k: Optimizer.resolve_anchors(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        return [Optimizer.resolve_anchors(item) for item in data]\n    else:\n        return data\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.save_optimized_config","title":"<code>save_optimized_config()</code>","text":"<p>Save the optimized configuration to a YAML file.</p> <p>This method creates a copy of the optimized configuration, resolves all anchors and aliases, and saves it to a new YAML file. The new file name is based on the original file name with '_opt' appended.</p> Source code in <code>docetl/builder.py</code> <pre><code>def save_optimized_config(self):\n    \"\"\"\n    Save the optimized configuration to a YAML file.\n\n    This method creates a copy of the optimized configuration, resolves all anchors and aliases,\n    and saves it to a new YAML file. The new file name is based on the original file name with '_opt' appended.\n    \"\"\"\n    resolved_config = self.clean_optimized_config()\n\n    with open(self.optimized_config_path, \"w\") as f:\n        yaml.safe_dump(resolved_config, f, default_flow_style=False, width=80)\n        self.console.log(\n            f\"[green italic]\ud83d\udcbe Optimized config saved to {self.optimized_config_path}[/green italic]\"\n        )\n</code></pre>"},{"location":"api-reference/docetl/#docetl.Optimizer.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Perform a syntax check on all operations defined in the configuration.</p> <p>This method validates each operation by attempting to instantiate it. If any operation fails to instantiate, a ValueError is raised.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any operation fails the syntax check.</p> Source code in <code>docetl/builder.py</code> <pre><code>def syntax_check(self):\n    \"\"\"\n    Perform a syntax check on all operations defined in the configuration.\n\n    This method validates each operation by attempting to instantiate it.\n    If any operation fails to instantiate, a ValueError is raised.\n\n    Raises:\n        ValueError: If any operation fails the syntax check.\n    \"\"\"\n    for operation_config in self.config[\"operations\"]:\n        operation = operation_config[\"name\"]\n        operation_type = operation_config[\"type\"]\n\n        try:\n            operation_class = get_operation(operation_type)\n            operation_class(\n                self,\n                operation_config,\n                self.config.get(\"default_model\", \"gpt-4o-mini\"),\n                self.max_threads,\n                console=self.console,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Syntax check failed for operation '{operation}': {str(e)}\"\n            )\n\n    self.console.log(\"[green]Syntax check passed for all operations.[/green]\")\n</code></pre>"},{"location":"api-reference/operations/","title":"LLM-Powered Operators","text":""},{"location":"api-reference/operations/#docetl.operations.map.MapOperation","title":"<code>docetl.operations.map.MapOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/map.py</code> <pre><code>class MapOperation(BaseOperation):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.max_batch_size: int = self.config.get(\n            \"max_batch_size\", kwargs.get(\"max_batch_size\", float(\"inf\"))\n        )\n        self.clustering_method = \"random\"\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n            Checks the configuration of the MapOperation for required keys and valid structure.\n\n        Raises:\n            ValueError: If required keys are missing or invalid in the configuration.\n            TypeError: If configuration values have incorrect types.\n        \"\"\"\n        config = MapOp(**self.config)\n\n        if config.drop_keys:\n            if any(not isinstance(key, str) for key in config.drop_keys):\n                raise TypeError(\"All items in 'drop_keys' must be strings\")\n        elif not (config.prompt and config.output):\n            raise ValueError(\n                \"If 'drop_keys' is not specified, both 'prompt' and 'output' must be present in the configuration\"\n            )\n\n        if config.prompt or config.output:\n            for key in [\"prompt\", \"output\"]:\n                if not getattr(config, key):\n                    raise ValueError(\n                        f\"Missing required key '{key}' in MapOperation configuration\"\n                    )\n\n            if config.output and not config.output[\"schema\"]:\n                raise ValueError(\"Missing 'schema' in 'output' configuration\")\n\n            if config.prompt:\n                try:\n                    Template(config.prompt)\n                except Exception as e:\n                    raise ValueError(\n                        f\"Invalid Jinja2 template in 'prompt': {str(e)}\"\n                    ) from e\n\n            if config.model and not isinstance(config.model, str):\n                raise TypeError(\"'model' in configuration must be a string\")\n\n            if config.tools:\n                for tool in config.tools:\n                    try:\n                        tool_obj = Tool(**tool)\n                    except Exception as e:\n                        raise TypeError(\"Tool must be a dictionary\")\n\n                    if not (tool_obj.code and tool_obj.function):\n                        raise ValueError(\n                            \"Tool is missing required 'code' or 'function' key\"\n                        )\n\n                    if not isinstance(tool_obj.function, ToolFunction):\n                        raise TypeError(\"'function' in tool must be a dictionary\")\n\n                    for key in [\"name\", \"description\", \"parameters\"]:\n                        if not getattr(tool_obj.function, key):\n                            raise ValueError(\n                                f\"Tool is missing required '{key}' in 'function'\"\n                            )\n\n            self.gleaning_check()\n\n    def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Executes the map operation on the provided input data.\n\n        Args:\n            input_data (List[Dict]): The input data to process.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.\n\n        This method performs the following steps:\n        1. If a prompt is specified, it processes each input item using the specified prompt and LLM model\n        2. Applies gleaning if configured\n        3. Validates the output\n        4. If drop_keys is specified, it drops the specified keys from each document\n        5. Aggregates results and calculates total cost\n\n        The method uses parallel processing to improve performance.\n        \"\"\"\n        # Check if there's no prompt and only drop_keys\n        if \"prompt\" not in self.config and \"drop_keys\" in self.config:\n            # If only drop_keys is specified, simply drop the keys and return\n            dropped_results = []\n            for item in input_data:\n                new_item = {\n                    k: v for k, v in item.items() if k not in self.config[\"drop_keys\"]\n                }\n                dropped_results.append(new_item)\n            return dropped_results, 0.0  # Return the modified data with no cost\n\n        if self.status:\n            self.status.stop()\n\n        def _process_map_item(item: Dict) -&gt; Tuple[Optional[Dict], float]:\n            prompt_template = Template(self.config[\"prompt\"])\n            prompt = prompt_template.render(input=item)\n\n            def validation_fn(response: Dict[str, Any]):\n                output = self.runner.api.parse_llm_response(\n                    response,\n                    schema=self.config[\"output\"][\"schema\"],\n                    tools=self.config.get(\"tools\", None),\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0]\n                for key, value in item.items():\n                    if key not in self.config[\"output\"][\"schema\"]:\n                        output[key] = value\n                if self.runner.api.validate_output(self.config, output, self.console):\n                    return output, True\n                return output, False\n\n            self.runner.rate_limiter.try_acquire(\"call\", weight=1)\n            llm_result = self.runner.api.call_llm(\n                self.config.get(\"model\", self.default_model),\n                \"map\",\n                [{\"role\": \"user\", \"content\": prompt}],\n                self.config[\"output\"][\"schema\"],\n                tools=self.config.get(\"tools\", None),\n                scratchpad=None,\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n                validation_config=(\n                    {\n                        \"num_retries\": self.num_retries_on_validate_failure,\n                        \"val_rule\": self.config.get(\"validate\", []),\n                        \"validation_fn\": validation_fn,\n                    }\n                    if self.config.get(\"validate\", None)\n                    else None\n                ),\n                gleaning_config=self.config.get(\"gleaning\", None),\n                verbose=self.config.get(\"verbose\", False),\n                bypass_cache=self.config.get(\"bypass_cache\", False),\n            )\n\n            if llm_result.validated:\n                # Parse the response\n                output = self.runner.api.parse_llm_response(\n                    llm_result.response,\n                    schema=self.config[\"output\"][\"schema\"],\n                    tools=self.config.get(\"tools\", None),\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0]\n                # Augment the output with the original item\n                output = {**item, **output}\n                return output, llm_result.total_cost\n\n            return None, llm_result.total_cost\n\n        with ThreadPoolExecutor(max_workers=self.max_batch_size) as executor:\n            futures = [executor.submit(_process_map_item, item) for item in input_data]\n            results = []\n            total_cost = 0\n            pbar = RichLoopBar(\n                range(len(futures)),\n                desc=f\"Processing {self.config['name']} (map) on all documents\",\n                console=self.console,\n            )\n            for i in pbar:\n                result, item_cost = futures[i].result()\n                if result is not None:\n                    if \"drop_keys\" in self.config:\n                        result = {\n                            k: v\n                            for k, v in result.items()\n                            if k not in self.config[\"drop_keys\"]\n                        }\n                    results.append(result)\n                total_cost += item_cost\n                pbar.update(i)\n\n        if self.status:\n            self.status.start()\n\n        return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.map.MapOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Executes the map operation on the provided input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Dict]</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Dict], float]</code> <p>Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.</p> <p>This method performs the following steps: 1. If a prompt is specified, it processes each input item using the specified prompt and LLM model 2. Applies gleaning if configured 3. Validates the output 4. If drop_keys is specified, it drops the specified keys from each document 5. Aggregates results and calculates total cost</p> <p>The method uses parallel processing to improve performance.</p> Source code in <code>docetl/operations/map.py</code> <pre><code>def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Executes the map operation on the provided input data.\n\n    Args:\n        input_data (List[Dict]): The input data to process.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.\n\n    This method performs the following steps:\n    1. If a prompt is specified, it processes each input item using the specified prompt and LLM model\n    2. Applies gleaning if configured\n    3. Validates the output\n    4. If drop_keys is specified, it drops the specified keys from each document\n    5. Aggregates results and calculates total cost\n\n    The method uses parallel processing to improve performance.\n    \"\"\"\n    # Check if there's no prompt and only drop_keys\n    if \"prompt\" not in self.config and \"drop_keys\" in self.config:\n        # If only drop_keys is specified, simply drop the keys and return\n        dropped_results = []\n        for item in input_data:\n            new_item = {\n                k: v for k, v in item.items() if k not in self.config[\"drop_keys\"]\n            }\n            dropped_results.append(new_item)\n        return dropped_results, 0.0  # Return the modified data with no cost\n\n    if self.status:\n        self.status.stop()\n\n    def _process_map_item(item: Dict) -&gt; Tuple[Optional[Dict], float]:\n        prompt_template = Template(self.config[\"prompt\"])\n        prompt = prompt_template.render(input=item)\n\n        def validation_fn(response: Dict[str, Any]):\n            output = self.runner.api.parse_llm_response(\n                response,\n                schema=self.config[\"output\"][\"schema\"],\n                tools=self.config.get(\"tools\", None),\n                manually_fix_errors=self.manually_fix_errors,\n            )[0]\n            for key, value in item.items():\n                if key not in self.config[\"output\"][\"schema\"]:\n                    output[key] = value\n            if self.runner.api.validate_output(self.config, output, self.console):\n                return output, True\n            return output, False\n\n        self.runner.rate_limiter.try_acquire(\"call\", weight=1)\n        llm_result = self.runner.api.call_llm(\n            self.config.get(\"model\", self.default_model),\n            \"map\",\n            [{\"role\": \"user\", \"content\": prompt}],\n            self.config[\"output\"][\"schema\"],\n            tools=self.config.get(\"tools\", None),\n            scratchpad=None,\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            validation_config=(\n                {\n                    \"num_retries\": self.num_retries_on_validate_failure,\n                    \"val_rule\": self.config.get(\"validate\", []),\n                    \"validation_fn\": validation_fn,\n                }\n                if self.config.get(\"validate\", None)\n                else None\n            ),\n            gleaning_config=self.config.get(\"gleaning\", None),\n            verbose=self.config.get(\"verbose\", False),\n            bypass_cache=self.config.get(\"bypass_cache\", False),\n        )\n\n        if llm_result.validated:\n            # Parse the response\n            output = self.runner.api.parse_llm_response(\n                llm_result.response,\n                schema=self.config[\"output\"][\"schema\"],\n                tools=self.config.get(\"tools\", None),\n                manually_fix_errors=self.manually_fix_errors,\n            )[0]\n            # Augment the output with the original item\n            output = {**item, **output}\n            return output, llm_result.total_cost\n\n        return None, llm_result.total_cost\n\n    with ThreadPoolExecutor(max_workers=self.max_batch_size) as executor:\n        futures = [executor.submit(_process_map_item, item) for item in input_data]\n        results = []\n        total_cost = 0\n        pbar = RichLoopBar(\n            range(len(futures)),\n            desc=f\"Processing {self.config['name']} (map) on all documents\",\n            console=self.console,\n        )\n        for i in pbar:\n            result, item_cost = futures[i].result()\n            if result is not None:\n                if \"drop_keys\" in self.config:\n                    result = {\n                        k: v\n                        for k, v in result.items()\n                        if k not in self.config[\"drop_keys\"]\n                    }\n                results.append(result)\n            total_cost += item_cost\n            pbar.update(i)\n\n    if self.status:\n        self.status.start()\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.map.MapOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<pre><code>Checks the configuration of the MapOperation for required keys and valid structure.\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing or invalid in the configuration.</p> <code>TypeError</code> <p>If configuration values have incorrect types.</p> Source code in <code>docetl/operations/map.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n        Checks the configuration of the MapOperation for required keys and valid structure.\n\n    Raises:\n        ValueError: If required keys are missing or invalid in the configuration.\n        TypeError: If configuration values have incorrect types.\n    \"\"\"\n    config = MapOp(**self.config)\n\n    if config.drop_keys:\n        if any(not isinstance(key, str) for key in config.drop_keys):\n            raise TypeError(\"All items in 'drop_keys' must be strings\")\n    elif not (config.prompt and config.output):\n        raise ValueError(\n            \"If 'drop_keys' is not specified, both 'prompt' and 'output' must be present in the configuration\"\n        )\n\n    if config.prompt or config.output:\n        for key in [\"prompt\", \"output\"]:\n            if not getattr(config, key):\n                raise ValueError(\n                    f\"Missing required key '{key}' in MapOperation configuration\"\n                )\n\n        if config.output and not config.output[\"schema\"]:\n            raise ValueError(\"Missing 'schema' in 'output' configuration\")\n\n        if config.prompt:\n            try:\n                Template(config.prompt)\n            except Exception as e:\n                raise ValueError(\n                    f\"Invalid Jinja2 template in 'prompt': {str(e)}\"\n                ) from e\n\n        if config.model and not isinstance(config.model, str):\n            raise TypeError(\"'model' in configuration must be a string\")\n\n        if config.tools:\n            for tool in config.tools:\n                try:\n                    tool_obj = Tool(**tool)\n                except Exception as e:\n                    raise TypeError(\"Tool must be a dictionary\")\n\n                if not (tool_obj.code and tool_obj.function):\n                    raise ValueError(\n                        \"Tool is missing required 'code' or 'function' key\"\n                    )\n\n                if not isinstance(tool_obj.function, ToolFunction):\n                    raise TypeError(\"'function' in tool must be a dictionary\")\n\n                for key in [\"name\", \"description\", \"parameters\"]:\n                    if not getattr(tool_obj.function, key):\n                        raise ValueError(\n                            f\"Tool is missing required '{key}' in 'function'\"\n                        )\n\n        self.gleaning_check()\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.resolve.ResolveOperation","title":"<code>docetl.operations.resolve.ResolveOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/resolve.py</code> <pre><code>class ResolveOperation(BaseOperation):\n    def compare_pair(\n        self,\n        comparison_prompt: str,\n        model: str,\n        item1: Dict,\n        item2: Dict,\n        blocking_keys: List[str] = [],\n        timeout_seconds: int = 120,\n        max_retries_per_timeout: int = 2,\n    ) -&gt; Tuple[bool, float]:\n        \"\"\"\n        Compares two items using an LLM model to determine if they match.\n\n        Args:\n            comparison_prompt (str): The prompt template for comparison.\n            model (str): The LLM model to use for comparison.\n            item1 (Dict): The first item to compare.\n            item2 (Dict): The second item to compare.\n\n        Returns:\n            Tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.\n        \"\"\"\n        if blocking_keys:\n            if all(\n                key in item1\n                and key in item2\n                and item1[key].lower() == item2[key].lower()\n                for key in blocking_keys\n            ):\n                return True, 0\n\n        prompt_template = Template(comparison_prompt)\n        prompt = prompt_template.render(input1=item1, input2=item2)\n        response = self.runner.api.call_llm(\n            model,\n            \"compare\",\n            [{\"role\": \"user\", \"content\": prompt}],\n            {\"is_match\": \"bool\"},\n            timeout_seconds=timeout_seconds,\n            max_retries_per_timeout=max_retries_per_timeout,\n            bypass_cache=self.config.get(\"bypass_cache\", False),\n        )\n        output = self.runner.api.parse_llm_response(\n            response.response,\n            {\"is_match\": \"bool\"},\n        )[0]\n        return output[\"is_match\"], response.total_cost\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Checks the configuration of the ResolveOperation for required keys and valid structure.\n\n        This method performs the following checks:\n        1. Verifies the presence of required keys: 'comparison_prompt' and 'output'.\n        2. Ensures 'output' contains a 'schema' key.\n        3. Validates that 'schema' in 'output' is a non-empty dictionary.\n        4. Checks if 'comparison_prompt' is a valid Jinja2 template with 'input1' and 'input2' variables.\n        5. If 'resolution_prompt' is present, verifies it as a valid Jinja2 template with 'inputs' variable.\n        6. Optionally checks if 'model' is a string (if present).\n        7. Optionally checks 'blocking_keys' (if present, further checks are performed).\n\n        Raises:\n            ValueError: If required keys are missing, if templates are invalid or missing required variables,\n                        or if any other configuration aspect is incorrect or inconsistent.\n            TypeError: If the types of configuration values are incorrect, such as 'schema' not being a dict\n                       or 'model' not being a string.\n        \"\"\"\n        required_keys = [\"comparison_prompt\", \"output\"]\n        for key in required_keys:\n            if key not in self.config:\n                raise ValueError(\n                    f\"Missing required key '{key}' in ResolveOperation configuration\"\n                )\n\n        if \"schema\" not in self.config[\"output\"]:\n            raise ValueError(\"Missing 'schema' in 'output' configuration\")\n\n        if not isinstance(self.config[\"output\"][\"schema\"], dict):\n            raise TypeError(\"'schema' in 'output' configuration must be a dictionary\")\n\n        if not self.config[\"output\"][\"schema\"]:\n            raise ValueError(\"'schema' in 'output' configuration cannot be empty\")\n\n        # Check if the comparison_prompt is a valid Jinja2 template\n        try:\n            comparison_template = Template(self.config[\"comparison_prompt\"])\n            comparison_vars = comparison_template.environment.parse(\n                self.config[\"comparison_prompt\"]\n            ).find_all(jinja2.nodes.Name)\n            comparison_var_names = {var.name for var in comparison_vars}\n            if (\n                \"input1\" not in comparison_var_names\n                or \"input2\" not in comparison_var_names\n            ):\n                raise ValueError(\n                    \"'comparison_prompt' must contain both 'input1' and 'input2' variables\"\n                )\n\n            if \"resolution_prompt\" in self.config:\n                reduction_template = Template(self.config[\"resolution_prompt\"])\n                reduction_vars = reduction_template.environment.parse(\n                    self.config[\"resolution_prompt\"]\n                ).find_all(jinja2.nodes.Name)\n                reduction_var_names = {var.name for var in reduction_vars}\n                if \"inputs\" not in reduction_var_names:\n                    raise ValueError(\n                        \"'resolution_prompt' must contain 'inputs' variable\"\n                    )\n        except Exception as e:\n            raise ValueError(f\"Invalid Jinja2 template: {str(e)}\")\n\n        # Check if the model is specified (optional)\n        if \"model\" in self.config and not isinstance(self.config[\"model\"], str):\n            raise TypeError(\"'model' in configuration must be a string\")\n\n        # Check blocking_keys (optional)\n        if \"blocking_keys\" in self.config:\n            if not isinstance(self.config[\"blocking_keys\"], list):\n                raise TypeError(\"'blocking_keys' must be a list\")\n            if not all(isinstance(key, str) for key in self.config[\"blocking_keys\"]):\n                raise TypeError(\"All items in 'blocking_keys' must be strings\")\n\n        # Check blocking_threshold (optional)\n        if \"blocking_threshold\" in self.config:\n            if not isinstance(self.config[\"blocking_threshold\"], (int, float)):\n                raise TypeError(\"'blocking_threshold' must be a number\")\n            if not 0 &lt;= self.config[\"blocking_threshold\"] &lt;= 1:\n                raise ValueError(\"'blocking_threshold' must be between 0 and 1\")\n\n        # Check blocking_conditions (optional)\n        if \"blocking_conditions\" in self.config:\n            if not isinstance(self.config[\"blocking_conditions\"], list):\n                raise TypeError(\"'blocking_conditions' must be a list\")\n            if not all(\n                isinstance(cond, str) for cond in self.config[\"blocking_conditions\"]\n            ):\n                raise TypeError(\"All items in 'blocking_conditions' must be strings\")\n\n        # Check if input schema is provided and valid (optional)\n        if \"input\" in self.config:\n            if \"schema\" not in self.config[\"input\"]:\n                raise ValueError(\"Missing 'schema' in 'input' configuration\")\n            if not isinstance(self.config[\"input\"][\"schema\"], dict):\n                raise TypeError(\n                    \"'schema' in 'input' configuration must be a dictionary\"\n                )\n\n        # Check limit_comparisons (optional)\n        if \"limit_comparisons\" in self.config:\n            if not isinstance(self.config[\"limit_comparisons\"], int):\n                raise TypeError(\"'limit_comparisons' must be an integer\")\n            if self.config[\"limit_comparisons\"] &lt;= 0:\n                raise ValueError(\"'limit_comparisons' must be a positive integer\")\n\n    def validation_fn(self, response: Dict[str, Any]):\n        output = self.runner.api.parse_llm_response(\n            response,\n            schema=self.config[\"output\"][\"schema\"],\n        )[0]\n        if self.runner.api.validate_output(self.config, output, self.console):\n            return output, True\n        return output, False\n\n    def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Executes the resolve operation on the provided dataset.\n\n        Args:\n            input_data (List[Dict]): The dataset to resolve.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the resolved results and the total cost of the operation.\n\n        This method performs the following steps:\n        1. Initial blocking based on specified conditions and/or embedding similarity\n        2. Pairwise comparison of potentially matching entries using LLM\n        3. Clustering of matched entries\n        4. Resolution of each cluster into a single entry (if applicable)\n        5. Result aggregation and validation\n\n        The method also calculates and logs statistics such as comparisons saved by blocking and self-join selectivity.\n        \"\"\"\n        if len(input_data) == 0:\n            return [], 0\n\n        blocking_keys = self.config.get(\"blocking_keys\", [])\n        blocking_threshold = self.config.get(\"blocking_threshold\")\n        blocking_conditions = self.config.get(\"blocking_conditions\", [])\n        if self.status:\n            self.status.stop()\n\n        if not blocking_threshold and not blocking_conditions:\n            # Prompt the user for confirmation\n            if not Confirm.ask(\n                f\"[yellow]Warning: No blocking keys or conditions specified. \"\n                f\"This may result in a large number of comparisons. \"\n                f\"We recommend specifying at least one blocking key or condition, or using the optimizer to automatically come up with these. \"\n                f\"Do you want to continue without blocking?[/yellow]\",\n            ):\n                raise ValueError(\"Operation cancelled by user.\")\n\n        input_schema = self.config.get(\"input\", {}).get(\"schema\", {})\n        if not blocking_keys:\n            # Set them to all keys in the input data\n            blocking_keys = list(input_data[0].keys())\n        limit_comparisons = self.config.get(\"limit_comparisons\")\n        total_cost = 0\n\n        def is_match(item1: Dict[str, Any], item2: Dict[str, Any]) -&gt; bool:\n            return any(\n                eval(condition, {\"input1\": item1, \"input2\": item2})\n                for condition in blocking_conditions\n            )\n\n        # Calculate embeddings if blocking_threshold is set\n        embeddings = None\n        if blocking_threshold is not None:\n            embedding_model = self.config.get(\"embedding_model\", self.default_model)\n\n            def get_embeddings_batch(\n                items: List[Dict[str, Any]]\n            ) -&gt; List[Tuple[List[float], float]]:\n                texts = [\n                    \" \".join(str(item[key]) for key in blocking_keys if key in item)\n                    for item in items\n                ]\n                response = self.runner.api.gen_embedding(\n                    model=embedding_model, input=texts\n                )\n                return [\n                    (data[\"embedding\"], completion_cost(response))\n                    for data in response[\"data\"]\n                ]\n\n            embeddings = []\n            costs = []\n            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n                for i in range(\n                    0, len(input_data), self.config.get(\"embedding_batch_size\", 1000)\n                ):\n                    batch = input_data[\n                        i : i + self.config.get(\"embedding_batch_size\", 1000)\n                    ]\n                    batch_results = list(executor.map(get_embeddings_batch, [batch]))\n\n                    for result in batch_results:\n                        embeddings.extend([r[0] for r in result])\n                        costs.extend([r[1] for r in result])\n\n                total_cost += sum(costs)\n\n        # Initialize clusters\n        clusters = [{i} for i in range(len(input_data))]\n        cluster_map = {i: i for i in range(len(input_data))}\n\n        def find_cluster(item):\n            while item != cluster_map[item]:\n                cluster_map[item] = cluster_map[cluster_map[item]]\n                item = cluster_map[item]\n            return item\n\n        def merge_clusters(item1, item2):\n            root1, root2 = find_cluster(item1), find_cluster(item2)\n            if root1 != root2:\n                if len(clusters[root1]) &lt; len(clusters[root2]):\n                    root1, root2 = root2, root1\n                clusters[root1] |= clusters[root2]\n                cluster_map[root2] = root1\n                clusters[root2] = set()\n\n        # Generate all pairs to compare\n        # TODO: virtualize this if possible\n        all_pairs = [\n            (i, j)\n            for i in range(len(input_data))\n            for j in range(i + 1, len(input_data))\n        ]\n\n        # Filter pairs based on blocking conditions\n        def meets_blocking_conditions(pair):\n            i, j = pair\n            return (\n                is_match(input_data[i], input_data[j]) if blocking_conditions else False\n            )\n\n        blocked_pairs = list(filter(meets_blocking_conditions, all_pairs))\n\n        # Apply limit_comparisons to blocked pairs\n        if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n            self.console.log(\n                f\"Randomly sampling {limit_comparisons} pairs out of {len(blocked_pairs)} blocked pairs.\"\n            )\n            blocked_pairs = random.sample(blocked_pairs, limit_comparisons)\n\n        # If there are remaining comparisons, fill with highest cosine similarities\n        remaining_comparisons = (\n            limit_comparisons - len(blocked_pairs)\n            if limit_comparisons is not None\n            else float(\"inf\")\n        )\n        if remaining_comparisons &gt; 0 and blocking_threshold is not None:\n            # Compute cosine similarity for all pairs efficiently\n            from sklearn.metrics.pairwise import cosine_similarity\n\n            similarity_matrix = cosine_similarity(embeddings)\n\n            cosine_pairs = []\n            for i, j in all_pairs:\n                if (i, j) not in blocked_pairs and find_cluster(i) != find_cluster(j):\n                    similarity = similarity_matrix[i, j]\n                    if similarity &gt;= blocking_threshold:\n                        cosine_pairs.append((i, j, similarity))\n\n            if remaining_comparisons != float(\"inf\"):\n                cosine_pairs.sort(key=lambda x: x[2], reverse=True)\n                additional_pairs = [\n                    (i, j) for i, j, _ in cosine_pairs[: int(remaining_comparisons)]\n                ]\n                blocked_pairs.extend(additional_pairs)\n            else:\n                blocked_pairs.extend((i, j) for i, j, _ in cosine_pairs)\n\n        filtered_pairs = blocked_pairs\n\n        # Calculate and print statistics\n        total_possible_comparisons = len(input_data) * (len(input_data) - 1) // 2\n        comparisons_made = len(filtered_pairs)\n        comparisons_saved = total_possible_comparisons - comparisons_made\n        self.console.log(\n            f\"[green]Comparisons saved by blocking: {comparisons_saved} \"\n            f\"({(comparisons_saved / total_possible_comparisons) * 100:.2f}%)[/green]\"\n        )\n\n        # Compare pairs and update clusters in real-time\n        batch_size = self.config.get(\"compare_batch_size\", 100)\n        pair_costs = 0\n\n        pbar = RichLoopBar(\n            range(0, len(filtered_pairs), batch_size),\n            desc=f\"Processing batches of {batch_size} LLM comparisons\",\n            console=self.console,\n        )\n        for i in pbar:\n            batch = filtered_pairs[i : i + batch_size]\n\n            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n                future_to_pair = {\n                    executor.submit(\n                        self.compare_pair,\n                        self.config[\"comparison_prompt\"],\n                        self.config.get(\"comparison_model\", self.default_model),\n                        input_data[pair[0]],\n                        input_data[pair[1]],\n                        blocking_keys,\n                        timeout_seconds=self.config.get(\"timeout\", 120),\n                        max_retries_per_timeout=self.config.get(\n                            \"max_retries_per_timeout\", 2\n                        ),\n                    ): pair\n                    for pair in batch\n                }\n\n                for future in as_completed(future_to_pair):\n                    pair = future_to_pair[future]\n                    is_match_result, cost = future.result()\n                    pair_costs += cost\n                    if is_match_result:\n                        merge_clusters(pair[0], pair[1])\n\n                    pbar.update(i)\n\n        total_cost += pair_costs\n\n        # Collect final clusters\n        final_clusters = [cluster for cluster in clusters if cluster]\n\n        # Process each cluster\n        results = []\n\n        def process_cluster(cluster):\n            if len(cluster) &gt; 1:\n                cluster_items = [input_data[i] for i in cluster]\n                reduction_template = Template(self.config[\"resolution_prompt\"])\n                if input_schema:\n                    cluster_items = [\n                        {k: item[k] for k in input_schema.keys() if k in item}\n                        for item in cluster_items\n                    ]\n\n                resolution_prompt = reduction_template.render(inputs=cluster_items)\n                reduction_response = self.runner.api.call_llm(\n                    self.config.get(\"resolution_model\", self.default_model),\n                    \"reduce\",\n                    [{\"role\": \"user\", \"content\": resolution_prompt}],\n                    self.config[\"output\"][\"schema\"],\n                    timeout_seconds=self.config.get(\"timeout\", 120),\n                    max_retries_per_timeout=self.config.get(\n                        \"max_retries_per_timeout\", 2\n                    ),\n                    bypass_cache=self.config.get(\"bypass_cache\", False),\n                    validation_config=(\n                        {\n                            \"val_rule\": self.config.get(\"validate\", []),\n                            \"validation_fn\": self.validation_fn,\n                        }\n                        if self.config.get(\"validate\", None)\n                        else None\n                    ),\n                )\n                reduction_cost = reduction_response.total_cost\n\n                if reduction_response.validated:\n                    reduction_output = self.runner.api.parse_llm_response(\n                        reduction_response.response,\n                        self.config[\"output\"][\"schema\"],\n                        manually_fix_errors=self.manually_fix_errors,\n                    )[0]\n                    return (\n                        [\n                            {\n                                **item,\n                                **{\n                                    k: reduction_output[k]\n                                    for k in self.config[\"output\"][\"schema\"]\n                                },\n                            }\n                            for item in [input_data[i] for i in cluster]\n                        ],\n                        reduction_cost,\n                    )\n                return [], reduction_cost\n            else:\n                # Set the output schema to be the keys found in the compare_prompt\n                compare_prompt_keys = extract_jinja_variables(\n                    self.config[\"comparison_prompt\"]\n                )\n                # Get the set of keys in the compare_prompt\n                compare_prompt_keys = set(\n                    [\n                        k.replace(\"input1.\", \"\")\n                        for k in compare_prompt_keys\n                        if \"input1\" in k\n                    ]\n                )\n\n                # For each key in the output schema, find the most similar key in the compare_prompt\n                output_keys = set(self.config[\"output\"][\"schema\"].keys())\n                key_mapping = {}\n                for output_key in output_keys:\n                    best_match = None\n                    best_score = 0\n                    for compare_key in compare_prompt_keys:\n                        score = sum(\n                            c1 == c2 for c1, c2 in zip(output_key, compare_key)\n                        ) / max(len(output_key), len(compare_key))\n                        if score &gt; best_score:\n                            best_score = score\n                            best_match = compare_key\n                    key_mapping[output_key] = best_match\n\n                # Create the result dictionary using the key mapping\n                result = input_data[list(cluster)[0]].copy()\n                for output_key, compare_key in key_mapping.items():\n                    if compare_key in input_data[list(cluster)[0]]:\n                        result[output_key] = input_data[list(cluster)[0]][compare_key]\n                    else:\n                        result[output_key] = None  # or some default value\n\n                return [result], 0\n\n        # Calculate the number of records before and clusters after\n        num_records_before = len(input_data)\n        num_clusters_after = len(final_clusters)\n        self.console.log(f\"Number of keys before resolution: {num_records_before}\")\n        self.console.log(\n            f\"Number of distinct keys after resolution: {num_clusters_after}\"\n        )\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(process_cluster, cluster) for cluster in final_clusters\n            ]\n            for future in rich_as_completed(\n                futures,\n                total=len(futures),\n                desc=\"Determining resolved key for each group of equivalent keys\",\n                console=self.console,\n            ):\n                cluster_results, cluster_cost = future.result()\n                results.extend(cluster_results)\n                total_cost += cluster_cost\n\n        total_pairs = len(input_data) * (len(input_data) - 1) // 2\n        true_match_count = sum(\n            len(cluster) * (len(cluster) - 1) // 2\n            for cluster in final_clusters\n            if len(cluster) &gt; 1\n        )\n        true_match_selectivity = (\n            true_match_count / total_pairs if total_pairs &gt; 0 else 0\n        )\n        self.console.log(f\"Self-join selectivity: {true_match_selectivity:.4f}\")\n\n        if self.status:\n            self.status.start()\n\n        return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.resolve.ResolveOperation.compare_pair","title":"<code>compare_pair(comparison_prompt, model, item1, item2, blocking_keys=[], timeout_seconds=120, max_retries_per_timeout=2)</code>","text":"<p>Compares two items using an LLM model to determine if they match.</p> <p>Parameters:</p> Name Type Description Default <code>comparison_prompt</code> <code>str</code> <p>The prompt template for comparison.</p> required <code>model</code> <code>str</code> <p>The LLM model to use for comparison.</p> required <code>item1</code> <code>Dict</code> <p>The first item to compare.</p> required <code>item2</code> <code>Dict</code> <p>The second item to compare.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, float]</code> <p>Tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.</p> Source code in <code>docetl/operations/resolve.py</code> <pre><code>def compare_pair(\n    self,\n    comparison_prompt: str,\n    model: str,\n    item1: Dict,\n    item2: Dict,\n    blocking_keys: List[str] = [],\n    timeout_seconds: int = 120,\n    max_retries_per_timeout: int = 2,\n) -&gt; Tuple[bool, float]:\n    \"\"\"\n    Compares two items using an LLM model to determine if they match.\n\n    Args:\n        comparison_prompt (str): The prompt template for comparison.\n        model (str): The LLM model to use for comparison.\n        item1 (Dict): The first item to compare.\n        item2 (Dict): The second item to compare.\n\n    Returns:\n        Tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.\n    \"\"\"\n    if blocking_keys:\n        if all(\n            key in item1\n            and key in item2\n            and item1[key].lower() == item2[key].lower()\n            for key in blocking_keys\n        ):\n            return True, 0\n\n    prompt_template = Template(comparison_prompt)\n    prompt = prompt_template.render(input1=item1, input2=item2)\n    response = self.runner.api.call_llm(\n        model,\n        \"compare\",\n        [{\"role\": \"user\", \"content\": prompt}],\n        {\"is_match\": \"bool\"},\n        timeout_seconds=timeout_seconds,\n        max_retries_per_timeout=max_retries_per_timeout,\n        bypass_cache=self.config.get(\"bypass_cache\", False),\n    )\n    output = self.runner.api.parse_llm_response(\n        response.response,\n        {\"is_match\": \"bool\"},\n    )[0]\n    return output[\"is_match\"], response.total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.resolve.ResolveOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Executes the resolve operation on the provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Dict]</code> <p>The dataset to resolve.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Dict], float]</code> <p>Tuple[List[Dict], float]: A tuple containing the resolved results and the total cost of the operation.</p> <p>This method performs the following steps: 1. Initial blocking based on specified conditions and/or embedding similarity 2. Pairwise comparison of potentially matching entries using LLM 3. Clustering of matched entries 4. Resolution of each cluster into a single entry (if applicable) 5. Result aggregation and validation</p> <p>The method also calculates and logs statistics such as comparisons saved by blocking and self-join selectivity.</p> Source code in <code>docetl/operations/resolve.py</code> <pre><code>def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Executes the resolve operation on the provided dataset.\n\n    Args:\n        input_data (List[Dict]): The dataset to resolve.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the resolved results and the total cost of the operation.\n\n    This method performs the following steps:\n    1. Initial blocking based on specified conditions and/or embedding similarity\n    2. Pairwise comparison of potentially matching entries using LLM\n    3. Clustering of matched entries\n    4. Resolution of each cluster into a single entry (if applicable)\n    5. Result aggregation and validation\n\n    The method also calculates and logs statistics such as comparisons saved by blocking and self-join selectivity.\n    \"\"\"\n    if len(input_data) == 0:\n        return [], 0\n\n    blocking_keys = self.config.get(\"blocking_keys\", [])\n    blocking_threshold = self.config.get(\"blocking_threshold\")\n    blocking_conditions = self.config.get(\"blocking_conditions\", [])\n    if self.status:\n        self.status.stop()\n\n    if not blocking_threshold and not blocking_conditions:\n        # Prompt the user for confirmation\n        if not Confirm.ask(\n            f\"[yellow]Warning: No blocking keys or conditions specified. \"\n            f\"This may result in a large number of comparisons. \"\n            f\"We recommend specifying at least one blocking key or condition, or using the optimizer to automatically come up with these. \"\n            f\"Do you want to continue without blocking?[/yellow]\",\n        ):\n            raise ValueError(\"Operation cancelled by user.\")\n\n    input_schema = self.config.get(\"input\", {}).get(\"schema\", {})\n    if not blocking_keys:\n        # Set them to all keys in the input data\n        blocking_keys = list(input_data[0].keys())\n    limit_comparisons = self.config.get(\"limit_comparisons\")\n    total_cost = 0\n\n    def is_match(item1: Dict[str, Any], item2: Dict[str, Any]) -&gt; bool:\n        return any(\n            eval(condition, {\"input1\": item1, \"input2\": item2})\n            for condition in blocking_conditions\n        )\n\n    # Calculate embeddings if blocking_threshold is set\n    embeddings = None\n    if blocking_threshold is not None:\n        embedding_model = self.config.get(\"embedding_model\", self.default_model)\n\n        def get_embeddings_batch(\n            items: List[Dict[str, Any]]\n        ) -&gt; List[Tuple[List[float], float]]:\n            texts = [\n                \" \".join(str(item[key]) for key in blocking_keys if key in item)\n                for item in items\n            ]\n            response = self.runner.api.gen_embedding(\n                model=embedding_model, input=texts\n            )\n            return [\n                (data[\"embedding\"], completion_cost(response))\n                for data in response[\"data\"]\n            ]\n\n        embeddings = []\n        costs = []\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            for i in range(\n                0, len(input_data), self.config.get(\"embedding_batch_size\", 1000)\n            ):\n                batch = input_data[\n                    i : i + self.config.get(\"embedding_batch_size\", 1000)\n                ]\n                batch_results = list(executor.map(get_embeddings_batch, [batch]))\n\n                for result in batch_results:\n                    embeddings.extend([r[0] for r in result])\n                    costs.extend([r[1] for r in result])\n\n            total_cost += sum(costs)\n\n    # Initialize clusters\n    clusters = [{i} for i in range(len(input_data))]\n    cluster_map = {i: i for i in range(len(input_data))}\n\n    def find_cluster(item):\n        while item != cluster_map[item]:\n            cluster_map[item] = cluster_map[cluster_map[item]]\n            item = cluster_map[item]\n        return item\n\n    def merge_clusters(item1, item2):\n        root1, root2 = find_cluster(item1), find_cluster(item2)\n        if root1 != root2:\n            if len(clusters[root1]) &lt; len(clusters[root2]):\n                root1, root2 = root2, root1\n            clusters[root1] |= clusters[root2]\n            cluster_map[root2] = root1\n            clusters[root2] = set()\n\n    # Generate all pairs to compare\n    # TODO: virtualize this if possible\n    all_pairs = [\n        (i, j)\n        for i in range(len(input_data))\n        for j in range(i + 1, len(input_data))\n    ]\n\n    # Filter pairs based on blocking conditions\n    def meets_blocking_conditions(pair):\n        i, j = pair\n        return (\n            is_match(input_data[i], input_data[j]) if blocking_conditions else False\n        )\n\n    blocked_pairs = list(filter(meets_blocking_conditions, all_pairs))\n\n    # Apply limit_comparisons to blocked pairs\n    if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n        self.console.log(\n            f\"Randomly sampling {limit_comparisons} pairs out of {len(blocked_pairs)} blocked pairs.\"\n        )\n        blocked_pairs = random.sample(blocked_pairs, limit_comparisons)\n\n    # If there are remaining comparisons, fill with highest cosine similarities\n    remaining_comparisons = (\n        limit_comparisons - len(blocked_pairs)\n        if limit_comparisons is not None\n        else float(\"inf\")\n    )\n    if remaining_comparisons &gt; 0 and blocking_threshold is not None:\n        # Compute cosine similarity for all pairs efficiently\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        similarity_matrix = cosine_similarity(embeddings)\n\n        cosine_pairs = []\n        for i, j in all_pairs:\n            if (i, j) not in blocked_pairs and find_cluster(i) != find_cluster(j):\n                similarity = similarity_matrix[i, j]\n                if similarity &gt;= blocking_threshold:\n                    cosine_pairs.append((i, j, similarity))\n\n        if remaining_comparisons != float(\"inf\"):\n            cosine_pairs.sort(key=lambda x: x[2], reverse=True)\n            additional_pairs = [\n                (i, j) for i, j, _ in cosine_pairs[: int(remaining_comparisons)]\n            ]\n            blocked_pairs.extend(additional_pairs)\n        else:\n            blocked_pairs.extend((i, j) for i, j, _ in cosine_pairs)\n\n    filtered_pairs = blocked_pairs\n\n    # Calculate and print statistics\n    total_possible_comparisons = len(input_data) * (len(input_data) - 1) // 2\n    comparisons_made = len(filtered_pairs)\n    comparisons_saved = total_possible_comparisons - comparisons_made\n    self.console.log(\n        f\"[green]Comparisons saved by blocking: {comparisons_saved} \"\n        f\"({(comparisons_saved / total_possible_comparisons) * 100:.2f}%)[/green]\"\n    )\n\n    # Compare pairs and update clusters in real-time\n    batch_size = self.config.get(\"compare_batch_size\", 100)\n    pair_costs = 0\n\n    pbar = RichLoopBar(\n        range(0, len(filtered_pairs), batch_size),\n        desc=f\"Processing batches of {batch_size} LLM comparisons\",\n        console=self.console,\n    )\n    for i in pbar:\n        batch = filtered_pairs[i : i + batch_size]\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            future_to_pair = {\n                executor.submit(\n                    self.compare_pair,\n                    self.config[\"comparison_prompt\"],\n                    self.config.get(\"comparison_model\", self.default_model),\n                    input_data[pair[0]],\n                    input_data[pair[1]],\n                    blocking_keys,\n                    timeout_seconds=self.config.get(\"timeout\", 120),\n                    max_retries_per_timeout=self.config.get(\n                        \"max_retries_per_timeout\", 2\n                    ),\n                ): pair\n                for pair in batch\n            }\n\n            for future in as_completed(future_to_pair):\n                pair = future_to_pair[future]\n                is_match_result, cost = future.result()\n                pair_costs += cost\n                if is_match_result:\n                    merge_clusters(pair[0], pair[1])\n\n                pbar.update(i)\n\n    total_cost += pair_costs\n\n    # Collect final clusters\n    final_clusters = [cluster for cluster in clusters if cluster]\n\n    # Process each cluster\n    results = []\n\n    def process_cluster(cluster):\n        if len(cluster) &gt; 1:\n            cluster_items = [input_data[i] for i in cluster]\n            reduction_template = Template(self.config[\"resolution_prompt\"])\n            if input_schema:\n                cluster_items = [\n                    {k: item[k] for k in input_schema.keys() if k in item}\n                    for item in cluster_items\n                ]\n\n            resolution_prompt = reduction_template.render(inputs=cluster_items)\n            reduction_response = self.runner.api.call_llm(\n                self.config.get(\"resolution_model\", self.default_model),\n                \"reduce\",\n                [{\"role\": \"user\", \"content\": resolution_prompt}],\n                self.config[\"output\"][\"schema\"],\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\n                    \"max_retries_per_timeout\", 2\n                ),\n                bypass_cache=self.config.get(\"bypass_cache\", False),\n                validation_config=(\n                    {\n                        \"val_rule\": self.config.get(\"validate\", []),\n                        \"validation_fn\": self.validation_fn,\n                    }\n                    if self.config.get(\"validate\", None)\n                    else None\n                ),\n            )\n            reduction_cost = reduction_response.total_cost\n\n            if reduction_response.validated:\n                reduction_output = self.runner.api.parse_llm_response(\n                    reduction_response.response,\n                    self.config[\"output\"][\"schema\"],\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0]\n                return (\n                    [\n                        {\n                            **item,\n                            **{\n                                k: reduction_output[k]\n                                for k in self.config[\"output\"][\"schema\"]\n                            },\n                        }\n                        for item in [input_data[i] for i in cluster]\n                    ],\n                    reduction_cost,\n                )\n            return [], reduction_cost\n        else:\n            # Set the output schema to be the keys found in the compare_prompt\n            compare_prompt_keys = extract_jinja_variables(\n                self.config[\"comparison_prompt\"]\n            )\n            # Get the set of keys in the compare_prompt\n            compare_prompt_keys = set(\n                [\n                    k.replace(\"input1.\", \"\")\n                    for k in compare_prompt_keys\n                    if \"input1\" in k\n                ]\n            )\n\n            # For each key in the output schema, find the most similar key in the compare_prompt\n            output_keys = set(self.config[\"output\"][\"schema\"].keys())\n            key_mapping = {}\n            for output_key in output_keys:\n                best_match = None\n                best_score = 0\n                for compare_key in compare_prompt_keys:\n                    score = sum(\n                        c1 == c2 for c1, c2 in zip(output_key, compare_key)\n                    ) / max(len(output_key), len(compare_key))\n                    if score &gt; best_score:\n                        best_score = score\n                        best_match = compare_key\n                key_mapping[output_key] = best_match\n\n            # Create the result dictionary using the key mapping\n            result = input_data[list(cluster)[0]].copy()\n            for output_key, compare_key in key_mapping.items():\n                if compare_key in input_data[list(cluster)[0]]:\n                    result[output_key] = input_data[list(cluster)[0]][compare_key]\n                else:\n                    result[output_key] = None  # or some default value\n\n            return [result], 0\n\n    # Calculate the number of records before and clusters after\n    num_records_before = len(input_data)\n    num_clusters_after = len(final_clusters)\n    self.console.log(f\"Number of keys before resolution: {num_records_before}\")\n    self.console.log(\n        f\"Number of distinct keys after resolution: {num_clusters_after}\"\n    )\n\n    with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n        futures = [\n            executor.submit(process_cluster, cluster) for cluster in final_clusters\n        ]\n        for future in rich_as_completed(\n            futures,\n            total=len(futures),\n            desc=\"Determining resolved key for each group of equivalent keys\",\n            console=self.console,\n        ):\n            cluster_results, cluster_cost = future.result()\n            results.extend(cluster_results)\n            total_cost += cluster_cost\n\n    total_pairs = len(input_data) * (len(input_data) - 1) // 2\n    true_match_count = sum(\n        len(cluster) * (len(cluster) - 1) // 2\n        for cluster in final_clusters\n        if len(cluster) &gt; 1\n    )\n    true_match_selectivity = (\n        true_match_count / total_pairs if total_pairs &gt; 0 else 0\n    )\n    self.console.log(f\"Self-join selectivity: {true_match_selectivity:.4f}\")\n\n    if self.status:\n        self.status.start()\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.resolve.ResolveOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Checks the configuration of the ResolveOperation for required keys and valid structure.</p> <p>This method performs the following checks: 1. Verifies the presence of required keys: 'comparison_prompt' and 'output'. 2. Ensures 'output' contains a 'schema' key. 3. Validates that 'schema' in 'output' is a non-empty dictionary. 4. Checks if 'comparison_prompt' is a valid Jinja2 template with 'input1' and 'input2' variables. 5. If 'resolution_prompt' is present, verifies it as a valid Jinja2 template with 'inputs' variable. 6. Optionally checks if 'model' is a string (if present). 7. Optionally checks 'blocking_keys' (if present, further checks are performed).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing, if templates are invalid or missing required variables,         or if any other configuration aspect is incorrect or inconsistent.</p> <code>TypeError</code> <p>If the types of configuration values are incorrect, such as 'schema' not being a dict        or 'model' not being a string.</p> Source code in <code>docetl/operations/resolve.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Checks the configuration of the ResolveOperation for required keys and valid structure.\n\n    This method performs the following checks:\n    1. Verifies the presence of required keys: 'comparison_prompt' and 'output'.\n    2. Ensures 'output' contains a 'schema' key.\n    3. Validates that 'schema' in 'output' is a non-empty dictionary.\n    4. Checks if 'comparison_prompt' is a valid Jinja2 template with 'input1' and 'input2' variables.\n    5. If 'resolution_prompt' is present, verifies it as a valid Jinja2 template with 'inputs' variable.\n    6. Optionally checks if 'model' is a string (if present).\n    7. Optionally checks 'blocking_keys' (if present, further checks are performed).\n\n    Raises:\n        ValueError: If required keys are missing, if templates are invalid or missing required variables,\n                    or if any other configuration aspect is incorrect or inconsistent.\n        TypeError: If the types of configuration values are incorrect, such as 'schema' not being a dict\n                   or 'model' not being a string.\n    \"\"\"\n    required_keys = [\"comparison_prompt\", \"output\"]\n    for key in required_keys:\n        if key not in self.config:\n            raise ValueError(\n                f\"Missing required key '{key}' in ResolveOperation configuration\"\n            )\n\n    if \"schema\" not in self.config[\"output\"]:\n        raise ValueError(\"Missing 'schema' in 'output' configuration\")\n\n    if not isinstance(self.config[\"output\"][\"schema\"], dict):\n        raise TypeError(\"'schema' in 'output' configuration must be a dictionary\")\n\n    if not self.config[\"output\"][\"schema\"]:\n        raise ValueError(\"'schema' in 'output' configuration cannot be empty\")\n\n    # Check if the comparison_prompt is a valid Jinja2 template\n    try:\n        comparison_template = Template(self.config[\"comparison_prompt\"])\n        comparison_vars = comparison_template.environment.parse(\n            self.config[\"comparison_prompt\"]\n        ).find_all(jinja2.nodes.Name)\n        comparison_var_names = {var.name for var in comparison_vars}\n        if (\n            \"input1\" not in comparison_var_names\n            or \"input2\" not in comparison_var_names\n        ):\n            raise ValueError(\n                \"'comparison_prompt' must contain both 'input1' and 'input2' variables\"\n            )\n\n        if \"resolution_prompt\" in self.config:\n            reduction_template = Template(self.config[\"resolution_prompt\"])\n            reduction_vars = reduction_template.environment.parse(\n                self.config[\"resolution_prompt\"]\n            ).find_all(jinja2.nodes.Name)\n            reduction_var_names = {var.name for var in reduction_vars}\n            if \"inputs\" not in reduction_var_names:\n                raise ValueError(\n                    \"'resolution_prompt' must contain 'inputs' variable\"\n                )\n    except Exception as e:\n        raise ValueError(f\"Invalid Jinja2 template: {str(e)}\")\n\n    # Check if the model is specified (optional)\n    if \"model\" in self.config and not isinstance(self.config[\"model\"], str):\n        raise TypeError(\"'model' in configuration must be a string\")\n\n    # Check blocking_keys (optional)\n    if \"blocking_keys\" in self.config:\n        if not isinstance(self.config[\"blocking_keys\"], list):\n            raise TypeError(\"'blocking_keys' must be a list\")\n        if not all(isinstance(key, str) for key in self.config[\"blocking_keys\"]):\n            raise TypeError(\"All items in 'blocking_keys' must be strings\")\n\n    # Check blocking_threshold (optional)\n    if \"blocking_threshold\" in self.config:\n        if not isinstance(self.config[\"blocking_threshold\"], (int, float)):\n            raise TypeError(\"'blocking_threshold' must be a number\")\n        if not 0 &lt;= self.config[\"blocking_threshold\"] &lt;= 1:\n            raise ValueError(\"'blocking_threshold' must be between 0 and 1\")\n\n    # Check blocking_conditions (optional)\n    if \"blocking_conditions\" in self.config:\n        if not isinstance(self.config[\"blocking_conditions\"], list):\n            raise TypeError(\"'blocking_conditions' must be a list\")\n        if not all(\n            isinstance(cond, str) for cond in self.config[\"blocking_conditions\"]\n        ):\n            raise TypeError(\"All items in 'blocking_conditions' must be strings\")\n\n    # Check if input schema is provided and valid (optional)\n    if \"input\" in self.config:\n        if \"schema\" not in self.config[\"input\"]:\n            raise ValueError(\"Missing 'schema' in 'input' configuration\")\n        if not isinstance(self.config[\"input\"][\"schema\"], dict):\n            raise TypeError(\n                \"'schema' in 'input' configuration must be a dictionary\"\n            )\n\n    # Check limit_comparisons (optional)\n    if \"limit_comparisons\" in self.config:\n        if not isinstance(self.config[\"limit_comparisons\"], int):\n            raise TypeError(\"'limit_comparisons' must be an integer\")\n        if self.config[\"limit_comparisons\"] &lt;= 0:\n            raise ValueError(\"'limit_comparisons' must be a positive integer\")\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation","title":"<code>docetl.operations.reduce.ReduceOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>A class that implements a reduce operation on input data using language models.</p> <p>This class extends BaseOperation to provide functionality for reducing grouped data using various strategies including batch reduce, incremental reduce, and parallel fold and merge.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>class ReduceOperation(BaseOperation):\n    \"\"\"\n    A class that implements a reduce operation on input data using language models.\n\n    This class extends BaseOperation to provide functionality for reducing grouped data\n    using various strategies including batch reduce, incremental reduce, and parallel fold and merge.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the ReduceOperation.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.min_samples = 5\n        self.max_samples = 1000\n        self.fold_times = deque(maxlen=self.max_samples)\n        self.merge_times = deque(maxlen=self.max_samples)\n        self.lock = Lock()\n        self.config[\"reduce_key\"] = (\n            [self.config[\"reduce_key\"]]\n            if isinstance(self.config[\"reduce_key\"], str)\n            else self.config[\"reduce_key\"]\n        )\n        self.intermediates = {}\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Perform comprehensive syntax checks on the configuration of the ReduceOperation.\n\n        This method validates the presence and correctness of all required configuration keys, Jinja2 templates, and ensures the correct\n        structure and types of the entire configuration.\n\n        The method performs the following checks:\n        1. Verifies the presence of all required keys in the configuration.\n        2. Validates the structure and content of the 'output' configuration, including its 'schema'.\n        3. Checks if the main 'prompt' is a valid Jinja2 template and contains the required 'inputs' variable.\n        4. If 'merge_prompt' is specified, ensures that 'fold_prompt' is also present.\n        5. If 'fold_prompt' is present, verifies the existence of 'fold_batch_size'.\n        6. Validates the 'fold_prompt' as a Jinja2 template with required variables 'inputs' and 'output'.\n        7. If present, checks 'merge_prompt' as a valid Jinja2 template with required 'outputs' variable.\n        8. Verifies types of various configuration inputs (e.g., 'fold_batch_size' as int).\n        9. Checks for the presence and validity of optional configurations like 'model'.\n\n        Raises:\n            ValueError: If any required configuration is missing, if templates are invalid or missing required\n                        variables, or if any other configuration aspect is incorrect or inconsistent.\n            TypeError: If any configuration value has an incorrect type, such as 'schema' not being a dict\n                       or 'fold_batch_size' not being an integer.\n        \"\"\"\n        required_keys = [\"reduce_key\", \"prompt\", \"output\"]\n        for key in required_keys:\n            if key not in self.config:\n                raise ValueError(\n                    f\"Missing required key '{key}' in {self.config['name']} configuration\"\n                )\n\n        if \"schema\" not in self.config[\"output\"]:\n            raise ValueError(\n                f\"Missing 'schema' in {self.config['name']} 'output' configuration\"\n            )\n\n        if not isinstance(self.config[\"output\"][\"schema\"], dict):\n            raise TypeError(\n                f\"'schema' in {self.config['name']} 'output' configuration must be a dictionary\"\n            )\n\n        if not self.config[\"output\"][\"schema\"]:\n            raise ValueError(\n                f\"'schema' in {self.config['name']} 'output' configuration cannot be empty\"\n            )\n\n        # Check if the prompt is a valid Jinja2 template\n        try:\n            template = Template(self.config[\"prompt\"])\n            template_vars = template.environment.parse(self.config[\"prompt\"]).find_all(\n                jinja2.nodes.Name\n            )\n            template_var_names = {var.name for var in template_vars}\n            if \"inputs\" not in template_var_names:\n                raise ValueError(\n                    f\"Prompt template for {self.config['name']} must include the 'inputs' variable\"\n                )\n        except Exception as e:\n            raise ValueError(\n                f\"Invalid Jinja2 template in {self.config['name']} 'prompt': {str(e)}\"\n            )\n\n        # Check if fold_prompt is a valid Jinja2 template (now required if merge exists)\n        if \"merge_prompt\" in self.config:\n            if \"fold_prompt\" not in self.config:\n                raise ValueError(\n                    f\"'fold_prompt' is required when 'merge_prompt' is specified in {self.config['name']}\"\n                )\n\n        if \"fold_prompt\" in self.config:\n            if \"fold_batch_size\" not in self.config:\n                raise ValueError(\n                    f\"'fold_batch_size' is required when 'fold_prompt' is specified in {self.config['name']}\"\n                )\n\n            try:\n                fold_template = Template(self.config[\"fold_prompt\"])\n                fold_template_vars = fold_template.environment.parse(\n                    self.config[\"fold_prompt\"]\n                ).find_all(jinja2.nodes.Name)\n                fold_template_var_names = {var.name for var in fold_template_vars}\n                required_vars = {\"inputs\", \"output\"}\n                if not required_vars.issubset(fold_template_var_names):\n                    raise ValueError(\n                        f\"Fold template in {self.config['name']} must include variables: {required_vars}. Current template includes: {fold_template_var_names}\"\n                    )\n            except Exception as e:\n                raise ValueError(\n                    f\"Invalid Jinja2 template in {self.config['name']} 'fold_prompt': {str(e)}\"\n                )\n\n        # Check merge_prompt and merge_batch_size\n        if \"merge_prompt\" in self.config:\n            if \"merge_batch_size\" not in self.config:\n                raise ValueError(\n                    f\"'merge_batch_size' is required when 'merge_prompt' is specified in {self.config['name']}\"\n                )\n\n            try:\n                merge_template = Template(self.config[\"merge_prompt\"])\n                merge_template_vars = merge_template.environment.parse(\n                    self.config[\"merge_prompt\"]\n                ).find_all(jinja2.nodes.Name)\n                merge_template_var_names = {var.name for var in merge_template_vars}\n                if \"outputs\" not in merge_template_var_names:\n                    raise ValueError(\n                        f\"Merge template in {self.config['name']} must include the 'outputs' variable\"\n                    )\n            except Exception as e:\n                raise ValueError(\n                    f\"Invalid Jinja2 template in {self.config['name']} 'merge_prompt': {str(e)}\"\n                )\n\n        # Check if the model is specified (optional)\n        if \"model\" in self.config and not isinstance(self.config[\"model\"], str):\n            raise TypeError(\n                f\"'model' in {self.config['name']} configuration must be a string\"\n            )\n\n        # Check if reduce_key is a string or a list of strings\n        if not isinstance(self.config[\"reduce_key\"], (str, list)):\n            raise TypeError(\n                f\"'reduce_key' in {self.config['name']} configuration must be a string or a list of strings\"\n            )\n        if isinstance(self.config[\"reduce_key\"], list):\n            if not all(isinstance(key, str) for key in self.config[\"reduce_key\"]):\n                raise TypeError(\n                    f\"All elements in 'reduce_key' list in {self.config['name']} configuration must be strings\"\n                )\n\n        # Check if input schema is provided and valid (optional)\n        if \"input\" in self.config:\n            if \"schema\" not in self.config[\"input\"]:\n                raise ValueError(\n                    f\"Missing 'schema' in {self.config['name']} 'input' configuration\"\n                )\n            if not isinstance(self.config[\"input\"][\"schema\"], dict):\n                raise TypeError(\n                    f\"'schema' in {self.config['name']} 'input' configuration must be a dictionary\"\n                )\n\n        # Check if fold_batch_size and merge_batch_size are positive integers\n        for key in [\"fold_batch_size\", \"merge_batch_size\"]:\n            if key in self.config:\n                if not isinstance(self.config[key], int) or self.config[key] &lt;= 0:\n                    raise ValueError(\n                        f\"'{key}' in {self.config['name']} configuration must be a positive integer\"\n                    )\n\n        if \"value_sampling\" in self.config:\n            sampling = self.config[\"value_sampling\"]\n            if not isinstance(sampling, dict):\n                raise TypeError(\n                    f\"'value_sampling' in {self.config['name']} configuration must be a dictionary\"\n                )\n\n            if \"enabled\" not in sampling:\n                raise ValueError(\n                    f\"'enabled' is required in {self.config['name']} 'value_sampling' configuration\"\n                )\n            if not isinstance(sampling[\"enabled\"], bool):\n                raise TypeError(\n                    f\"'enabled' in {self.config['name']} 'value_sampling' configuration must be a boolean\"\n                )\n\n            if sampling[\"enabled\"]:\n                if \"sample_size\" not in sampling:\n                    raise ValueError(\n                        f\"'sample_size' is required when value_sampling is enabled in {self.config['name']}\"\n                    )\n                if (\n                    not isinstance(sampling[\"sample_size\"], int)\n                    or sampling[\"sample_size\"] &lt;= 0\n                ):\n                    raise ValueError(\n                        f\"'sample_size' in {self.config['name']} configuration must be a positive integer\"\n                    )\n\n                if \"method\" not in sampling:\n                    raise ValueError(\n                        f\"'method' is required when value_sampling is enabled in {self.config['name']}\"\n                    )\n                if sampling[\"method\"] not in [\n                    \"random\",\n                    \"first_n\",\n                    \"cluster\",\n                    \"sem_sim\",\n                ]:\n                    raise ValueError(\n                        f\"Invalid 'method'. Must be 'random', 'first_n', or 'embedding' in {self.config['name']}\"\n                    )\n\n                if sampling[\"method\"] == \"embedding\":\n                    if \"embedding_model\" not in sampling:\n                        raise ValueError(\n                            f\"'embedding_model' is required when using embedding-based sampling in {self.config['name']}\"\n                        )\n                    if \"embedding_keys\" not in sampling:\n                        raise ValueError(\n                            f\"'embedding_keys' is required when using embedding-based sampling in {self.config['name']}\"\n                        )\n\n        self.gleaning_check()\n\n    def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Execute the reduce operation on the provided input data.\n\n        This method sorts and groups the input data by the reduce key(s), then processes each group\n        using either parallel fold and merge, incremental reduce, or batch reduce strategies.\n\n        Args:\n            input_data (List[Dict]): The input data to process.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.\n        \"\"\"\n        if self.config.get(\"gleaning\", {}).get(\"validation_prompt\", None):\n            self.console.log(\n                f\"Using gleaning with validation prompt: {self.config.get('gleaning', {}).get('validation_prompt', '')}\"\n            )\n\n        reduce_keys = self.config[\"reduce_key\"]\n        if isinstance(reduce_keys, str):\n            reduce_keys = [reduce_keys]\n        input_schema = self.config.get(\"input\", {}).get(\"schema\", {})\n\n        if self.status:\n            self.status.stop()\n\n        # Check if we need to group everything into one group\n        if reduce_keys == [\"_all\"] or reduce_keys == \"_all\":\n            grouped_data = [(\"_all\", input_data)]\n        else:\n            # Group the input data by the reduce key(s) while maintaining original order\n            def get_group_key(item):\n                return tuple(item[key] for key in reduce_keys)\n\n            grouped_data = {}\n            for item in input_data:\n                key = get_group_key(item)\n                if key not in grouped_data:\n                    grouped_data[key] = []\n                grouped_data[key].append(item)\n\n            # Convert the grouped data to a list of tuples\n            grouped_data = list(grouped_data.items())\n\n        def process_group(\n            key: Tuple, group_elems: List[Dict]\n        ) -&gt; Tuple[Optional[Dict], float]:\n            if input_schema:\n                group_list = [\n                    {k: item[k] for k in input_schema.keys() if k in item}\n                    for item in group_elems\n                ]\n            else:\n                group_list = group_elems\n\n            total_cost = 0.0\n\n            # Apply value sampling if enabled\n            value_sampling = self.config.get(\"value_sampling\", {})\n            if value_sampling.get(\"enabled\", False):\n                sample_size = min(value_sampling[\"sample_size\"], len(group_list))\n                method = value_sampling[\"method\"]\n\n                if method == \"random\":\n                    group_sample = random.sample(group_list, sample_size)\n                    group_sample.sort(key=lambda x: group_list.index(x))\n                elif method == \"first_n\":\n                    group_sample = group_list[:sample_size]\n                elif method == \"cluster\":\n                    group_sample, embedding_cost = self._cluster_based_sampling(\n                        group_list, value_sampling, sample_size\n                    )\n                    group_sample.sort(key=lambda x: group_list.index(x))\n                    total_cost += embedding_cost\n                elif method == \"sem_sim\":\n                    group_sample, embedding_cost = self._semantic_similarity_sampling(\n                        key, group_list, value_sampling, sample_size\n                    )\n                    group_sample.sort(key=lambda x: group_list.index(x))\n                    total_cost += embedding_cost\n\n                group_list = group_sample\n\n            # Only execute merge-based plans if associative = True\n            if \"merge_prompt\" in self.config and self.config.get(\"associative\", True):\n                result, cost = self._parallel_fold_and_merge(key, group_list)\n            elif \"fold_prompt\" in self.config:\n                result, cost = self._incremental_reduce(key, group_list)\n            else:\n                result, cost = self._batch_reduce(key, group_list)\n\n            total_cost += cost\n\n            # Apply pass-through at the group level\n            if (\n                result is not None\n                and self.config.get(\"pass_through\", False)\n                and group_elems\n            ):\n                for k, v in group_elems[0].items():\n                    if k not in self.config[\"output\"][\"schema\"] and k not in result:\n                        result[k] = v\n\n            return result, total_cost\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(process_group, key, group)\n                for key, group in grouped_data\n            ]\n            results = []\n            total_cost = 0\n            for future in rich_as_completed(\n                futures,\n                total=len(futures),\n                desc=f\"Processing {self.config['name']} (reduce) on all documents\",\n                leave=True,\n                console=self.console,\n            ):\n                output, item_cost = future.result()\n                total_cost += item_cost\n                if output is not None:\n                    results.append(output)\n\n        if self.config.get(\"persist_intermediates\", False):\n            for result in results:\n                key = tuple(result[k] for k in self.config[\"reduce_key\"])\n                if key in self.intermediates:\n                    result[f\"_{self.config['name']}_intermediates\"] = (\n                        self.intermediates[key]\n                    )\n\n        if self.status:\n            self.status.start()\n\n        return results, total_cost\n\n    def _cluster_based_sampling(\n        self, group_list: List[Dict], value_sampling: Dict, sample_size: int\n    ) -&gt; Tuple[List[Dict], float]:\n        if sample_size &gt;= len(group_list):\n            return group_list, 0\n\n        clusters, cost = cluster_documents(\n            group_list, value_sampling, sample_size, self.runner.api\n        )\n\n        sampled_items = []\n        idx_added_already = set()\n        num_clusters = len(clusters)\n        for i in range(sample_size):\n            # Add a random item from the cluster\n            idx = i % num_clusters\n\n            # Skip if there are no items in the cluster\n            if len(clusters[idx]) == 0:\n                continue\n\n            if len(clusters[idx]) == 1:\n                # If there's only one item in the cluster, add it directly if we haven't already\n                if idx not in idx_added_already:\n                    sampled_items.append(clusters[idx][0])\n                continue\n\n            random_choice_idx = random.randint(0, len(clusters[idx]) - 1)\n            max_attempts = 10\n            while random_choice_idx in idx_added_already and max_attempts &gt; 0:\n                random_choice_idx = random.randint(0, len(clusters[idx]) - 1)\n                max_attempts -= 1\n            idx_added_already.add(random_choice_idx)\n            sampled_items.append(clusters[idx][random_choice_idx])\n\n        return sampled_items, cost\n\n    def _semantic_similarity_sampling(\n        self, key: Tuple, group_list: List[Dict], value_sampling: Dict, sample_size: int\n    ) -&gt; Tuple[List[Dict], float]:\n        embedding_model = value_sampling[\"embedding_model\"]\n        query_text_template = Template(value_sampling[\"query_text\"])\n        query_text = query_text_template.render(\n            reduce_key=dict(zip(self.config[\"reduce_key\"], key))\n        )\n\n        embeddings, cost = get_embeddings_for_clustering(\n            group_list, value_sampling, self.runner.api\n        )\n\n        query_response = self.runner.api.gen_embedding(embedding_model, [query_text])\n        query_embedding = query_response[\"data\"][0][\"embedding\"]\n        cost += completion_cost(query_response)\n\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        similarities = cosine_similarity([query_embedding], embeddings)[0]\n\n        top_k_indices = np.argsort(similarities)[-sample_size:]\n\n        return [group_list[i] for i in top_k_indices], cost\n\n    def _parallel_fold_and_merge(\n        self, key: Tuple, group_list: List[Dict]\n    ) -&gt; Tuple[Optional[Dict], float]:\n        \"\"\"\n        Perform parallel folding and merging on a group of items.\n\n        This method implements a strategy that combines parallel folding of input items\n        and merging of intermediate results to efficiently process large groups. It works as follows:\n        1. The input group is initially divided into smaller batches for efficient processing.\n        2. The method performs an initial round of folding operations on these batches.\n        3. After the first round of folds, a few merges are performed to estimate the merge runtime.\n        4. Based on the estimated merge runtime and observed fold runtime, it calculates the optimal number of parallel folds. Subsequent rounds of folding are then performed concurrently, with the number of parallel folds determined by the runtime estimates.\n        5. The folding process repeats in rounds, progressively reducing the number of items to be processed.\n        6. Once all folding operations are complete, the method recursively performs final merges on the fold results to combine them into a final result.\n        7. Throughout this process, the method may adjust the number of parallel folds based on updated performance metrics (i.e., fold and merge runtimes) to maintain efficiency.\n\n        Args:\n            key (Tuple): The reduce key tuple for the group.\n            group_list (List[Dict]): The list of items in the group to be processed.\n\n        Returns:\n            Tuple[Optional[Dict], float]: A tuple containing the final merged result (or None if processing failed)\n            and the total cost of the operation.\n        \"\"\"\n        fold_batch_size = self.config[\"fold_batch_size\"]\n        merge_batch_size = self.config[\"merge_batch_size\"]\n        total_cost = 0\n\n        def calculate_num_parallel_folds():\n            fold_time, fold_default = self.get_fold_time()\n            merge_time, merge_default = self.get_merge_time()\n            num_group_items = len(group_list)\n            return (\n                max(\n                    1,\n                    int(\n                        (fold_time * num_group_items * math.log(merge_batch_size))\n                        / (fold_batch_size * merge_time)\n                    ),\n                ),\n                fold_default or merge_default,\n            )\n\n        num_parallel_folds, used_default_times = calculate_num_parallel_folds()\n        fold_results = []\n        remaining_items = group_list\n\n        if self.config.get(\"persist_intermediates\", False):\n            self.intermediates[key] = []\n            iter_count = 0\n\n        # Parallel folding and merging\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            while remaining_items:\n                # Folding phase\n                fold_futures = []\n                for i in range(min(num_parallel_folds, len(remaining_items))):\n                    batch = remaining_items[:fold_batch_size]\n                    remaining_items = remaining_items[fold_batch_size:]\n                    current_output = fold_results[i] if i &lt; len(fold_results) else None\n                    fold_futures.append(\n                        executor.submit(\n                            self._increment_fold, key, batch, current_output\n                        )\n                    )\n\n                new_fold_results = []\n                for future in as_completed(fold_futures):\n                    result, cost = future.result()\n                    total_cost += cost\n                    if result is not None:\n                        new_fold_results.append(result)\n                        if self.config.get(\"persist_intermediates\", False):\n                            self.intermediates[key].append(\n                                {\n                                    \"iter\": iter_count,\n                                    \"intermediate\": result,\n                                    \"scratchpad\": result[\"updated_scratchpad\"],\n                                }\n                            )\n                            iter_count += 1\n\n                # Update fold_results with new results\n                fold_results = new_fold_results + fold_results[len(new_fold_results) :]\n\n                # Single pass merging phase\n                if (\n                    len(self.merge_times) &lt; self.min_samples\n                    and len(fold_results) &gt;= merge_batch_size\n                ):\n                    merge_futures = []\n                    for i in range(0, len(fold_results), merge_batch_size):\n                        batch = fold_results[i : i + merge_batch_size]\n                        merge_futures.append(\n                            executor.submit(self._merge_results, key, batch)\n                        )\n\n                    new_results = []\n                    for future in as_completed(merge_futures):\n                        result, cost = future.result()\n                        total_cost += cost\n                        if result is not None:\n                            new_results.append(result)\n                            if self.config.get(\"persist_intermediates\", False):\n                                self.intermediates[key].append(\n                                    {\n                                        \"iter\": iter_count,\n                                        \"intermediate\": result,\n                                        \"scratchpad\": None,\n                                    }\n                                )\n                                iter_count += 1\n\n                    fold_results = new_results\n\n                # Recalculate num_parallel_folds if we used default times\n                if used_default_times:\n                    new_num_parallel_folds, used_default_times = (\n                        calculate_num_parallel_folds()\n                    )\n                    if not used_default_times:\n                        self.console.log(\n                            f\"Recalculated num_parallel_folds from {num_parallel_folds} to {new_num_parallel_folds}\"\n                        )\n                        num_parallel_folds = new_num_parallel_folds\n\n        # Final merging if needed\n        while len(fold_results) &gt; 1:\n            self.console.log(f\"Finished folding! Merging {len(fold_results)} items.\")\n            with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n                merge_futures = []\n                for i in range(0, len(fold_results), merge_batch_size):\n                    batch = fold_results[i : i + merge_batch_size]\n                    merge_futures.append(\n                        executor.submit(self._merge_results, key, batch)\n                    )\n\n                new_results = []\n                for future in as_completed(merge_futures):\n                    result, cost = future.result()\n                    total_cost += cost\n                    if result is not None:\n                        new_results.append(result)\n                        if self.config.get(\"persist_intermediates\", False):\n                            self.intermediates[key].append(\n                                {\n                                    \"iter\": iter_count,\n                                    \"intermediate\": result,\n                                    \"scratchpad\": None,\n                                }\n                            )\n                            iter_count += 1\n\n                fold_results = new_results\n\n        return (fold_results[0], total_cost) if fold_results else (None, total_cost)\n\n    def _incremental_reduce(\n        self, key: Tuple, group_list: List[Dict]\n    ) -&gt; Tuple[Optional[Dict], float]:\n        \"\"\"\n        Perform an incremental reduce operation on a group of items.\n\n        This method processes the group in batches, incrementally folding the results.\n\n        Args:\n            key (Tuple): The reduce key tuple for the group.\n            group_list (List[Dict]): The list of items in the group to be processed.\n\n        Returns:\n            Tuple[Optional[Dict], float]: A tuple containing the final reduced result (or None if processing failed)\n            and the total cost of the operation.\n        \"\"\"\n        fold_batch_size = self.config[\"fold_batch_size\"]\n        total_cost = 0\n        current_output = None\n\n        # Calculate and log the number of folds to be performed\n        num_folds = (len(group_list) + fold_batch_size - 1) // fold_batch_size\n\n        scratchpad = \"\"\n        if self.config.get(\"persist_intermediates\", False):\n            self.intermediates[key] = []\n            iter_count = 0\n\n        for i in range(0, len(group_list), fold_batch_size):\n            # Log the current iteration and total number of folds\n            current_fold = i // fold_batch_size + 1\n            if self.config.get(\"verbose\", False):\n                self.console.log(\n                    f\"Processing fold {current_fold} of {num_folds} for group with key {key}\"\n                )\n            batch = group_list[i : i + fold_batch_size]\n\n            folded_output, fold_cost = self._increment_fold(\n                key, batch, current_output, scratchpad\n            )\n            total_cost += fold_cost\n\n            if folded_output is None:\n                continue\n\n            if self.config.get(\"persist_intermediates\", False):\n                self.intermediates[key].append(\n                    {\n                        \"iter\": iter_count,\n                        \"intermediate\": folded_output,\n                        \"scratchpad\": folded_output[\"updated_scratchpad\"],\n                    }\n                )\n                iter_count += 1\n\n            # Pop off updated_scratchpad\n            if \"updated_scratchpad\" in folded_output:\n                scratchpad = folded_output[\"updated_scratchpad\"]\n                if self.config.get(\"verbose\", False):\n                    self.console.log(\n                        f\"Updated scratchpad for fold {current_fold}: {scratchpad}\"\n                    )\n                del folded_output[\"updated_scratchpad\"]\n\n            current_output = folded_output\n\n        return current_output, total_cost\n\n    def validation_fn(self, response: Dict[str, Any]):\n        output = self.runner.api.parse_llm_response(\n            response,\n            schema=self.config[\"output\"][\"schema\"],\n        )[0]\n        if self.runner.api.validate_output(self.config, output, self.console):\n            return output, True\n        return output, False\n\n    def _increment_fold(\n        self,\n        key: Tuple,\n        batch: List[Dict],\n        current_output: Optional[Dict],\n        scratchpad: Optional[str] = None,\n    ) -&gt; Tuple[Optional[Dict], float]:\n        \"\"\"\n        Perform an incremental fold operation on a batch of items.\n\n        This method folds a batch of items into the current output using the fold prompt.\n\n        Args:\n            key (Tuple): The reduce key tuple for the group.\n            batch (List[Dict]): The batch of items to be folded.\n            current_output (Optional[Dict]): The current accumulated output, if any.\n            scratchpad (Optional[str]): The scratchpad to use for the fold operation.\n        Returns:\n            Tuple[Optional[Dict], float]: A tuple containing the folded output (or None if processing failed)\n            and the cost of the fold operation.\n        \"\"\"\n        if current_output is None:\n            return self._batch_reduce(key, batch, scratchpad)\n\n        start_time = time.time()\n        fold_prompt_template = Template(self.config[\"fold_prompt\"])\n        fold_prompt = fold_prompt_template.render(\n            inputs=batch,\n            output=current_output,\n            reduce_key=dict(zip(self.config[\"reduce_key\"], key)),\n        )\n\n        response = self.runner.api.call_llm(\n            self.config.get(\"model\", self.default_model),\n            \"reduce\",\n            [{\"role\": \"user\", \"content\": fold_prompt}],\n            self.config[\"output\"][\"schema\"],\n            scratchpad=scratchpad,\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            validation_config=(\n                {\n                    \"num_retries\": self.num_retries_on_validate_failure,\n                    \"val_rule\": self.config.get(\"validate\", []),\n                    \"validation_fn\": self.validation_fn,\n                }\n                if self.config.get(\"validate\", None)\n                else None\n            ),\n            bypass_cache=self.config.get(\"bypass_cache\", False),\n            verbose=self.config.get(\"verbose\", False),\n        )\n\n        end_time = time.time()\n        self._update_fold_time(end_time - start_time)\n\n        if response.validated:\n            folded_output = self.runner.api.parse_llm_response(\n                response.response,\n                schema=self.config[\"output\"][\"schema\"],\n                manually_fix_errors=self.manually_fix_errors,\n            )[0]\n\n            folded_output.update(dict(zip(self.config[\"reduce_key\"], key)))\n            fold_cost = response.total_cost\n\n            return folded_output, fold_cost\n\n        return None, fold_cost\n\n    def _merge_results(\n        self, key: Tuple, outputs: List[Dict]\n    ) -&gt; Tuple[Optional[Dict], float]:\n        \"\"\"\n        Merge multiple outputs into a single result.\n\n        This method merges a list of outputs using the merge prompt.\n\n        Args:\n            key (Tuple): The reduce key tuple for the group.\n            outputs (List[Dict]): The list of outputs to be merged.\n\n        Returns:\n            Tuple[Optional[Dict], float]: A tuple containing the merged output (or None if processing failed)\n            and the cost of the merge operation.\n        \"\"\"\n        start_time = time.time()\n        merge_prompt_template = Template(self.config[\"merge_prompt\"])\n        merge_prompt = merge_prompt_template.render(\n            outputs=outputs, reduce_key=dict(zip(self.config[\"reduce_key\"], key))\n        )\n        response = self.runner.api.call_llm(\n            self.config.get(\"model\", self.default_model),\n            \"merge\",\n            [{\"role\": \"user\", \"content\": merge_prompt}],\n            self.config[\"output\"][\"schema\"],\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            validation_config=(\n                {\n                    \"num_retries\": self.num_retries_on_validate_failure,\n                    \"val_rule\": self.config.get(\"validate\", []),\n                    \"validation_fn\": self.validation_fn,\n                }\n                if self.config.get(\"validate\", None)\n                else None\n            ),\n            bypass_cache=self.config.get(\"bypass_cache\", False),\n            verbose=self.config.get(\"verbose\", False),\n        )\n\n        end_time = time.time()\n        self._update_merge_time(end_time - start_time)\n\n        if response.validated:\n            merged_output = self.runner.api.parse_llm_response(\n                response.response,\n                schema=self.config[\"output\"][\"schema\"],\n                manually_fix_errors=self.manually_fix_errors,\n            )[0]\n            merged_output.update(dict(zip(self.config[\"reduce_key\"], key)))\n            merge_cost = response.total_cost\n            return merged_output, merge_cost\n\n        return None, merge_cost\n\n    def get_fold_time(self) -&gt; Tuple[float, bool]:\n        \"\"\"\n        Get the average fold time or a default value.\n\n        Returns:\n            Tuple[float, bool]: A tuple containing the average fold time (or default) and a boolean\n            indicating whether the default value was used.\n        \"\"\"\n        if \"fold_time\" in self.config:\n            return self.config[\"fold_time\"], False\n        with self.lock:\n            if len(self.fold_times) &gt;= self.min_samples:\n                return sum(self.fold_times) / len(self.fold_times), False\n        return 1.0, True  # Default to 1 second if no data is available\n\n    def get_merge_time(self) -&gt; Tuple[float, bool]:\n        \"\"\"\n        Get the average merge time or a default value.\n\n        Returns:\n            Tuple[float, bool]: A tuple containing the average merge time (or default) and a boolean\n            indicating whether the default value was used.\n        \"\"\"\n        if \"merge_time\" in self.config:\n            return self.config[\"merge_time\"], False\n        with self.lock:\n            if len(self.merge_times) &gt;= self.min_samples:\n                return sum(self.merge_times) / len(self.merge_times), False\n        return 1.0, True  # Default to 1 second if no data is available\n\n    def _update_fold_time(self, time: float) -&gt; None:\n        \"\"\"\n        Update the fold time statistics.\n\n        Args:\n            time (float): The time taken for a fold operation.\n        \"\"\"\n        with self.lock:\n            self.fold_times.append(time)\n\n    def _update_merge_time(self, time: float) -&gt; None:\n        \"\"\"\n        Update the merge time statistics.\n\n        Args:\n            time (float): The time taken for a merge operation.\n        \"\"\"\n        with self.lock:\n            self.merge_times.append(time)\n\n    def _batch_reduce(\n        self, key: Tuple, group_list: List[Dict], scratchpad: Optional[str] = None\n    ) -&gt; Tuple[Optional[Dict], float]:\n        \"\"\"\n        Perform a batch reduce operation on a group of items.\n\n        This method reduces a group of items into a single output using the reduce prompt.\n\n        Args:\n            key (Tuple): The reduce key tuple for the group.\n            group_list (List[Dict]): The list of items to be reduced.\n            scratchpad (Optional[str]): The scratchpad to use for the reduce operation.\n        Returns:\n            Tuple[Optional[Dict], float]: A tuple containing the reduced output (or None if processing failed)\n            and the cost of the reduce operation.\n        \"\"\"\n        prompt_template = Template(self.config[\"prompt\"])\n        prompt = prompt_template.render(\n            reduce_key=dict(zip(self.config[\"reduce_key\"], key)), inputs=group_list\n        )\n        item_cost = 0\n\n        response = self.runner.api.call_llm(\n            self.config.get(\"model\", self.default_model),\n            \"reduce\",\n            [{\"role\": \"user\", \"content\": prompt}],\n            self.config[\"output\"][\"schema\"],\n            scratchpad=scratchpad,\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            bypass_cache=self.config.get(\"bypass_cache\", False),\n            validation_config=(\n                {\n                    \"num_retries\": self.num_retries_on_validate_failure,\n                    \"val_rule\": self.config.get(\"validate\", []),\n                    \"validation_fn\": self.validation_fn,\n                }\n                if self.config.get(\"validate\", None)\n                else None\n            ),\n            gleaning_config=self.config.get(\"gleaning\", None),\n            verbose=self.config.get(\"verbose\", False),\n        )\n\n        item_cost += response.total_cost\n\n        if response.validated:\n            output = self.runner.api.parse_llm_response(\n                response.response,\n                schema=self.config[\"output\"][\"schema\"],\n                manually_fix_errors=self.manually_fix_errors,\n            )[0]\n            output.update(dict(zip(self.config[\"reduce_key\"], key)))\n\n            return output, item_cost\n        return None, item_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the ReduceOperation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the ReduceOperation.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.min_samples = 5\n    self.max_samples = 1000\n    self.fold_times = deque(maxlen=self.max_samples)\n    self.merge_times = deque(maxlen=self.max_samples)\n    self.lock = Lock()\n    self.config[\"reduce_key\"] = (\n        [self.config[\"reduce_key\"]]\n        if isinstance(self.config[\"reduce_key\"], str)\n        else self.config[\"reduce_key\"]\n    )\n    self.intermediates = {}\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Execute the reduce operation on the provided input data.</p> <p>This method sorts and groups the input data by the reduce key(s), then processes each group using either parallel fold and merge, incremental reduce, or batch reduce strategies.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Dict]</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Dict], float]</code> <p>Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Execute the reduce operation on the provided input data.\n\n    This method sorts and groups the input data by the reduce key(s), then processes each group\n    using either parallel fold and merge, incremental reduce, or batch reduce strategies.\n\n    Args:\n        input_data (List[Dict]): The input data to process.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.\n    \"\"\"\n    if self.config.get(\"gleaning\", {}).get(\"validation_prompt\", None):\n        self.console.log(\n            f\"Using gleaning with validation prompt: {self.config.get('gleaning', {}).get('validation_prompt', '')}\"\n        )\n\n    reduce_keys = self.config[\"reduce_key\"]\n    if isinstance(reduce_keys, str):\n        reduce_keys = [reduce_keys]\n    input_schema = self.config.get(\"input\", {}).get(\"schema\", {})\n\n    if self.status:\n        self.status.stop()\n\n    # Check if we need to group everything into one group\n    if reduce_keys == [\"_all\"] or reduce_keys == \"_all\":\n        grouped_data = [(\"_all\", input_data)]\n    else:\n        # Group the input data by the reduce key(s) while maintaining original order\n        def get_group_key(item):\n            return tuple(item[key] for key in reduce_keys)\n\n        grouped_data = {}\n        for item in input_data:\n            key = get_group_key(item)\n            if key not in grouped_data:\n                grouped_data[key] = []\n            grouped_data[key].append(item)\n\n        # Convert the grouped data to a list of tuples\n        grouped_data = list(grouped_data.items())\n\n    def process_group(\n        key: Tuple, group_elems: List[Dict]\n    ) -&gt; Tuple[Optional[Dict], float]:\n        if input_schema:\n            group_list = [\n                {k: item[k] for k in input_schema.keys() if k in item}\n                for item in group_elems\n            ]\n        else:\n            group_list = group_elems\n\n        total_cost = 0.0\n\n        # Apply value sampling if enabled\n        value_sampling = self.config.get(\"value_sampling\", {})\n        if value_sampling.get(\"enabled\", False):\n            sample_size = min(value_sampling[\"sample_size\"], len(group_list))\n            method = value_sampling[\"method\"]\n\n            if method == \"random\":\n                group_sample = random.sample(group_list, sample_size)\n                group_sample.sort(key=lambda x: group_list.index(x))\n            elif method == \"first_n\":\n                group_sample = group_list[:sample_size]\n            elif method == \"cluster\":\n                group_sample, embedding_cost = self._cluster_based_sampling(\n                    group_list, value_sampling, sample_size\n                )\n                group_sample.sort(key=lambda x: group_list.index(x))\n                total_cost += embedding_cost\n            elif method == \"sem_sim\":\n                group_sample, embedding_cost = self._semantic_similarity_sampling(\n                    key, group_list, value_sampling, sample_size\n                )\n                group_sample.sort(key=lambda x: group_list.index(x))\n                total_cost += embedding_cost\n\n            group_list = group_sample\n\n        # Only execute merge-based plans if associative = True\n        if \"merge_prompt\" in self.config and self.config.get(\"associative\", True):\n            result, cost = self._parallel_fold_and_merge(key, group_list)\n        elif \"fold_prompt\" in self.config:\n            result, cost = self._incremental_reduce(key, group_list)\n        else:\n            result, cost = self._batch_reduce(key, group_list)\n\n        total_cost += cost\n\n        # Apply pass-through at the group level\n        if (\n            result is not None\n            and self.config.get(\"pass_through\", False)\n            and group_elems\n        ):\n            for k, v in group_elems[0].items():\n                if k not in self.config[\"output\"][\"schema\"] and k not in result:\n                    result[k] = v\n\n        return result, total_cost\n\n    with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n        futures = [\n            executor.submit(process_group, key, group)\n            for key, group in grouped_data\n        ]\n        results = []\n        total_cost = 0\n        for future in rich_as_completed(\n            futures,\n            total=len(futures),\n            desc=f\"Processing {self.config['name']} (reduce) on all documents\",\n            leave=True,\n            console=self.console,\n        ):\n            output, item_cost = future.result()\n            total_cost += item_cost\n            if output is not None:\n                results.append(output)\n\n    if self.config.get(\"persist_intermediates\", False):\n        for result in results:\n            key = tuple(result[k] for k in self.config[\"reduce_key\"])\n            if key in self.intermediates:\n                result[f\"_{self.config['name']}_intermediates\"] = (\n                    self.intermediates[key]\n                )\n\n    if self.status:\n        self.status.start()\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.get_fold_time","title":"<code>get_fold_time()</code>","text":"<p>Get the average fold time or a default value.</p> <p>Returns:</p> Type Description <code>float</code> <p>Tuple[float, bool]: A tuple containing the average fold time (or default) and a boolean</p> <code>bool</code> <p>indicating whether the default value was used.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def get_fold_time(self) -&gt; Tuple[float, bool]:\n    \"\"\"\n    Get the average fold time or a default value.\n\n    Returns:\n        Tuple[float, bool]: A tuple containing the average fold time (or default) and a boolean\n        indicating whether the default value was used.\n    \"\"\"\n    if \"fold_time\" in self.config:\n        return self.config[\"fold_time\"], False\n    with self.lock:\n        if len(self.fold_times) &gt;= self.min_samples:\n            return sum(self.fold_times) / len(self.fold_times), False\n    return 1.0, True  # Default to 1 second if no data is available\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.get_merge_time","title":"<code>get_merge_time()</code>","text":"<p>Get the average merge time or a default value.</p> <p>Returns:</p> Type Description <code>float</code> <p>Tuple[float, bool]: A tuple containing the average merge time (or default) and a boolean</p> <code>bool</code> <p>indicating whether the default value was used.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def get_merge_time(self) -&gt; Tuple[float, bool]:\n    \"\"\"\n    Get the average merge time or a default value.\n\n    Returns:\n        Tuple[float, bool]: A tuple containing the average merge time (or default) and a boolean\n        indicating whether the default value was used.\n    \"\"\"\n    if \"merge_time\" in self.config:\n        return self.config[\"merge_time\"], False\n    with self.lock:\n        if len(self.merge_times) &gt;= self.min_samples:\n            return sum(self.merge_times) / len(self.merge_times), False\n    return 1.0, True  # Default to 1 second if no data is available\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.reduce.ReduceOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Perform comprehensive syntax checks on the configuration of the ReduceOperation.</p> <p>This method validates the presence and correctness of all required configuration keys, Jinja2 templates, and ensures the correct structure and types of the entire configuration.</p> <p>The method performs the following checks: 1. Verifies the presence of all required keys in the configuration. 2. Validates the structure and content of the 'output' configuration, including its 'schema'. 3. Checks if the main 'prompt' is a valid Jinja2 template and contains the required 'inputs' variable. 4. If 'merge_prompt' is specified, ensures that 'fold_prompt' is also present. 5. If 'fold_prompt' is present, verifies the existence of 'fold_batch_size'. 6. Validates the 'fold_prompt' as a Jinja2 template with required variables 'inputs' and 'output'. 7. If present, checks 'merge_prompt' as a valid Jinja2 template with required 'outputs' variable. 8. Verifies types of various configuration inputs (e.g., 'fold_batch_size' as int). 9. Checks for the presence and validity of optional configurations like 'model'.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any required configuration is missing, if templates are invalid or missing required         variables, or if any other configuration aspect is incorrect or inconsistent.</p> <code>TypeError</code> <p>If any configuration value has an incorrect type, such as 'schema' not being a dict        or 'fold_batch_size' not being an integer.</p> Source code in <code>docetl/operations/reduce.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Perform comprehensive syntax checks on the configuration of the ReduceOperation.\n\n    This method validates the presence and correctness of all required configuration keys, Jinja2 templates, and ensures the correct\n    structure and types of the entire configuration.\n\n    The method performs the following checks:\n    1. Verifies the presence of all required keys in the configuration.\n    2. Validates the structure and content of the 'output' configuration, including its 'schema'.\n    3. Checks if the main 'prompt' is a valid Jinja2 template and contains the required 'inputs' variable.\n    4. If 'merge_prompt' is specified, ensures that 'fold_prompt' is also present.\n    5. If 'fold_prompt' is present, verifies the existence of 'fold_batch_size'.\n    6. Validates the 'fold_prompt' as a Jinja2 template with required variables 'inputs' and 'output'.\n    7. If present, checks 'merge_prompt' as a valid Jinja2 template with required 'outputs' variable.\n    8. Verifies types of various configuration inputs (e.g., 'fold_batch_size' as int).\n    9. Checks for the presence and validity of optional configurations like 'model'.\n\n    Raises:\n        ValueError: If any required configuration is missing, if templates are invalid or missing required\n                    variables, or if any other configuration aspect is incorrect or inconsistent.\n        TypeError: If any configuration value has an incorrect type, such as 'schema' not being a dict\n                   or 'fold_batch_size' not being an integer.\n    \"\"\"\n    required_keys = [\"reduce_key\", \"prompt\", \"output\"]\n    for key in required_keys:\n        if key not in self.config:\n            raise ValueError(\n                f\"Missing required key '{key}' in {self.config['name']} configuration\"\n            )\n\n    if \"schema\" not in self.config[\"output\"]:\n        raise ValueError(\n            f\"Missing 'schema' in {self.config['name']} 'output' configuration\"\n        )\n\n    if not isinstance(self.config[\"output\"][\"schema\"], dict):\n        raise TypeError(\n            f\"'schema' in {self.config['name']} 'output' configuration must be a dictionary\"\n        )\n\n    if not self.config[\"output\"][\"schema\"]:\n        raise ValueError(\n            f\"'schema' in {self.config['name']} 'output' configuration cannot be empty\"\n        )\n\n    # Check if the prompt is a valid Jinja2 template\n    try:\n        template = Template(self.config[\"prompt\"])\n        template_vars = template.environment.parse(self.config[\"prompt\"]).find_all(\n            jinja2.nodes.Name\n        )\n        template_var_names = {var.name for var in template_vars}\n        if \"inputs\" not in template_var_names:\n            raise ValueError(\n                f\"Prompt template for {self.config['name']} must include the 'inputs' variable\"\n            )\n    except Exception as e:\n        raise ValueError(\n            f\"Invalid Jinja2 template in {self.config['name']} 'prompt': {str(e)}\"\n        )\n\n    # Check if fold_prompt is a valid Jinja2 template (now required if merge exists)\n    if \"merge_prompt\" in self.config:\n        if \"fold_prompt\" not in self.config:\n            raise ValueError(\n                f\"'fold_prompt' is required when 'merge_prompt' is specified in {self.config['name']}\"\n            )\n\n    if \"fold_prompt\" in self.config:\n        if \"fold_batch_size\" not in self.config:\n            raise ValueError(\n                f\"'fold_batch_size' is required when 'fold_prompt' is specified in {self.config['name']}\"\n            )\n\n        try:\n            fold_template = Template(self.config[\"fold_prompt\"])\n            fold_template_vars = fold_template.environment.parse(\n                self.config[\"fold_prompt\"]\n            ).find_all(jinja2.nodes.Name)\n            fold_template_var_names = {var.name for var in fold_template_vars}\n            required_vars = {\"inputs\", \"output\"}\n            if not required_vars.issubset(fold_template_var_names):\n                raise ValueError(\n                    f\"Fold template in {self.config['name']} must include variables: {required_vars}. Current template includes: {fold_template_var_names}\"\n                )\n        except Exception as e:\n            raise ValueError(\n                f\"Invalid Jinja2 template in {self.config['name']} 'fold_prompt': {str(e)}\"\n            )\n\n    # Check merge_prompt and merge_batch_size\n    if \"merge_prompt\" in self.config:\n        if \"merge_batch_size\" not in self.config:\n            raise ValueError(\n                f\"'merge_batch_size' is required when 'merge_prompt' is specified in {self.config['name']}\"\n            )\n\n        try:\n            merge_template = Template(self.config[\"merge_prompt\"])\n            merge_template_vars = merge_template.environment.parse(\n                self.config[\"merge_prompt\"]\n            ).find_all(jinja2.nodes.Name)\n            merge_template_var_names = {var.name for var in merge_template_vars}\n            if \"outputs\" not in merge_template_var_names:\n                raise ValueError(\n                    f\"Merge template in {self.config['name']} must include the 'outputs' variable\"\n                )\n        except Exception as e:\n            raise ValueError(\n                f\"Invalid Jinja2 template in {self.config['name']} 'merge_prompt': {str(e)}\"\n            )\n\n    # Check if the model is specified (optional)\n    if \"model\" in self.config and not isinstance(self.config[\"model\"], str):\n        raise TypeError(\n            f\"'model' in {self.config['name']} configuration must be a string\"\n        )\n\n    # Check if reduce_key is a string or a list of strings\n    if not isinstance(self.config[\"reduce_key\"], (str, list)):\n        raise TypeError(\n            f\"'reduce_key' in {self.config['name']} configuration must be a string or a list of strings\"\n        )\n    if isinstance(self.config[\"reduce_key\"], list):\n        if not all(isinstance(key, str) for key in self.config[\"reduce_key\"]):\n            raise TypeError(\n                f\"All elements in 'reduce_key' list in {self.config['name']} configuration must be strings\"\n            )\n\n    # Check if input schema is provided and valid (optional)\n    if \"input\" in self.config:\n        if \"schema\" not in self.config[\"input\"]:\n            raise ValueError(\n                f\"Missing 'schema' in {self.config['name']} 'input' configuration\"\n            )\n        if not isinstance(self.config[\"input\"][\"schema\"], dict):\n            raise TypeError(\n                f\"'schema' in {self.config['name']} 'input' configuration must be a dictionary\"\n            )\n\n    # Check if fold_batch_size and merge_batch_size are positive integers\n    for key in [\"fold_batch_size\", \"merge_batch_size\"]:\n        if key in self.config:\n            if not isinstance(self.config[key], int) or self.config[key] &lt;= 0:\n                raise ValueError(\n                    f\"'{key}' in {self.config['name']} configuration must be a positive integer\"\n                )\n\n    if \"value_sampling\" in self.config:\n        sampling = self.config[\"value_sampling\"]\n        if not isinstance(sampling, dict):\n            raise TypeError(\n                f\"'value_sampling' in {self.config['name']} configuration must be a dictionary\"\n            )\n\n        if \"enabled\" not in sampling:\n            raise ValueError(\n                f\"'enabled' is required in {self.config['name']} 'value_sampling' configuration\"\n            )\n        if not isinstance(sampling[\"enabled\"], bool):\n            raise TypeError(\n                f\"'enabled' in {self.config['name']} 'value_sampling' configuration must be a boolean\"\n            )\n\n        if sampling[\"enabled\"]:\n            if \"sample_size\" not in sampling:\n                raise ValueError(\n                    f\"'sample_size' is required when value_sampling is enabled in {self.config['name']}\"\n                )\n            if (\n                not isinstance(sampling[\"sample_size\"], int)\n                or sampling[\"sample_size\"] &lt;= 0\n            ):\n                raise ValueError(\n                    f\"'sample_size' in {self.config['name']} configuration must be a positive integer\"\n                )\n\n            if \"method\" not in sampling:\n                raise ValueError(\n                    f\"'method' is required when value_sampling is enabled in {self.config['name']}\"\n                )\n            if sampling[\"method\"] not in [\n                \"random\",\n                \"first_n\",\n                \"cluster\",\n                \"sem_sim\",\n            ]:\n                raise ValueError(\n                    f\"Invalid 'method'. Must be 'random', 'first_n', or 'embedding' in {self.config['name']}\"\n                )\n\n            if sampling[\"method\"] == \"embedding\":\n                if \"embedding_model\" not in sampling:\n                    raise ValueError(\n                        f\"'embedding_model' is required when using embedding-based sampling in {self.config['name']}\"\n                    )\n                if \"embedding_keys\" not in sampling:\n                    raise ValueError(\n                        f\"'embedding_keys' is required when using embedding-based sampling in {self.config['name']}\"\n                    )\n\n    self.gleaning_check()\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.map.ParallelMapOperation","title":"<code>docetl.operations.map.ParallelMapOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/map.py</code> <pre><code>class ParallelMapOperation(BaseOperation):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Checks the configuration of the ParallelMapOperation for required keys and valid structure.\n\n        Raises:\n            ValueError: If required keys are missing or if the configuration structure is invalid.\n            TypeError: If the configuration values have incorrect types.\n        \"\"\"\n        if \"drop_keys\" in self.config:\n            if not isinstance(self.config[\"drop_keys\"], list):\n                raise TypeError(\n                    \"'drop_keys' in configuration must be a list of strings\"\n                )\n            for key in self.config[\"drop_keys\"]:\n                if not isinstance(key, str):\n                    raise TypeError(\"All items in 'drop_keys' must be strings\")\n        elif \"prompts\" not in self.config:\n            raise ValueError(\n                \"If 'drop_keys' is not specified, 'prompts' must be present in the configuration\"\n            )\n\n        if \"prompts\" in self.config:\n            if not isinstance(self.config[\"prompts\"], list):\n                raise ValueError(\n                    \"ParallelMapOperation requires a 'prompts' list in the configuration\"\n                )\n\n            if not self.config[\"prompts\"]:\n                raise ValueError(\"The 'prompts' list cannot be empty\")\n\n            for i, prompt_config in enumerate(self.config[\"prompts\"]):\n                if not isinstance(prompt_config, dict):\n                    raise TypeError(f\"Prompt configuration {i} must be a dictionary\")\n\n                required_keys = [\"prompt\", \"output_keys\"]\n                for key in required_keys:\n                    if key not in prompt_config:\n                        raise ValueError(\n                            f\"Missing required key '{key}' in prompt configuration {i}\"\n                        )\n                if not isinstance(prompt_config[\"prompt\"], str):\n                    raise TypeError(\n                        f\"'prompt' in prompt configuration {i} must be a string\"\n                    )\n\n                if not isinstance(prompt_config[\"output_keys\"], list):\n                    raise TypeError(\n                        f\"'output_keys' in prompt configuration {i} must be a list\"\n                    )\n\n                if not prompt_config[\"output_keys\"]:\n                    raise ValueError(\n                        f\"'output_keys' list in prompt configuration {i} cannot be empty\"\n                    )\n\n                # Check if the prompt is a valid Jinja2 template\n                try:\n                    Template(prompt_config[\"prompt\"])\n                except Exception as e:\n                    raise ValueError(\n                        f\"Invalid Jinja2 template in prompt configuration {i}: {str(e)}\"\n                    ) from e\n\n                # Check if the model is specified (optional)\n                if \"model\" in prompt_config and not isinstance(\n                    prompt_config[\"model\"], str\n                ):\n                    raise TypeError(\n                        f\"'model' in prompt configuration {i} must be a string\"\n                    )\n\n            # Check if all output schema keys are covered by the prompts\n            output_schema = self.config[\"output\"][\"schema\"]\n            output_keys_covered = set()\n            for prompt_config in self.config[\"prompts\"]:\n                output_keys_covered.update(prompt_config[\"output_keys\"])\n\n            missing_keys = set(output_schema.keys()) - output_keys_covered\n            if missing_keys:\n                raise ValueError(\n                    f\"The following output schema keys are not covered by any prompt: {missing_keys}\"\n                )\n\n    def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Executes the parallel map operation on the provided input data.\n\n        Args:\n            input_data (List[Dict]): The input data to process.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.\n\n        This method performs the following steps:\n        1. If prompts are specified, it processes each input item using multiple prompts in parallel\n        2. Aggregates results from different prompts for each input item\n        3. Validates the combined output for each item\n        4. If drop_keys is specified, it drops the specified keys from each document\n        5. Calculates total cost of the operation\n        \"\"\"\n        results = {}\n        total_cost = 0\n        output_schema = self.config.get(\"output\", {}).get(\"schema\", {})\n\n        # Check if there's no prompt and only drop_keys\n        if \"prompts\" not in self.config and \"drop_keys\" in self.config:\n            # If only drop_keys is specified, simply drop the keys and return\n            dropped_results = []\n            for item in input_data:\n                new_item = {\n                    k: v for k, v in item.items() if k not in self.config[\"drop_keys\"]\n                }\n                dropped_results.append(new_item)\n            return dropped_results, 0.0  # Return the modified data with no cost\n\n        if self.status:\n            self.status.stop()\n\n        def process_prompt(item, prompt_config):\n            prompt = render_jinja_template(prompt_config[\"prompt\"], item)\n            local_output_schema = {\n                key: output_schema[key] for key in prompt_config[\"output_keys\"]\n            }\n\n            # Start of Selection\n            # If there are tools, we need to pass in the tools\n            response = self.runner.api.call_llm(\n                prompt_config.get(\"model\", self.default_model),\n                \"parallel_map\",\n                [{\"role\": \"user\", \"content\": prompt}],\n                local_output_schema,\n                tools=prompt_config.get(\"tools\", None),\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n                bypass_cache=self.config.get(\"bypass_cache\", False),\n            )\n            output = self.runner.api.parse_llm_response(\n                response.response,\n                schema=local_output_schema,\n                tools=prompt_config.get(\"tools\", None),\n                manually_fix_errors=self.manually_fix_errors,\n            )[0]\n            return output, response.total_cost\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            if \"prompts\" in self.config:\n                # Create all futures at once\n                all_futures = [\n                    executor.submit(process_prompt, item, prompt_config)\n                    for item in input_data\n                    for prompt_config in self.config[\"prompts\"]\n                ]\n\n                # Process results in order\n                for i in tqdm(\n                    range(len(all_futures)),\n                    desc=\"Processing parallel map items\",\n                ):\n                    future = all_futures[i]\n                    output, cost = future.result()\n                    total_cost += cost\n\n                    # Determine which item this future corresponds to\n                    item_index = i // len(self.config[\"prompts\"])\n                    prompt_index = i % len(self.config[\"prompts\"])\n\n                    # Initialize or update the item_result\n                    if prompt_index == 0:\n                        item_result = input_data[item_index].copy()\n                        results[item_index] = item_result\n\n                    # Fetch the item_result\n                    item_result = results[item_index]\n\n                    # Update the item_result with the output\n                    item_result.update(output)\n\n            else:\n                results = {i: item.copy() for i, item in enumerate(input_data)}\n\n        # Apply drop_keys if specified\n        if \"drop_keys\" in self.config:\n            drop_keys = self.config[\"drop_keys\"]\n            for item in results.values():\n                for key in drop_keys:\n                    item.pop(key, None)\n\n        if self.status:\n            self.status.start()\n\n        # Return the results in order\n        return [results[i] for i in range(len(input_data)) if i in results], total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.map.ParallelMapOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Executes the parallel map operation on the provided input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Dict]</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Dict], float]</code> <p>Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.</p> <p>This method performs the following steps: 1. If prompts are specified, it processes each input item using multiple prompts in parallel 2. Aggregates results from different prompts for each input item 3. Validates the combined output for each item 4. If drop_keys is specified, it drops the specified keys from each document 5. Calculates total cost of the operation</p> Source code in <code>docetl/operations/map.py</code> <pre><code>def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Executes the parallel map operation on the provided input data.\n\n    Args:\n        input_data (List[Dict]): The input data to process.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the processed results and the total cost of the operation.\n\n    This method performs the following steps:\n    1. If prompts are specified, it processes each input item using multiple prompts in parallel\n    2. Aggregates results from different prompts for each input item\n    3. Validates the combined output for each item\n    4. If drop_keys is specified, it drops the specified keys from each document\n    5. Calculates total cost of the operation\n    \"\"\"\n    results = {}\n    total_cost = 0\n    output_schema = self.config.get(\"output\", {}).get(\"schema\", {})\n\n    # Check if there's no prompt and only drop_keys\n    if \"prompts\" not in self.config and \"drop_keys\" in self.config:\n        # If only drop_keys is specified, simply drop the keys and return\n        dropped_results = []\n        for item in input_data:\n            new_item = {\n                k: v for k, v in item.items() if k not in self.config[\"drop_keys\"]\n            }\n            dropped_results.append(new_item)\n        return dropped_results, 0.0  # Return the modified data with no cost\n\n    if self.status:\n        self.status.stop()\n\n    def process_prompt(item, prompt_config):\n        prompt = render_jinja_template(prompt_config[\"prompt\"], item)\n        local_output_schema = {\n            key: output_schema[key] for key in prompt_config[\"output_keys\"]\n        }\n\n        # Start of Selection\n        # If there are tools, we need to pass in the tools\n        response = self.runner.api.call_llm(\n            prompt_config.get(\"model\", self.default_model),\n            \"parallel_map\",\n            [{\"role\": \"user\", \"content\": prompt}],\n            local_output_schema,\n            tools=prompt_config.get(\"tools\", None),\n            timeout_seconds=self.config.get(\"timeout\", 120),\n            max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n            bypass_cache=self.config.get(\"bypass_cache\", False),\n        )\n        output = self.runner.api.parse_llm_response(\n            response.response,\n            schema=local_output_schema,\n            tools=prompt_config.get(\"tools\", None),\n            manually_fix_errors=self.manually_fix_errors,\n        )[0]\n        return output, response.total_cost\n\n    with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n        if \"prompts\" in self.config:\n            # Create all futures at once\n            all_futures = [\n                executor.submit(process_prompt, item, prompt_config)\n                for item in input_data\n                for prompt_config in self.config[\"prompts\"]\n            ]\n\n            # Process results in order\n            for i in tqdm(\n                range(len(all_futures)),\n                desc=\"Processing parallel map items\",\n            ):\n                future = all_futures[i]\n                output, cost = future.result()\n                total_cost += cost\n\n                # Determine which item this future corresponds to\n                item_index = i // len(self.config[\"prompts\"])\n                prompt_index = i % len(self.config[\"prompts\"])\n\n                # Initialize or update the item_result\n                if prompt_index == 0:\n                    item_result = input_data[item_index].copy()\n                    results[item_index] = item_result\n\n                # Fetch the item_result\n                item_result = results[item_index]\n\n                # Update the item_result with the output\n                item_result.update(output)\n\n        else:\n            results = {i: item.copy() for i, item in enumerate(input_data)}\n\n    # Apply drop_keys if specified\n    if \"drop_keys\" in self.config:\n        drop_keys = self.config[\"drop_keys\"]\n        for item in results.values():\n            for key in drop_keys:\n                item.pop(key, None)\n\n    if self.status:\n        self.status.start()\n\n    # Return the results in order\n    return [results[i] for i in range(len(input_data)) if i in results], total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.map.ParallelMapOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Checks the configuration of the ParallelMapOperation for required keys and valid structure.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing or if the configuration structure is invalid.</p> <code>TypeError</code> <p>If the configuration values have incorrect types.</p> Source code in <code>docetl/operations/map.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Checks the configuration of the ParallelMapOperation for required keys and valid structure.\n\n    Raises:\n        ValueError: If required keys are missing or if the configuration structure is invalid.\n        TypeError: If the configuration values have incorrect types.\n    \"\"\"\n    if \"drop_keys\" in self.config:\n        if not isinstance(self.config[\"drop_keys\"], list):\n            raise TypeError(\n                \"'drop_keys' in configuration must be a list of strings\"\n            )\n        for key in self.config[\"drop_keys\"]:\n            if not isinstance(key, str):\n                raise TypeError(\"All items in 'drop_keys' must be strings\")\n    elif \"prompts\" not in self.config:\n        raise ValueError(\n            \"If 'drop_keys' is not specified, 'prompts' must be present in the configuration\"\n        )\n\n    if \"prompts\" in self.config:\n        if not isinstance(self.config[\"prompts\"], list):\n            raise ValueError(\n                \"ParallelMapOperation requires a 'prompts' list in the configuration\"\n            )\n\n        if not self.config[\"prompts\"]:\n            raise ValueError(\"The 'prompts' list cannot be empty\")\n\n        for i, prompt_config in enumerate(self.config[\"prompts\"]):\n            if not isinstance(prompt_config, dict):\n                raise TypeError(f\"Prompt configuration {i} must be a dictionary\")\n\n            required_keys = [\"prompt\", \"output_keys\"]\n            for key in required_keys:\n                if key not in prompt_config:\n                    raise ValueError(\n                        f\"Missing required key '{key}' in prompt configuration {i}\"\n                    )\n            if not isinstance(prompt_config[\"prompt\"], str):\n                raise TypeError(\n                    f\"'prompt' in prompt configuration {i} must be a string\"\n                )\n\n            if not isinstance(prompt_config[\"output_keys\"], list):\n                raise TypeError(\n                    f\"'output_keys' in prompt configuration {i} must be a list\"\n                )\n\n            if not prompt_config[\"output_keys\"]:\n                raise ValueError(\n                    f\"'output_keys' list in prompt configuration {i} cannot be empty\"\n                )\n\n            # Check if the prompt is a valid Jinja2 template\n            try:\n                Template(prompt_config[\"prompt\"])\n            except Exception as e:\n                raise ValueError(\n                    f\"Invalid Jinja2 template in prompt configuration {i}: {str(e)}\"\n                ) from e\n\n            # Check if the model is specified (optional)\n            if \"model\" in prompt_config and not isinstance(\n                prompt_config[\"model\"], str\n            ):\n                raise TypeError(\n                    f\"'model' in prompt configuration {i} must be a string\"\n                )\n\n        # Check if all output schema keys are covered by the prompts\n        output_schema = self.config[\"output\"][\"schema\"]\n        output_keys_covered = set()\n        for prompt_config in self.config[\"prompts\"]:\n            output_keys_covered.update(prompt_config[\"output_keys\"])\n\n        missing_keys = set(output_schema.keys()) - output_keys_covered\n        if missing_keys:\n            raise ValueError(\n                f\"The following output schema keys are not covered by any prompt: {missing_keys}\"\n            )\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.filter.FilterOperation","title":"<code>docetl.operations.filter.FilterOperation</code>","text":"<p>               Bases: <code>MapOperation</code></p> Source code in <code>docetl/operations/filter.py</code> <pre><code>class FilterOperation(MapOperation):\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Checks the configuration of the FilterOperation for required keys and valid structure.\n\n        Raises:\n            ValueError: If required keys are missing or if the output schema structure is invalid.\n            TypeError: If the schema in the output configuration is not a dictionary or if the schema value is not of type bool.\n\n        This method checks for the following:\n        - Presence of required keys: 'prompt' and 'output'\n        - Presence of 'schema' in the 'output' configuration\n        - The 'schema' is a non-empty dictionary with exactly one key-value pair\n        - The value in the schema is of type bool\n        \"\"\"\n        required_keys = [\"prompt\", \"output\"]\n        for key in required_keys:\n            if key not in self.config:\n                raise ValueError(\n                    f\"Missing required key '{key}' in FilterOperation configuration\"\n                )\n\n        if \"schema\" not in self.config[\"output\"]:\n            raise ValueError(\"Missing 'schema' in 'output' configuration\")\n\n        if not isinstance(self.config[\"output\"][\"schema\"], dict):\n            raise TypeError(\"'schema' in 'output' configuration must be a dictionary\")\n\n        if not self.config[\"output\"][\"schema\"]:\n            raise ValueError(\"'schema' in 'output' configuration cannot be empty\")\n\n        schema = self.config[\"output\"][\"schema\"]\n        if \"_short_explanation\" in schema:\n            schema = {k: v for k, v in schema.items() if k != \"_short_explanation\"}\n        if len(schema) != 1:\n            raise ValueError(\n                \"The 'schema' in 'output' configuration must have exactly one key-value pair that maps to a boolean value\"\n            )\n\n        key, value = next(iter(schema.items()))\n        if value not in [\"bool\", \"boolean\"]:\n            raise TypeError(\n                f\"The value in the 'schema' must be of type bool, got {value}\"\n            )\n\n    def execute(\n        self, input_data: List[Dict], is_build: bool = False\n    ) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Executes the filter operation on the input data.\n\n        Args:\n            input_data (List[Dict]): A list of dictionaries to process.\n            is_build (bool): Whether the operation is being executed in the build phase. Defaults to False.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the filtered list of dictionaries\n            and the total cost of the operation.\n\n        This method performs the following steps:\n        1. Processes each input item using an LLM model\n        2. Validates the output\n        3. Filters the results based on the specified filter key\n        4. Calculates the total cost of the operation\n\n        The method uses multi-threading to process items in parallel, improving performance\n        for large datasets.\n\n        Usage:\n        ```python\n        from docetl.operations import FilterOperation\n\n        config = {\n            \"prompt\": \"Determine if the following item is important: {{input}}\",\n            \"output\": {\n                \"schema\": {\"is_important\": \"bool\"}\n            },\n            \"model\": \"gpt-3.5-turbo\"\n        }\n        filter_op = FilterOperation(config)\n        input_data = [\n            {\"id\": 1, \"text\": \"Critical update\"},\n            {\"id\": 2, \"text\": \"Regular maintenance\"}\n        ]\n        results, cost = filter_op.execute(input_data)\n        print(f\"Filtered results: {results}\")\n        print(f\"Total cost: {cost}\")\n        ```\n        \"\"\"\n        filter_key = next(\n            iter(\n                [\n                    k\n                    for k in self.config[\"output\"][\"schema\"].keys()\n                    if k != \"_short_explanation\"\n                ]\n            )\n        )\n\n        results, total_cost = super().execute(input_data)\n\n        # Drop records with filter_key values that are False\n        results = [result for result in results if result[filter_key]]\n\n        return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.filter.FilterOperation.execute","title":"<code>execute(input_data, is_build=False)</code>","text":"<p>Executes the filter operation on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Dict]</code> <p>A list of dictionaries to process.</p> required <code>is_build</code> <code>bool</code> <p>Whether the operation is being executed in the build phase. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>Tuple[List[Dict], float]: A tuple containing the filtered list of dictionaries</p> <code>float</code> <p>and the total cost of the operation.</p> <p>This method performs the following steps: 1. Processes each input item using an LLM model 2. Validates the output 3. Filters the results based on the specified filter key 4. Calculates the total cost of the operation</p> <p>The method uses multi-threading to process items in parallel, improving performance for large datasets.</p> <p>Usage: <pre><code>from docetl.operations import FilterOperation\n\nconfig = {\n    \"prompt\": \"Determine if the following item is important: {{input}}\",\n    \"output\": {\n        \"schema\": {\"is_important\": \"bool\"}\n    },\n    \"model\": \"gpt-3.5-turbo\"\n}\nfilter_op = FilterOperation(config)\ninput_data = [\n    {\"id\": 1, \"text\": \"Critical update\"},\n    {\"id\": 2, \"text\": \"Regular maintenance\"}\n]\nresults, cost = filter_op.execute(input_data)\nprint(f\"Filtered results: {results}\")\nprint(f\"Total cost: {cost}\")\n</code></pre></p> Source code in <code>docetl/operations/filter.py</code> <pre><code>def execute(\n    self, input_data: List[Dict], is_build: bool = False\n) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Executes the filter operation on the input data.\n\n    Args:\n        input_data (List[Dict]): A list of dictionaries to process.\n        is_build (bool): Whether the operation is being executed in the build phase. Defaults to False.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the filtered list of dictionaries\n        and the total cost of the operation.\n\n    This method performs the following steps:\n    1. Processes each input item using an LLM model\n    2. Validates the output\n    3. Filters the results based on the specified filter key\n    4. Calculates the total cost of the operation\n\n    The method uses multi-threading to process items in parallel, improving performance\n    for large datasets.\n\n    Usage:\n    ```python\n    from docetl.operations import FilterOperation\n\n    config = {\n        \"prompt\": \"Determine if the following item is important: {{input}}\",\n        \"output\": {\n            \"schema\": {\"is_important\": \"bool\"}\n        },\n        \"model\": \"gpt-3.5-turbo\"\n    }\n    filter_op = FilterOperation(config)\n    input_data = [\n        {\"id\": 1, \"text\": \"Critical update\"},\n        {\"id\": 2, \"text\": \"Regular maintenance\"}\n    ]\n    results, cost = filter_op.execute(input_data)\n    print(f\"Filtered results: {results}\")\n    print(f\"Total cost: {cost}\")\n    ```\n    \"\"\"\n    filter_key = next(\n        iter(\n            [\n                k\n                for k in self.config[\"output\"][\"schema\"].keys()\n                if k != \"_short_explanation\"\n            ]\n        )\n    )\n\n    results, total_cost = super().execute(input_data)\n\n    # Drop records with filter_key values that are False\n    results = [result for result in results if result[filter_key]]\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.filter.FilterOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Checks the configuration of the FilterOperation for required keys and valid structure.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing or if the output schema structure is invalid.</p> <code>TypeError</code> <p>If the schema in the output configuration is not a dictionary or if the schema value is not of type bool.</p> <p>This method checks for the following: - Presence of required keys: 'prompt' and 'output' - Presence of 'schema' in the 'output' configuration - The 'schema' is a non-empty dictionary with exactly one key-value pair - The value in the schema is of type bool</p> Source code in <code>docetl/operations/filter.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Checks the configuration of the FilterOperation for required keys and valid structure.\n\n    Raises:\n        ValueError: If required keys are missing or if the output schema structure is invalid.\n        TypeError: If the schema in the output configuration is not a dictionary or if the schema value is not of type bool.\n\n    This method checks for the following:\n    - Presence of required keys: 'prompt' and 'output'\n    - Presence of 'schema' in the 'output' configuration\n    - The 'schema' is a non-empty dictionary with exactly one key-value pair\n    - The value in the schema is of type bool\n    \"\"\"\n    required_keys = [\"prompt\", \"output\"]\n    for key in required_keys:\n        if key not in self.config:\n            raise ValueError(\n                f\"Missing required key '{key}' in FilterOperation configuration\"\n            )\n\n    if \"schema\" not in self.config[\"output\"]:\n        raise ValueError(\"Missing 'schema' in 'output' configuration\")\n\n    if not isinstance(self.config[\"output\"][\"schema\"], dict):\n        raise TypeError(\"'schema' in 'output' configuration must be a dictionary\")\n\n    if not self.config[\"output\"][\"schema\"]:\n        raise ValueError(\"'schema' in 'output' configuration cannot be empty\")\n\n    schema = self.config[\"output\"][\"schema\"]\n    if \"_short_explanation\" in schema:\n        schema = {k: v for k, v in schema.items() if k != \"_short_explanation\"}\n    if len(schema) != 1:\n        raise ValueError(\n            \"The 'schema' in 'output' configuration must have exactly one key-value pair that maps to a boolean value\"\n        )\n\n    key, value = next(iter(schema.items()))\n    if value not in [\"bool\", \"boolean\"]:\n        raise TypeError(\n            f\"The value in the 'schema' must be of type bool, got {value}\"\n        )\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.equijoin.EquijoinOperation","title":"<code>docetl.operations.equijoin.EquijoinOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/equijoin.py</code> <pre><code>class EquijoinOperation(BaseOperation):\n    def compare_pair(\n        self,\n        comparison_prompt: str,\n        model: str,\n        item1: Dict,\n        item2: Dict,\n        timeout_seconds: int = 120,\n        max_retries_per_timeout: int = 2,\n    ) -&gt; Tuple[bool, float]:\n        \"\"\"\n        Compares two items using an LLM model to determine if they match.\n\n        Args:\n            comparison_prompt (str): The prompt template for comparison.\n            model (str): The LLM model to use for comparison.\n            item1 (Dict): The first item to compare.\n            item2 (Dict): The second item to compare.\n            timeout_seconds (int): The timeout for the LLM call in seconds.\n            max_retries_per_timeout (int): The maximum number of retries per timeout.\n\n        Returns:\n            Tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.\n        \"\"\"\n\n        prompt_template = Template(comparison_prompt)\n        prompt = prompt_template.render(left=item1, right=item2)\n        response = self.runner.api.call_llm(\n            model,\n            \"compare\",\n            [{\"role\": \"user\", \"content\": prompt}],\n            {\"is_match\": \"bool\"},\n            timeout_seconds=timeout_seconds,\n            max_retries_per_timeout=max_retries_per_timeout,\n            bypass_cache=self.config.get(\"bypass_cache\", False),\n        )\n        output = self.runner.api.parse_llm_response(\n            response.response, {\"is_match\": \"bool\"}\n        )[0]\n        return output[\"is_match\"], response.total_cost\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Checks the configuration of the EquijoinOperation for required keys and valid structure.\n\n        Raises:\n            ValueError: If required keys are missing or if the blocking_keys structure is invalid.\n            Specifically:\n            - Raises if 'comparison_prompt' is missing from the config.\n            - Raises if 'left' or 'right' are missing from the 'blocking_keys' structure (if present).\n            - Raises if 'left' or 'right' are missing from the 'limits' structure (if present).\n        \"\"\"\n        if \"comparison_prompt\" not in self.config:\n            raise ValueError(\n                \"Missing required key 'comparison_prompt' in EquijoinOperation configuration\"\n            )\n\n        if \"blocking_keys\" in self.config:\n            if (\n                \"left\" not in self.config[\"blocking_keys\"]\n                or \"right\" not in self.config[\"blocking_keys\"]\n            ):\n                raise ValueError(\n                    \"Both 'left' and 'right' must be specified in 'blocking_keys'\"\n                )\n\n        if \"limits\" in self.config:\n            if (\n                \"left\" not in self.config[\"limits\"]\n                or \"right\" not in self.config[\"limits\"]\n            ):\n                raise ValueError(\n                    \"Both 'left' and 'right' must be specified in 'limits'\"\n                )\n\n        if \"limit_comparisons\" in self.config:\n            if not isinstance(self.config[\"limit_comparisons\"], int):\n                raise ValueError(\"limit_comparisons must be an integer\")\n\n    def execute(\n        self, left_data: List[Dict], right_data: List[Dict]\n    ) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Executes the equijoin operation on the provided datasets.\n\n        Args:\n            left_data (List[Dict]): The left dataset to join.\n            right_data (List[Dict]): The right dataset to join.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the joined results and the total cost of the operation.\n\n        Usage:\n        ```python\n        from docetl.operations import EquijoinOperation\n\n        config = {\n            \"blocking_keys\": {\n                \"left\": [\"id\"],\n                \"right\": [\"user_id\"]\n            },\n            \"limits\": {\n                \"left\": 1,\n                \"right\": 1\n            },\n            \"comparison_prompt\": \"Compare {{left}} and {{right}} and determine if they match.\",\n            \"blocking_threshold\": 0.8,\n            \"blocking_conditions\": [\"left['id'] == right['user_id']\"],\n            \"limit_comparisons\": 1000\n        }\n        equijoin_op = EquijoinOperation(config)\n        left_data = [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\n        right_data = [{\"user_id\": 1, \"age\": 30}, {\"user_id\": 2, \"age\": 25}]\n        results, cost = equijoin_op.execute(left_data, right_data)\n        print(f\"Joined results: {results}\")\n        print(f\"Total cost: {cost}\")\n        ```\n\n        This method performs the following steps:\n        1. Initial blocking based on specified conditions (if any)\n        2. Embedding-based blocking (if threshold is provided)\n        3. LLM-based comparison for blocked pairs\n        4. Result aggregation and validation\n\n        The method also calculates and logs statistics such as comparisons saved by blocking and join selectivity.\n        \"\"\"\n\n        blocking_keys = self.config.get(\"blocking_keys\", {})\n        left_keys = blocking_keys.get(\n            \"left\", list(left_data[0].keys()) if left_data else []\n        )\n        right_keys = blocking_keys.get(\n            \"right\", list(right_data[0].keys()) if right_data else []\n        )\n        limits = self.config.get(\n            \"limits\", {\"left\": float(\"inf\"), \"right\": float(\"inf\")}\n        )\n        left_limit = limits[\"left\"]\n        right_limit = limits[\"right\"]\n        blocking_threshold = self.config.get(\"blocking_threshold\")\n        blocking_conditions = self.config.get(\"blocking_conditions\", [])\n        limit_comparisons = self.config.get(\"limit_comparisons\")\n        total_cost = 0\n\n        # LLM-based comparison for blocked pairs\n        def get_hashable_key(item: Dict) -&gt; str:\n            return json.dumps(item, sort_keys=True)\n\n        if len(left_data) == 0 or len(right_data) == 0:\n            return [], 0\n\n        if self.status:\n            self.status.stop()\n\n        # Initial blocking using multiprocessing\n        num_processes = min(cpu_count(), len(left_data))\n\n        self.console.log(\n            f\"Starting to run code-based blocking rules for {len(left_data)} left and {len(right_data)} right rows ({len(left_data) * len(right_data)} total pairs) with {num_processes} processes...\"\n        )\n\n        with Pool(\n            processes=num_processes,\n            initializer=init_worker,\n            initargs=(right_data, blocking_conditions),\n        ) as pool:\n            blocked_pairs_nested = pool.map(process_left_item, left_data)\n\n        # Flatten the nested list of blocked pairs\n        blocked_pairs = [pair for sublist in blocked_pairs_nested for pair in sublist]\n\n        # Check if we have exceeded the pairwise comparison limit\n        if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n            # Sample pairs randomly\n            sampled_pairs = random.sample(blocked_pairs, limit_comparisons)\n\n            # Calculate number of dropped pairs\n            dropped_pairs = len(blocked_pairs) - limit_comparisons\n\n            # Prompt the user for confirmation\n            if self.status:\n                self.status.stop()\n            if not Confirm.ask(\n                f\"[yellow]Warning: {dropped_pairs} pairs will be dropped due to the comparison limit. \"\n                f\"Proceeding with {limit_comparisons} randomly sampled pairs. \"\n                f\"Do you want to continue?[/yellow]\",\n            ):\n                raise ValueError(\"Operation cancelled by user due to pair limit.\")\n\n            if self.status:\n                self.status.start()\n\n            blocked_pairs = sampled_pairs\n\n        self.console.log(\n            f\"Number of blocked pairs after initial blocking: {len(blocked_pairs)}\"\n        )\n\n        if blocking_threshold is not None:\n            embedding_model = self.config.get(\"embedding_model\", self.default_model)\n            model_input_context_length = model_cost.get(embedding_model, {}).get(\n                \"max_input_tokens\", 8192\n            )\n\n            def get_embeddings(\n                input_data: List[Dict[str, Any]], keys: List[str], name: str\n            ) -&gt; Tuple[List[List[float]], float]:\n                texts = [\n                    \" \".join(str(item[key]) for key in keys if key in item)[\n                        : model_input_context_length * 4\n                    ]\n                    for item in input_data\n                ]\n\n                embeddings = []\n                total_cost = 0\n                batch_size = 2000\n                for i in range(0, len(texts), batch_size):\n                    batch = texts[i : i + batch_size]\n                    self.console.log(\n                        f\"On iteration {i} for creating embeddings for {name} data\"\n                    )\n                    response = self.runner.api.gen_embedding(\n                        model=embedding_model,\n                        input=batch,\n                    )\n                    embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n                    total_cost += completion_cost(response)\n                return embeddings, total_cost\n\n            left_embeddings, left_cost = get_embeddings(left_data, left_keys, \"left\")\n            right_embeddings, right_cost = get_embeddings(\n                right_data, right_keys, \"right\"\n            )\n            total_cost += left_cost + right_cost\n            self.console.log(\n                f\"Created embeddings for datasets. Total embedding creation cost: {total_cost}\"\n            )\n\n            # Compute all cosine similarities in one call\n            from sklearn.metrics.pairwise import cosine_similarity\n\n            similarities = cosine_similarity(left_embeddings, right_embeddings)\n\n            # Additional blocking based on embeddings\n            # Find indices where similarity is above threshold\n            above_threshold = np.argwhere(similarities &gt;= blocking_threshold)\n            self.console.log(\n                f\"There are {above_threshold.shape[0]} pairs above the threshold.\"\n            )\n            block_pair_set = set(\n                (get_hashable_key(left_item), get_hashable_key(right_item))\n                for left_item, right_item in blocked_pairs\n            )\n\n            # If limit_comparisons is set, take only the top pairs\n            if limit_comparisons is not None:\n                # First, get all pairs above threshold\n                above_threshold_pairs = [(int(i), int(j)) for i, j in above_threshold]\n\n                # Sort these pairs by their similarity scores\n                sorted_pairs = sorted(\n                    above_threshold_pairs,\n                    key=lambda pair: similarities[pair[0], pair[1]],\n                    reverse=True,\n                )\n\n                # Take the top 'limit_comparisons' pairs\n                top_pairs = sorted_pairs[:limit_comparisons]\n\n                # Create new blocked_pairs based on top similarities and existing blocked pairs\n                new_blocked_pairs = []\n                remaining_limit = limit_comparisons - len(blocked_pairs)\n\n                # First, include all existing blocked pairs\n                final_blocked_pairs = blocked_pairs.copy()\n\n                # Then, add new pairs from top similarities until we reach the limit\n                for i, j in top_pairs:\n                    if remaining_limit &lt;= 0:\n                        break\n                    left_item, right_item = left_data[i], right_data[j]\n                    left_key = get_hashable_key(left_item)\n                    right_key = get_hashable_key(right_item)\n                    if (left_key, right_key) not in block_pair_set:\n                        new_blocked_pairs.append((left_item, right_item))\n                        block_pair_set.add((left_key, right_key))\n                        remaining_limit -= 1\n\n                final_blocked_pairs.extend(new_blocked_pairs)\n                blocked_pairs = final_blocked_pairs\n\n                self.console.log(\n                    f\"Limited comparisons to top {limit_comparisons} pairs, including {len(blocked_pairs) - len(new_blocked_pairs)} from code-based blocking and {len(new_blocked_pairs)} based on cosine similarity. Lowest cosine similarity included: {similarities[top_pairs[-1]]:.4f}\"\n                )\n            else:\n                # Add new pairs to blocked_pairs\n                for i, j in above_threshold:\n                    left_item, right_item = left_data[i], right_data[j]\n                    left_key = get_hashable_key(left_item)\n                    right_key = get_hashable_key(right_item)\n                    if (left_key, right_key) not in block_pair_set:\n                        blocked_pairs.append((left_item, right_item))\n                        block_pair_set.add((left_key, right_key))\n\n        # If there are no blocking conditions or embedding threshold, use all pairs\n        if not blocking_conditions and blocking_threshold is None:\n            blocked_pairs = [\n                (left_item, right_item)\n                for left_item in left_data\n                for right_item in right_data\n            ]\n\n        # If there's a limit on the number of comparisons, randomly sample pairs\n        if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n            self.console.log(\n                f\"Randomly sampling {limit_comparisons} pairs out of {len(blocked_pairs)} blocked pairs.\"\n            )\n            blocked_pairs = random.sample(blocked_pairs, limit_comparisons)\n\n        self.console.log(\n            f\"Total pairs to compare after blocking and sampling: {len(blocked_pairs)}\"\n        )\n\n        # Calculate and print statistics\n        total_possible_comparisons = len(left_data) * len(right_data)\n        comparisons_made = len(blocked_pairs)\n        comparisons_saved = total_possible_comparisons - comparisons_made\n        self.console.log(\n            f\"[green]Comparisons saved by blocking: {comparisons_saved} \"\n            f\"({(comparisons_saved / total_possible_comparisons) * 100:.2f}%)[/green]\"\n        )\n\n        left_match_counts = defaultdict(int)\n        right_match_counts = defaultdict(int)\n        results = []\n        comparison_costs = 0\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            future_to_pair = {\n                executor.submit(\n                    self.compare_pair,\n                    self.config[\"comparison_prompt\"],\n                    self.config.get(\"comparison_model\", self.default_model),\n                    left,\n                    right,\n                    self.config.get(\"timeout\", 120),\n                    self.config.get(\"max_retries_per_timeout\", 2),\n                ): (left, right)\n                for left, right in blocked_pairs\n            }\n\n            for future in rich_as_completed(\n                future_to_pair,\n                total=len(future_to_pair),\n                desc=\"Comparing pairs\",\n                console=self.console,\n            ):\n                pair = future_to_pair[future]\n                is_match, cost = future.result()\n                comparison_costs += cost\n\n                if is_match:\n                    joined_item = {}\n                    left_item, right_item = pair\n                    left_key_hash = get_hashable_key(left_item)\n                    right_key_hash = get_hashable_key(right_item)\n                    if (\n                        left_match_counts[left_key_hash] &gt;= left_limit\n                        or right_match_counts[right_key_hash] &gt;= right_limit\n                    ):\n                        continue\n\n                    for key, value in left_item.items():\n                        joined_item[f\"{key}_left\" if key in right_item else key] = value\n                    for key, value in right_item.items():\n                        joined_item[f\"{key}_right\" if key in left_item else key] = value\n                    if self.runner.api.validate_output(\n                        self.config, joined_item, self.console\n                    ):\n                        results.append(joined_item)\n                        left_match_counts[left_key_hash] += 1\n                        right_match_counts[right_key_hash] += 1\n\n                    # TODO: support retry in validation failure\n\n        total_cost += comparison_costs\n\n        # Calculate and print the join selectivity\n        join_selectivity = (\n            len(results) / (len(left_data) * len(right_data))\n            if len(left_data) * len(right_data) &gt; 0\n            else 0\n        )\n        self.console.log(f\"Equijoin selectivity: {join_selectivity:.4f}\")\n\n        if self.status:\n            self.status.start()\n\n        return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.equijoin.EquijoinOperation.compare_pair","title":"<code>compare_pair(comparison_prompt, model, item1, item2, timeout_seconds=120, max_retries_per_timeout=2)</code>","text":"<p>Compares two items using an LLM model to determine if they match.</p> <p>Parameters:</p> Name Type Description Default <code>comparison_prompt</code> <code>str</code> <p>The prompt template for comparison.</p> required <code>model</code> <code>str</code> <p>The LLM model to use for comparison.</p> required <code>item1</code> <code>Dict</code> <p>The first item to compare.</p> required <code>item2</code> <code>Dict</code> <p>The second item to compare.</p> required <code>timeout_seconds</code> <code>int</code> <p>The timeout for the LLM call in seconds.</p> <code>120</code> <code>max_retries_per_timeout</code> <code>int</code> <p>The maximum number of retries per timeout.</p> <code>2</code> <p>Returns:</p> Type Description <code>Tuple[bool, float]</code> <p>Tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.</p> Source code in <code>docetl/operations/equijoin.py</code> <pre><code>def compare_pair(\n    self,\n    comparison_prompt: str,\n    model: str,\n    item1: Dict,\n    item2: Dict,\n    timeout_seconds: int = 120,\n    max_retries_per_timeout: int = 2,\n) -&gt; Tuple[bool, float]:\n    \"\"\"\n    Compares two items using an LLM model to determine if they match.\n\n    Args:\n        comparison_prompt (str): The prompt template for comparison.\n        model (str): The LLM model to use for comparison.\n        item1 (Dict): The first item to compare.\n        item2 (Dict): The second item to compare.\n        timeout_seconds (int): The timeout for the LLM call in seconds.\n        max_retries_per_timeout (int): The maximum number of retries per timeout.\n\n    Returns:\n        Tuple[bool, float]: A tuple containing a boolean indicating whether the items match and the cost of the comparison.\n    \"\"\"\n\n    prompt_template = Template(comparison_prompt)\n    prompt = prompt_template.render(left=item1, right=item2)\n    response = self.runner.api.call_llm(\n        model,\n        \"compare\",\n        [{\"role\": \"user\", \"content\": prompt}],\n        {\"is_match\": \"bool\"},\n        timeout_seconds=timeout_seconds,\n        max_retries_per_timeout=max_retries_per_timeout,\n        bypass_cache=self.config.get(\"bypass_cache\", False),\n    )\n    output = self.runner.api.parse_llm_response(\n        response.response, {\"is_match\": \"bool\"}\n    )[0]\n    return output[\"is_match\"], response.total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.equijoin.EquijoinOperation.execute","title":"<code>execute(left_data, right_data)</code>","text":"<p>Executes the equijoin operation on the provided datasets.</p> <p>Parameters:</p> Name Type Description Default <code>left_data</code> <code>List[Dict]</code> <p>The left dataset to join.</p> required <code>right_data</code> <code>List[Dict]</code> <p>The right dataset to join.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Dict], float]</code> <p>Tuple[List[Dict], float]: A tuple containing the joined results and the total cost of the operation.</p> <p>Usage: <pre><code>from docetl.operations import EquijoinOperation\n\nconfig = {\n    \"blocking_keys\": {\n        \"left\": [\"id\"],\n        \"right\": [\"user_id\"]\n    },\n    \"limits\": {\n        \"left\": 1,\n        \"right\": 1\n    },\n    \"comparison_prompt\": \"Compare {{left}} and {{right}} and determine if they match.\",\n    \"blocking_threshold\": 0.8,\n    \"blocking_conditions\": [\"left['id'] == right['user_id']\"],\n    \"limit_comparisons\": 1000\n}\nequijoin_op = EquijoinOperation(config)\nleft_data = [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\nright_data = [{\"user_id\": 1, \"age\": 30}, {\"user_id\": 2, \"age\": 25}]\nresults, cost = equijoin_op.execute(left_data, right_data)\nprint(f\"Joined results: {results}\")\nprint(f\"Total cost: {cost}\")\n</code></pre></p> <p>This method performs the following steps: 1. Initial blocking based on specified conditions (if any) 2. Embedding-based blocking (if threshold is provided) 3. LLM-based comparison for blocked pairs 4. Result aggregation and validation</p> <p>The method also calculates and logs statistics such as comparisons saved by blocking and join selectivity.</p> Source code in <code>docetl/operations/equijoin.py</code> <pre><code>def execute(\n    self, left_data: List[Dict], right_data: List[Dict]\n) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Executes the equijoin operation on the provided datasets.\n\n    Args:\n        left_data (List[Dict]): The left dataset to join.\n        right_data (List[Dict]): The right dataset to join.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the joined results and the total cost of the operation.\n\n    Usage:\n    ```python\n    from docetl.operations import EquijoinOperation\n\n    config = {\n        \"blocking_keys\": {\n            \"left\": [\"id\"],\n            \"right\": [\"user_id\"]\n        },\n        \"limits\": {\n            \"left\": 1,\n            \"right\": 1\n        },\n        \"comparison_prompt\": \"Compare {{left}} and {{right}} and determine if they match.\",\n        \"blocking_threshold\": 0.8,\n        \"blocking_conditions\": [\"left['id'] == right['user_id']\"],\n        \"limit_comparisons\": 1000\n    }\n    equijoin_op = EquijoinOperation(config)\n    left_data = [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\n    right_data = [{\"user_id\": 1, \"age\": 30}, {\"user_id\": 2, \"age\": 25}]\n    results, cost = equijoin_op.execute(left_data, right_data)\n    print(f\"Joined results: {results}\")\n    print(f\"Total cost: {cost}\")\n    ```\n\n    This method performs the following steps:\n    1. Initial blocking based on specified conditions (if any)\n    2. Embedding-based blocking (if threshold is provided)\n    3. LLM-based comparison for blocked pairs\n    4. Result aggregation and validation\n\n    The method also calculates and logs statistics such as comparisons saved by blocking and join selectivity.\n    \"\"\"\n\n    blocking_keys = self.config.get(\"blocking_keys\", {})\n    left_keys = blocking_keys.get(\n        \"left\", list(left_data[0].keys()) if left_data else []\n    )\n    right_keys = blocking_keys.get(\n        \"right\", list(right_data[0].keys()) if right_data else []\n    )\n    limits = self.config.get(\n        \"limits\", {\"left\": float(\"inf\"), \"right\": float(\"inf\")}\n    )\n    left_limit = limits[\"left\"]\n    right_limit = limits[\"right\"]\n    blocking_threshold = self.config.get(\"blocking_threshold\")\n    blocking_conditions = self.config.get(\"blocking_conditions\", [])\n    limit_comparisons = self.config.get(\"limit_comparisons\")\n    total_cost = 0\n\n    # LLM-based comparison for blocked pairs\n    def get_hashable_key(item: Dict) -&gt; str:\n        return json.dumps(item, sort_keys=True)\n\n    if len(left_data) == 0 or len(right_data) == 0:\n        return [], 0\n\n    if self.status:\n        self.status.stop()\n\n    # Initial blocking using multiprocessing\n    num_processes = min(cpu_count(), len(left_data))\n\n    self.console.log(\n        f\"Starting to run code-based blocking rules for {len(left_data)} left and {len(right_data)} right rows ({len(left_data) * len(right_data)} total pairs) with {num_processes} processes...\"\n    )\n\n    with Pool(\n        processes=num_processes,\n        initializer=init_worker,\n        initargs=(right_data, blocking_conditions),\n    ) as pool:\n        blocked_pairs_nested = pool.map(process_left_item, left_data)\n\n    # Flatten the nested list of blocked pairs\n    blocked_pairs = [pair for sublist in blocked_pairs_nested for pair in sublist]\n\n    # Check if we have exceeded the pairwise comparison limit\n    if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n        # Sample pairs randomly\n        sampled_pairs = random.sample(blocked_pairs, limit_comparisons)\n\n        # Calculate number of dropped pairs\n        dropped_pairs = len(blocked_pairs) - limit_comparisons\n\n        # Prompt the user for confirmation\n        if self.status:\n            self.status.stop()\n        if not Confirm.ask(\n            f\"[yellow]Warning: {dropped_pairs} pairs will be dropped due to the comparison limit. \"\n            f\"Proceeding with {limit_comparisons} randomly sampled pairs. \"\n            f\"Do you want to continue?[/yellow]\",\n        ):\n            raise ValueError(\"Operation cancelled by user due to pair limit.\")\n\n        if self.status:\n            self.status.start()\n\n        blocked_pairs = sampled_pairs\n\n    self.console.log(\n        f\"Number of blocked pairs after initial blocking: {len(blocked_pairs)}\"\n    )\n\n    if blocking_threshold is not None:\n        embedding_model = self.config.get(\"embedding_model\", self.default_model)\n        model_input_context_length = model_cost.get(embedding_model, {}).get(\n            \"max_input_tokens\", 8192\n        )\n\n        def get_embeddings(\n            input_data: List[Dict[str, Any]], keys: List[str], name: str\n        ) -&gt; Tuple[List[List[float]], float]:\n            texts = [\n                \" \".join(str(item[key]) for key in keys if key in item)[\n                    : model_input_context_length * 4\n                ]\n                for item in input_data\n            ]\n\n            embeddings = []\n            total_cost = 0\n            batch_size = 2000\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i : i + batch_size]\n                self.console.log(\n                    f\"On iteration {i} for creating embeddings for {name} data\"\n                )\n                response = self.runner.api.gen_embedding(\n                    model=embedding_model,\n                    input=batch,\n                )\n                embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n                total_cost += completion_cost(response)\n            return embeddings, total_cost\n\n        left_embeddings, left_cost = get_embeddings(left_data, left_keys, \"left\")\n        right_embeddings, right_cost = get_embeddings(\n            right_data, right_keys, \"right\"\n        )\n        total_cost += left_cost + right_cost\n        self.console.log(\n            f\"Created embeddings for datasets. Total embedding creation cost: {total_cost}\"\n        )\n\n        # Compute all cosine similarities in one call\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        similarities = cosine_similarity(left_embeddings, right_embeddings)\n\n        # Additional blocking based on embeddings\n        # Find indices where similarity is above threshold\n        above_threshold = np.argwhere(similarities &gt;= blocking_threshold)\n        self.console.log(\n            f\"There are {above_threshold.shape[0]} pairs above the threshold.\"\n        )\n        block_pair_set = set(\n            (get_hashable_key(left_item), get_hashable_key(right_item))\n            for left_item, right_item in blocked_pairs\n        )\n\n        # If limit_comparisons is set, take only the top pairs\n        if limit_comparisons is not None:\n            # First, get all pairs above threshold\n            above_threshold_pairs = [(int(i), int(j)) for i, j in above_threshold]\n\n            # Sort these pairs by their similarity scores\n            sorted_pairs = sorted(\n                above_threshold_pairs,\n                key=lambda pair: similarities[pair[0], pair[1]],\n                reverse=True,\n            )\n\n            # Take the top 'limit_comparisons' pairs\n            top_pairs = sorted_pairs[:limit_comparisons]\n\n            # Create new blocked_pairs based on top similarities and existing blocked pairs\n            new_blocked_pairs = []\n            remaining_limit = limit_comparisons - len(blocked_pairs)\n\n            # First, include all existing blocked pairs\n            final_blocked_pairs = blocked_pairs.copy()\n\n            # Then, add new pairs from top similarities until we reach the limit\n            for i, j in top_pairs:\n                if remaining_limit &lt;= 0:\n                    break\n                left_item, right_item = left_data[i], right_data[j]\n                left_key = get_hashable_key(left_item)\n                right_key = get_hashable_key(right_item)\n                if (left_key, right_key) not in block_pair_set:\n                    new_blocked_pairs.append((left_item, right_item))\n                    block_pair_set.add((left_key, right_key))\n                    remaining_limit -= 1\n\n            final_blocked_pairs.extend(new_blocked_pairs)\n            blocked_pairs = final_blocked_pairs\n\n            self.console.log(\n                f\"Limited comparisons to top {limit_comparisons} pairs, including {len(blocked_pairs) - len(new_blocked_pairs)} from code-based blocking and {len(new_blocked_pairs)} based on cosine similarity. Lowest cosine similarity included: {similarities[top_pairs[-1]]:.4f}\"\n            )\n        else:\n            # Add new pairs to blocked_pairs\n            for i, j in above_threshold:\n                left_item, right_item = left_data[i], right_data[j]\n                left_key = get_hashable_key(left_item)\n                right_key = get_hashable_key(right_item)\n                if (left_key, right_key) not in block_pair_set:\n                    blocked_pairs.append((left_item, right_item))\n                    block_pair_set.add((left_key, right_key))\n\n    # If there are no blocking conditions or embedding threshold, use all pairs\n    if not blocking_conditions and blocking_threshold is None:\n        blocked_pairs = [\n            (left_item, right_item)\n            for left_item in left_data\n            for right_item in right_data\n        ]\n\n    # If there's a limit on the number of comparisons, randomly sample pairs\n    if limit_comparisons is not None and len(blocked_pairs) &gt; limit_comparisons:\n        self.console.log(\n            f\"Randomly sampling {limit_comparisons} pairs out of {len(blocked_pairs)} blocked pairs.\"\n        )\n        blocked_pairs = random.sample(blocked_pairs, limit_comparisons)\n\n    self.console.log(\n        f\"Total pairs to compare after blocking and sampling: {len(blocked_pairs)}\"\n    )\n\n    # Calculate and print statistics\n    total_possible_comparisons = len(left_data) * len(right_data)\n    comparisons_made = len(blocked_pairs)\n    comparisons_saved = total_possible_comparisons - comparisons_made\n    self.console.log(\n        f\"[green]Comparisons saved by blocking: {comparisons_saved} \"\n        f\"({(comparisons_saved / total_possible_comparisons) * 100:.2f}%)[/green]\"\n    )\n\n    left_match_counts = defaultdict(int)\n    right_match_counts = defaultdict(int)\n    results = []\n    comparison_costs = 0\n\n    with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n        future_to_pair = {\n            executor.submit(\n                self.compare_pair,\n                self.config[\"comparison_prompt\"],\n                self.config.get(\"comparison_model\", self.default_model),\n                left,\n                right,\n                self.config.get(\"timeout\", 120),\n                self.config.get(\"max_retries_per_timeout\", 2),\n            ): (left, right)\n            for left, right in blocked_pairs\n        }\n\n        for future in rich_as_completed(\n            future_to_pair,\n            total=len(future_to_pair),\n            desc=\"Comparing pairs\",\n            console=self.console,\n        ):\n            pair = future_to_pair[future]\n            is_match, cost = future.result()\n            comparison_costs += cost\n\n            if is_match:\n                joined_item = {}\n                left_item, right_item = pair\n                left_key_hash = get_hashable_key(left_item)\n                right_key_hash = get_hashable_key(right_item)\n                if (\n                    left_match_counts[left_key_hash] &gt;= left_limit\n                    or right_match_counts[right_key_hash] &gt;= right_limit\n                ):\n                    continue\n\n                for key, value in left_item.items():\n                    joined_item[f\"{key}_left\" if key in right_item else key] = value\n                for key, value in right_item.items():\n                    joined_item[f\"{key}_right\" if key in left_item else key] = value\n                if self.runner.api.validate_output(\n                    self.config, joined_item, self.console\n                ):\n                    results.append(joined_item)\n                    left_match_counts[left_key_hash] += 1\n                    right_match_counts[right_key_hash] += 1\n\n                # TODO: support retry in validation failure\n\n    total_cost += comparison_costs\n\n    # Calculate and print the join selectivity\n    join_selectivity = (\n        len(results) / (len(left_data) * len(right_data))\n        if len(left_data) * len(right_data) &gt; 0\n        else 0\n    )\n    self.console.log(f\"Equijoin selectivity: {join_selectivity:.4f}\")\n\n    if self.status:\n        self.status.start()\n\n    return results, total_cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.equijoin.EquijoinOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Checks the configuration of the EquijoinOperation for required keys and valid structure.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing or if the blocking_keys structure is invalid.</p> <code>Specifically</code> Source code in <code>docetl/operations/equijoin.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Checks the configuration of the EquijoinOperation for required keys and valid structure.\n\n    Raises:\n        ValueError: If required keys are missing or if the blocking_keys structure is invalid.\n        Specifically:\n        - Raises if 'comparison_prompt' is missing from the config.\n        - Raises if 'left' or 'right' are missing from the 'blocking_keys' structure (if present).\n        - Raises if 'left' or 'right' are missing from the 'limits' structure (if present).\n    \"\"\"\n    if \"comparison_prompt\" not in self.config:\n        raise ValueError(\n            \"Missing required key 'comparison_prompt' in EquijoinOperation configuration\"\n        )\n\n    if \"blocking_keys\" in self.config:\n        if (\n            \"left\" not in self.config[\"blocking_keys\"]\n            or \"right\" not in self.config[\"blocking_keys\"]\n        ):\n            raise ValueError(\n                \"Both 'left' and 'right' must be specified in 'blocking_keys'\"\n            )\n\n    if \"limits\" in self.config:\n        if (\n            \"left\" not in self.config[\"limits\"]\n            or \"right\" not in self.config[\"limits\"]\n        ):\n            raise ValueError(\n                \"Both 'left' and 'right' must be specified in 'limits'\"\n            )\n\n    if \"limit_comparisons\" in self.config:\n        if not isinstance(self.config[\"limit_comparisons\"], int):\n            raise ValueError(\"limit_comparisons must be an integer\")\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.cluster.ClusterOperation","title":"<code>docetl.operations.cluster.ClusterOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> Source code in <code>docetl/operations/cluster.py</code> <pre><code>class ClusterOperation(BaseOperation):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.max_batch_size: int = self.config.get(\n            \"max_batch_size\", kwargs.get(\"max_batch_size\", float(\"inf\"))\n        )\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Checks the configuration of the ClusterOperation for required keys and valid structure.\n\n        Raises:\n            ValueError: If required keys are missing or invalid in the configuration.\n            TypeError: If configuration values have incorrect types.\n        \"\"\"\n        required_keys = [\"embedding_keys\", \"summary_schema\", \"summary_prompt\"]\n        for key in required_keys:\n            if key not in self.config:\n                raise ValueError(\n                    f\"Missing required key '{key}' in ClusterOperation configuration\"\n                )\n\n        if not isinstance(self.config[\"embedding_keys\"], list):\n            raise TypeError(\"'embedding_keys' must be a list of strings\")\n\n        if \"output_key\" in self.config:\n            if not isinstance(self.config[\"output_key\"], str):\n                raise TypeError(\"'output_key' must be a string\")\n\n        if not isinstance(self.config[\"summary_schema\"], dict):\n            raise TypeError(\"'summary_schema' must be a dictionary\")\n\n        if not isinstance(self.config[\"summary_prompt\"], str):\n            raise TypeError(\"'prompt' must be a string\")\n\n        # Check if the prompt is a valid Jinja2 template\n        try:\n            Template(self.config[\"summary_prompt\"])\n        except Exception as e:\n            raise ValueError(f\"Invalid Jinja2 template in 'prompt': {str(e)}\")\n\n        # Check optional parameters\n        if \"max_batch_size\" in self.config:\n            if not isinstance(self.config[\"max_batch_size\"], int):\n                raise TypeError(\"'max_batch_size' must be an integer\")\n\n        if \"embedding_model\" in self.config:\n            if not isinstance(self.config[\"embedding_model\"], str):\n                raise TypeError(\"'embedding_model' must be a string\")\n\n        if \"model\" in self.config:\n            if not isinstance(self.config[\"model\"], str):\n                raise TypeError(\"'model' must be a string\")\n\n        if \"validate\" in self.config:\n            if not isinstance(self.config[\"validate\"], list):\n                raise TypeError(\"'validate' must be a list of strings\")\n            for rule in self.config[\"validate\"]:\n                if not isinstance(rule, str):\n                    raise TypeError(\"Each validation rule must be a string\")\n\n    def execute(\n        self, input_data: List[Dict], is_build: bool = False\n    ) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Executes the cluster operation on the input data. Modifies the\n        input data and returns it in place.\n\n        Args:\n            input_data (List[Dict]): A list of dictionaries to process.\n            is_build (bool): Whether the operation is being executed\n              in the build phase. Defaults to False.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the clustered\n              list of dictionaries and the total cost of the operation.\n        \"\"\"\n        if not input_data:\n            return input_data, 0\n\n        if len(input_data) == 1:\n            input_data[0][self.config.get(\"output_key\", \"clusters\")] = ()\n            return input_data, 0\n\n        embeddings, cost = get_embeddings_for_clustering(\n            input_data, self.config, self.runner.api\n        )\n\n        tree = self.agglomerative_cluster_of_embeddings(input_data, embeddings)\n\n        if \"collapse\" in self.config:\n            tree = self.collapse_tree(tree, collapse = self.config[\"collapse\"])\n\n        self.prompt_template = Template(self.config[\"summary_prompt\"])\n        cost += self.annotate_clustering_tree(tree)\n        self.annotate_leaves(tree)\n\n        return input_data, cost\n\n    def agglomerative_cluster_of_embeddings(self, input_data, embeddings):\n        import sklearn.cluster\n\n        cl = sklearn.cluster.AgglomerativeClustering(\n            compute_full_tree=True, compute_distances=True\n        )\n        cl.fit(embeddings)\n\n        nsamples = len(embeddings)\n\n        def build_tree(i):\n            if i &lt; nsamples:\n                res = input_data[i]\n                #                res[\"embedding\"] = list(embeddings[i])\n                return res\n            return {\n                 \"children\": [\n                    build_tree(cl.children_[i - nsamples, 0]),\n                    build_tree(cl.children_[i - nsamples, 1]),\n                ],\n                \"distance\": cl.distances_[i - nsamples],\n            }\n\n        return build_tree(nsamples + len(cl.children_) - 1)\n\n    def get_tree_distances(self, t):\n        res = set()\n        if \"distance\" in t:\n            res.update(set([t[\"distance\"] - child[\"distance\"] for child in t[\"children\"] if \"distance\" in child]))\n        if \"children\" in t:\n            for child in t[\"children\"]:\n                res.update(self.get_tree_distances(child))\n        return res\n\n    def _collapse_tree(self, t, parent_dist = None, collapse = None):\n        if \"children\" in t:\n            if (    \"distance\" in t\n                and parent_dist is not None\n                and collapse is not None\n                and parent_dist - t[\"distance\"] &lt; collapse):\n                return [grandchild\n                        for child in t[\"children\"]\n                        for grandchild in self._collapse_tree(child, parent_dist=parent_dist, collapse=collapse)]\n            else:\n                res = dict(t)\n                res[\"children\"] = [grandchild\n                                   for idx, child in enumerate(t[\"children\"])\n                                   for grandchild in self._collapse_tree(child, parent_dist=t[\"distance\"], collapse=collapse)]\n                return [res]\n        else:\n            return [t]\n\n    def collapse_tree(self, tree, collapse = None):\n        if collapse is not None:\n            tree_distances = np.array(sorted(self.get_tree_distances(tree)))\n            collapse = tree_distances[int(len(tree_distances) * collapse)]\n        return self._collapse_tree(tree, collapse=collapse)[0]\n\n\n    def annotate_clustering_tree(self, t):\n        if \"children\" in t:\n            with ThreadPoolExecutor(max_workers=self.max_batch_size) as executor:\n                futures = [\n                    executor.submit(self.annotate_clustering_tree, child)\n                    for child in t[\"children\"]\n                ]\n\n                total_cost = 0\n                pbar = RichLoopBar(\n                    range(len(futures)),\n                    desc=f\"Processing {self.config['name']} (map) on all documents\",\n                    console=self.console,\n                )\n                for i in pbar:\n                    total_cost += futures[i].result()\n                    pbar.update(i)\n\n            prompt = self.prompt_template.render(\n                inputs=t[\"children\"]\n            )\n\n            def validation_fn(response: Dict[str, Any]):\n                output = self.runner.api.parse_llm_response(\n                    response,\n                    schema=self.config[\"summary_schema\"],\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0]\n                if self.runner.api.validate_output(self.config, output, self.console):\n                    return output, True\n                return output, False\n\n            response = self.runner.api.call_llm(\n                model=self.config.get(\"model\", self.default_model),\n                op_type=\"cluster\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                output_schema=self.config[\"summary_schema\"],\n                timeout_seconds=self.config.get(\"timeout\", 120),\n                bypass_cache=self.config.get(\"bypass_cache\", False),\n                max_retries_per_timeout=self.config.get(\"max_retries_per_timeout\", 2),\n                validation_config=(\n                    {\n                        \"num_retries\": self.num_retries_on_validate_failure,\n                        \"val_rule\": self.config.get(\"validate\", []),\n                        \"validation_fn\": validation_fn,\n                    }\n                    if self.config.get(\"validate\", None)\n                    else None\n                ),\n                verbose=self.config.get(\"verbose\", False),\n            )\n            total_cost += response.total_cost\n            if response.validated:\n                output = self.runner.api.parse_llm_response(\n                    response.response,\n                    schema=self.config[\"summary_schema\"],\n                    manually_fix_errors=self.manually_fix_errors,\n                )[0]\n                t.update(output)\n\n            return total_cost\n        return 0\n\n    def annotate_leaves(self, tree, path=()):\n        if \"children\" in tree:\n            item = dict(tree)\n            item.pop(\"children\")\n            for child in tree[\"children\"]:\n                self.annotate_leaves(child, path=(item,) + path)\n        else:\n            tree[self.config.get(\"output_key\", \"clusters\")] = path\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.cluster.ClusterOperation.execute","title":"<code>execute(input_data, is_build=False)</code>","text":"<p>Executes the cluster operation on the input data. Modifies the input data and returns it in place.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Dict]</code> <p>A list of dictionaries to process.</p> required <code>is_build</code> <code>bool</code> <p>Whether the operation is being executed in the build phase. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[Dict], float]</code> <p>Tuple[List[Dict], float]: A tuple containing the clustered list of dictionaries and the total cost of the operation.</p> Source code in <code>docetl/operations/cluster.py</code> <pre><code>def execute(\n    self, input_data: List[Dict], is_build: bool = False\n) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Executes the cluster operation on the input data. Modifies the\n    input data and returns it in place.\n\n    Args:\n        input_data (List[Dict]): A list of dictionaries to process.\n        is_build (bool): Whether the operation is being executed\n          in the build phase. Defaults to False.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the clustered\n          list of dictionaries and the total cost of the operation.\n    \"\"\"\n    if not input_data:\n        return input_data, 0\n\n    if len(input_data) == 1:\n        input_data[0][self.config.get(\"output_key\", \"clusters\")] = ()\n        return input_data, 0\n\n    embeddings, cost = get_embeddings_for_clustering(\n        input_data, self.config, self.runner.api\n    )\n\n    tree = self.agglomerative_cluster_of_embeddings(input_data, embeddings)\n\n    if \"collapse\" in self.config:\n        tree = self.collapse_tree(tree, collapse = self.config[\"collapse\"])\n\n    self.prompt_template = Template(self.config[\"summary_prompt\"])\n    cost += self.annotate_clustering_tree(tree)\n    self.annotate_leaves(tree)\n\n    return input_data, cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.cluster.ClusterOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Checks the configuration of the ClusterOperation for required keys and valid structure.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing or invalid in the configuration.</p> <code>TypeError</code> <p>If configuration values have incorrect types.</p> Source code in <code>docetl/operations/cluster.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Checks the configuration of the ClusterOperation for required keys and valid structure.\n\n    Raises:\n        ValueError: If required keys are missing or invalid in the configuration.\n        TypeError: If configuration values have incorrect types.\n    \"\"\"\n    required_keys = [\"embedding_keys\", \"summary_schema\", \"summary_prompt\"]\n    for key in required_keys:\n        if key not in self.config:\n            raise ValueError(\n                f\"Missing required key '{key}' in ClusterOperation configuration\"\n            )\n\n    if not isinstance(self.config[\"embedding_keys\"], list):\n        raise TypeError(\"'embedding_keys' must be a list of strings\")\n\n    if \"output_key\" in self.config:\n        if not isinstance(self.config[\"output_key\"], str):\n            raise TypeError(\"'output_key' must be a string\")\n\n    if not isinstance(self.config[\"summary_schema\"], dict):\n        raise TypeError(\"'summary_schema' must be a dictionary\")\n\n    if not isinstance(self.config[\"summary_prompt\"], str):\n        raise TypeError(\"'prompt' must be a string\")\n\n    # Check if the prompt is a valid Jinja2 template\n    try:\n        Template(self.config[\"summary_prompt\"])\n    except Exception as e:\n        raise ValueError(f\"Invalid Jinja2 template in 'prompt': {str(e)}\")\n\n    # Check optional parameters\n    if \"max_batch_size\" in self.config:\n        if not isinstance(self.config[\"max_batch_size\"], int):\n            raise TypeError(\"'max_batch_size' must be an integer\")\n\n    if \"embedding_model\" in self.config:\n        if not isinstance(self.config[\"embedding_model\"], str):\n            raise TypeError(\"'embedding_model' must be a string\")\n\n    if \"model\" in self.config:\n        if not isinstance(self.config[\"model\"], str):\n            raise TypeError(\"'model' must be a string\")\n\n    if \"validate\" in self.config:\n        if not isinstance(self.config[\"validate\"], list):\n            raise TypeError(\"'validate' must be a list of strings\")\n        for rule in self.config[\"validate\"]:\n            if not isinstance(rule, str):\n                raise TypeError(\"Each validation rule must be a string\")\n</code></pre>"},{"location":"api-reference/operations/#auxiliary-operators","title":"Auxiliary Operators","text":""},{"location":"api-reference/operations/#docetl.operations.split.SplitOperation","title":"<code>docetl.operations.split.SplitOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>A class that implements a split operation on input data, dividing it into manageable chunks.</p> <p>This class extends BaseOperation to: 1. Split input data into chunks of specified size based on the 'split_key' and 'token_count' configuration. 2. Assign unique identifiers to each original document and number chunks sequentially. 3. Return results containing:    - {split_key}_chunk: The content of the split chunk.    - {name}_id: A unique identifier for each original document.    - {name}_chunk_num: The sequential number of the chunk within its original document.</p> Source code in <code>docetl/operations/split.py</code> <pre><code>class SplitOperation(BaseOperation):\n    \"\"\"\n    A class that implements a split operation on input data, dividing it into manageable chunks.\n\n    This class extends BaseOperation to:\n    1. Split input data into chunks of specified size based on the 'split_key' and 'token_count' configuration.\n    2. Assign unique identifiers to each original document and number chunks sequentially.\n    3. Return results containing:\n       - {split_key}_chunk: The content of the split chunk.\n       - {name}_id: A unique identifier for each original document.\n       - {name}_chunk_num: The sequential number of the chunk within its original document.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = self.config[\"name\"]\n\n    def syntax_check(self) -&gt; None:\n        required_keys = [\"split_key\", \"method\", \"method_kwargs\"]\n        for key in required_keys:\n            if key not in self.config:\n                raise ValueError(\n                    f\"Missing required key '{key}' in SplitOperation configuration\"\n                )\n\n        if not isinstance(self.config[\"split_key\"], str):\n            raise TypeError(\"'split_key' must be a string\")\n\n        if self.config[\"method\"] not in [\"token_count\", \"delimiter\"]:\n            raise ValueError(f\"Invalid method '{self.config['method']}'\")\n\n        if self.config[\"method\"] == \"token_count\":\n            if (\n                not isinstance(self.config[\"method_kwargs\"][\"num_tokens\"], int)\n                or self.config[\"method_kwargs\"][\"num_tokens\"] &lt;= 0\n            ):\n                raise ValueError(\"'num_tokens' must be a positive integer\")\n        elif self.config[\"method\"] == \"delimiter\":\n            if not isinstance(self.config[\"method_kwargs\"][\"delimiter\"], str):\n                raise ValueError(\"'delimiter' must be a string\")\n\n    def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n        split_key = self.config[\"split_key\"]\n        method = self.config[\"method\"]\n        method_kwargs = self.config[\"method_kwargs\"]\n        try:\n            encoder = tiktoken.encoding_for_model(\n                self.config[\"method_kwargs\"]\n                .get(\"model\", self.default_model)\n                .split(\"/\")[-1]\n            )\n        except Exception:\n            encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n\n        results = []\n        cost = 0.0\n\n        for item in input_data:\n            if split_key not in item:\n                raise KeyError(f\"Split key '{split_key}' not found in item\")\n\n            content = item[split_key]\n            doc_id = str(uuid.uuid4())\n\n            if method == \"token_count\":\n                token_count = method_kwargs[\"num_tokens\"]\n                tokens = encoder.encode(content)\n\n                for chunk_num, i in enumerate(\n                    range(0, len(tokens), token_count), start=1\n                ):\n                    chunk_tokens = tokens[i : i + token_count]\n                    chunk = encoder.decode(chunk_tokens)\n\n                    result = item.copy()\n                    result.update(\n                        {\n                            f\"{split_key}_chunk\": chunk,\n                            f\"{self.name}_id\": doc_id,\n                            f\"{self.name}_chunk_num\": chunk_num,\n                        }\n                    )\n                    results.append(result)\n\n            elif method == \"delimiter\":\n                delimiter = method_kwargs[\"delimiter\"]\n                num_splits_to_group = method_kwargs.get(\"num_splits_to_group\", 1)\n                chunks = content.split(delimiter)\n\n                # Get rid of empty chunks\n                chunks = [chunk for chunk in chunks if chunk.strip()]\n\n                for chunk_num, i in enumerate(\n                    range(0, len(chunks), num_splits_to_group), start=1\n                ):\n                    grouped_chunks = chunks[i : i + num_splits_to_group]\n                    joined_chunk = delimiter.join(grouped_chunks).strip()\n\n                    result = item.copy()\n                    result.update(\n                        {\n                            f\"{split_key}_chunk\": joined_chunk,\n                            f\"{self.name}_id\": doc_id,\n                            f\"{self.name}_chunk_num\": chunk_num,\n                        }\n                    )\n                    results.append(result)\n\n        return results, cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation","title":"<code>docetl.operations.gather.GatherOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>A class that implements a gather operation on input data, adding contextual information from surrounding chunks.</p> <p>This class extends BaseOperation to: 1. Group chunks by their document ID. 2. Order chunks within each group. 3. Add peripheral context to each chunk based on the configuration. 4. Include headers for each chunk and its upward hierarchy. 5. Return results containing the rendered chunks with added context, including information about skipped characters and headers.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>class GatherOperation(BaseOperation):\n    \"\"\"\n    A class that implements a gather operation on input data, adding contextual information from surrounding chunks.\n\n    This class extends BaseOperation to:\n    1. Group chunks by their document ID.\n    2. Order chunks within each group.\n    3. Add peripheral context to each chunk based on the configuration.\n    4. Include headers for each chunk and its upward hierarchy.\n    5. Return results containing the rendered chunks with added context, including information about skipped characters and headers.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize the GatherOperation.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Perform a syntax check on the operation configuration.\n\n        Raises:\n            ValueError: If required keys are missing or if there are configuration errors.\n            TypeError: If main_chunk_start or main_chunk_end are not strings.\n        \"\"\"\n        required_keys = [\"content_key\", \"doc_id_key\", \"order_key\"]\n        for key in required_keys:\n            if key not in self.config:\n                raise ValueError(\n                    f\"Missing required key '{key}' in GatherOperation configuration\"\n                )\n\n        if \"peripheral_chunks\" not in self.config:\n            raise ValueError(\n                \"Missing 'peripheral_chunks' configuration in GatherOperation\"\n            )\n\n        peripheral_config = self.config[\"peripheral_chunks\"]\n        for direction in [\"previous\", \"next\"]:\n            if direction not in peripheral_config:\n                continue\n            for section in [\"head\", \"middle\", \"tail\"]:\n                if section in peripheral_config[direction]:\n                    section_config = peripheral_config[direction][section]\n                    if section != \"middle\" and \"count\" not in section_config:\n                        raise ValueError(\n                            f\"Missing 'count' in {direction}.{section} configuration\"\n                        )\n\n        if \"main_chunk_start\" in self.config and not isinstance(\n            self.config[\"main_chunk_start\"], str\n        ):\n            raise TypeError(\"'main_chunk_start' must be a string\")\n        if \"main_chunk_end\" in self.config and not isinstance(\n            self.config[\"main_chunk_end\"], str\n        ):\n            raise TypeError(\"'main_chunk_end' must be a string\")\n\n    def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Execute the gather operation on the input data.\n\n        Args:\n            input_data (List[Dict]): The input data to process.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the processed results and the cost of the operation.\n        \"\"\"\n        content_key = self.config[\"content_key\"]\n        doc_id_key = self.config[\"doc_id_key\"]\n        order_key = self.config[\"order_key\"]\n        peripheral_config = self.config[\"peripheral_chunks\"]\n        main_chunk_start = self.config.get(\n            \"main_chunk_start\", \"--- Begin Main Chunk ---\"\n        )\n        main_chunk_end = self.config.get(\"main_chunk_end\", \"--- End Main Chunk ---\")\n        doc_header_key = self.config.get(\"doc_header_key\", None)\n        results = []\n        cost = 0.0\n\n        # Group chunks by document ID\n        grouped_chunks = {}\n        for item in input_data:\n            doc_id = item[doc_id_key]\n            if doc_id not in grouped_chunks:\n                grouped_chunks[doc_id] = []\n            grouped_chunks[doc_id].append(item)\n\n        # Process each group of chunks\n        for chunks in grouped_chunks.values():\n            # Sort chunks by their order within the document\n            chunks.sort(key=lambda x: x[order_key])\n\n            # Process each chunk with its peripheral context and headers\n            for i, chunk in enumerate(chunks):\n                rendered_chunk = self.render_chunk_with_context(\n                    chunks,\n                    i,\n                    peripheral_config,\n                    content_key,\n                    order_key,\n                    main_chunk_start,\n                    main_chunk_end,\n                    doc_header_key,\n                )\n\n                result = chunk.copy()\n                result[f\"{content_key}_rendered\"] = rendered_chunk\n                results.append(result)\n\n        return results, cost\n\n    def render_chunk_with_context(\n        self,\n        chunks: List[Dict],\n        current_index: int,\n        peripheral_config: Dict,\n        content_key: str,\n        order_key: str,\n        main_chunk_start: str,\n        main_chunk_end: str,\n        doc_header_key: str,\n    ) -&gt; str:\n        \"\"\"\n        Render a chunk with its peripheral context and headers.\n\n        Args:\n            chunks (List[Dict]): List of all chunks in the document.\n            current_index (int): Index of the current chunk being processed.\n            peripheral_config (Dict): Configuration for peripheral chunks.\n            content_key (str): Key for the content in each chunk.\n            order_key (str): Key for the order of each chunk.\n            main_chunk_start (str): String to mark the start of the main chunk.\n            main_chunk_end (str): String to mark the end of the main chunk.\n            doc_header_key (str): The key for the headers in the current chunk.\n\n        Returns:\n            str: Renderted chunk with context and headers.\n        \"\"\"\n        combined_parts = [\"--- Previous Context ---\"]\n\n        combined_parts.extend(\n            self.process_peripheral_chunks(\n                chunks[:current_index],\n                peripheral_config.get(\"previous\", {}),\n                content_key,\n                order_key,\n            )\n        )\n        combined_parts.append(\"--- End Previous Context ---\\n\")\n\n        # Process main chunk\n        main_chunk = chunks[current_index]\n        if headers := self.render_hierarchy_headers(\n            main_chunk, chunks[: current_index + 1], doc_header_key\n        ):\n            combined_parts.append(headers)\n        combined_parts.extend(\n            (\n                f\"{main_chunk_start}\",\n                f\"{main_chunk[content_key]}\",\n                f\"{main_chunk_end}\",\n                \"\\n--- Next Context ---\",\n            )\n        )\n        combined_parts.extend(\n            self.process_peripheral_chunks(\n                chunks[current_index + 1 :],\n                peripheral_config.get(\"next\", {}),\n                content_key,\n                order_key,\n            )\n        )\n        combined_parts.append(\"--- End Next Context ---\")\n\n        return \"\\n\".join(combined_parts)\n\n    def process_peripheral_chunks(\n        self,\n        chunks: List[Dict],\n        config: Dict,\n        content_key: str,\n        order_key: str,\n        reverse: bool = False,\n    ) -&gt; List[str]:\n        \"\"\"\n        Process peripheral chunks according to the configuration.\n\n        Args:\n            chunks (List[Dict]): List of chunks to process.\n            config (Dict): Configuration for processing peripheral chunks.\n            content_key (str): Key for the content in each chunk.\n            order_key (str): Key for the order of each chunk.\n            reverse (bool, optional): Whether to process chunks in reverse order. Defaults to False.\n\n        Returns:\n            List[str]: List of processed chunk strings.\n        \"\"\"\n        if reverse:\n            chunks = list(reversed(chunks))\n\n        processed_parts = []\n        included_chunks = []\n        total_chunks = len(chunks)\n\n        head_config = config.get(\"head\", {})\n        tail_config = config.get(\"tail\", {})\n\n        head_count = int(head_config.get(\"count\", 0))\n        tail_count = int(tail_config.get(\"count\", 0))\n        in_skip = False\n        skip_char_count = 0\n\n        for i, chunk in enumerate(chunks):\n            if i &lt; head_count:\n                section = \"head\"\n            elif i &gt;= total_chunks - tail_count:\n                section = \"tail\"\n            elif \"middle\" in config:\n                section = \"middle\"\n            else:\n                # Show number of characters skipped\n                skipped_chars = len(chunk[content_key])\n                if not in_skip:\n                    skip_char_count = skipped_chars\n                    in_skip = True\n                else:\n                    skip_char_count += skipped_chars\n\n                continue\n\n            if in_skip:\n                processed_parts.append(\n                    f\"[... {skip_char_count} characters skipped ...]\"\n                )\n                in_skip = False\n                skip_char_count = 0\n\n            section_config = config.get(section, {})\n            section_content_key = section_config.get(\"content_key\", content_key)\n\n            is_summary = section_content_key != content_key\n            summary_suffix = \" (Summary)\" if is_summary else \"\"\n\n            chunk_prefix = f\"[Chunk {chunk[order_key]}{summary_suffix}]\"\n            processed_parts.extend((chunk_prefix, f\"{chunk[section_content_key]}\"))\n            included_chunks.append(chunk)\n\n        if in_skip:\n            processed_parts.append(f\"[... {skip_char_count} characters skipped ...]\")\n\n        if reverse:\n            processed_parts = list(reversed(processed_parts))\n\n        return processed_parts\n\n    def render_hierarchy_headers(\n        self,\n        current_chunk: Dict,\n        chunks: List[Dict],\n        doc_header_key: str,\n    ) -&gt; str:\n        \"\"\"\n        Render headers for the current chunk's hierarchy.\n\n        Args:\n            current_chunk (Dict): The current chunk being processed.\n            chunks (List[Dict]): List of chunks up to and including the current chunk.\n            doc_header_key (str): The key for the headers in the current chunk.\n        Returns:\n            str: Renderted headers in the current chunk's hierarchy.\n        \"\"\"\n        current_hierarchy = {}\n\n        if doc_header_key is None:\n            return \"\"\n\n        # Find the largest/highest level in the current chunk\n        current_chunk_headers = current_chunk.get(doc_header_key, [])\n        highest_level = float(\"inf\")  # Initialize with positive infinity\n        for header_info in current_chunk_headers:\n            level = header_info.get(\"level\")\n            if level is not None and level &lt; highest_level:\n                highest_level = level\n\n        # If no headers found in the current chunk, set highest_level to None\n        if highest_level == float(\"inf\"):\n            highest_level = None\n\n        for chunk in chunks:\n            for header_info in chunk.get(doc_header_key, []):\n                header = header_info[\"header\"]\n                level = header_info[\"level\"]\n                if header and level:\n                    current_hierarchy[level] = header\n                    # Clear lower levels when a higher level header is found\n                    for lower_level in range(level + 1, len(current_hierarchy) + 1):\n                        if lower_level in current_hierarchy:\n                            current_hierarchy[lower_level] = None\n\n        rendered_headers = [\n            f\"{'#' * level} {header}\"\n            for level, header in sorted(current_hierarchy.items())\n            if header is not None and (highest_level is None or level &lt; highest_level)\n        ]\n        rendered_headers = \" &gt; \".join(rendered_headers)\n        return f\"_Current Section:_ {rendered_headers}\" if rendered_headers else \"\"\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the GatherOperation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>docetl/operations/gather.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Initialize the GatherOperation.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Execute the gather operation on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Dict]</code> <p>The input data to process.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Dict], float]</code> <p>Tuple[List[Dict], float]: A tuple containing the processed results and the cost of the operation.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Execute the gather operation on the input data.\n\n    Args:\n        input_data (List[Dict]): The input data to process.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the processed results and the cost of the operation.\n    \"\"\"\n    content_key = self.config[\"content_key\"]\n    doc_id_key = self.config[\"doc_id_key\"]\n    order_key = self.config[\"order_key\"]\n    peripheral_config = self.config[\"peripheral_chunks\"]\n    main_chunk_start = self.config.get(\n        \"main_chunk_start\", \"--- Begin Main Chunk ---\"\n    )\n    main_chunk_end = self.config.get(\"main_chunk_end\", \"--- End Main Chunk ---\")\n    doc_header_key = self.config.get(\"doc_header_key\", None)\n    results = []\n    cost = 0.0\n\n    # Group chunks by document ID\n    grouped_chunks = {}\n    for item in input_data:\n        doc_id = item[doc_id_key]\n        if doc_id not in grouped_chunks:\n            grouped_chunks[doc_id] = []\n        grouped_chunks[doc_id].append(item)\n\n    # Process each group of chunks\n    for chunks in grouped_chunks.values():\n        # Sort chunks by their order within the document\n        chunks.sort(key=lambda x: x[order_key])\n\n        # Process each chunk with its peripheral context and headers\n        for i, chunk in enumerate(chunks):\n            rendered_chunk = self.render_chunk_with_context(\n                chunks,\n                i,\n                peripheral_config,\n                content_key,\n                order_key,\n                main_chunk_start,\n                main_chunk_end,\n                doc_header_key,\n            )\n\n            result = chunk.copy()\n            result[f\"{content_key}_rendered\"] = rendered_chunk\n            results.append(result)\n\n    return results, cost\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.process_peripheral_chunks","title":"<code>process_peripheral_chunks(chunks, config, content_key, order_key, reverse=False)</code>","text":"<p>Process peripheral chunks according to the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>List[Dict]</code> <p>List of chunks to process.</p> required <code>config</code> <code>Dict</code> <p>Configuration for processing peripheral chunks.</p> required <code>content_key</code> <code>str</code> <p>Key for the content in each chunk.</p> required <code>order_key</code> <code>str</code> <p>Key for the order of each chunk.</p> required <code>reverse</code> <code>bool</code> <p>Whether to process chunks in reverse order. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of processed chunk strings.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def process_peripheral_chunks(\n    self,\n    chunks: List[Dict],\n    config: Dict,\n    content_key: str,\n    order_key: str,\n    reverse: bool = False,\n) -&gt; List[str]:\n    \"\"\"\n    Process peripheral chunks according to the configuration.\n\n    Args:\n        chunks (List[Dict]): List of chunks to process.\n        config (Dict): Configuration for processing peripheral chunks.\n        content_key (str): Key for the content in each chunk.\n        order_key (str): Key for the order of each chunk.\n        reverse (bool, optional): Whether to process chunks in reverse order. Defaults to False.\n\n    Returns:\n        List[str]: List of processed chunk strings.\n    \"\"\"\n    if reverse:\n        chunks = list(reversed(chunks))\n\n    processed_parts = []\n    included_chunks = []\n    total_chunks = len(chunks)\n\n    head_config = config.get(\"head\", {})\n    tail_config = config.get(\"tail\", {})\n\n    head_count = int(head_config.get(\"count\", 0))\n    tail_count = int(tail_config.get(\"count\", 0))\n    in_skip = False\n    skip_char_count = 0\n\n    for i, chunk in enumerate(chunks):\n        if i &lt; head_count:\n            section = \"head\"\n        elif i &gt;= total_chunks - tail_count:\n            section = \"tail\"\n        elif \"middle\" in config:\n            section = \"middle\"\n        else:\n            # Show number of characters skipped\n            skipped_chars = len(chunk[content_key])\n            if not in_skip:\n                skip_char_count = skipped_chars\n                in_skip = True\n            else:\n                skip_char_count += skipped_chars\n\n            continue\n\n        if in_skip:\n            processed_parts.append(\n                f\"[... {skip_char_count} characters skipped ...]\"\n            )\n            in_skip = False\n            skip_char_count = 0\n\n        section_config = config.get(section, {})\n        section_content_key = section_config.get(\"content_key\", content_key)\n\n        is_summary = section_content_key != content_key\n        summary_suffix = \" (Summary)\" if is_summary else \"\"\n\n        chunk_prefix = f\"[Chunk {chunk[order_key]}{summary_suffix}]\"\n        processed_parts.extend((chunk_prefix, f\"{chunk[section_content_key]}\"))\n        included_chunks.append(chunk)\n\n    if in_skip:\n        processed_parts.append(f\"[... {skip_char_count} characters skipped ...]\")\n\n    if reverse:\n        processed_parts = list(reversed(processed_parts))\n\n    return processed_parts\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.render_chunk_with_context","title":"<code>render_chunk_with_context(chunks, current_index, peripheral_config, content_key, order_key, main_chunk_start, main_chunk_end, doc_header_key)</code>","text":"<p>Render a chunk with its peripheral context and headers.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>List[Dict]</code> <p>List of all chunks in the document.</p> required <code>current_index</code> <code>int</code> <p>Index of the current chunk being processed.</p> required <code>peripheral_config</code> <code>Dict</code> <p>Configuration for peripheral chunks.</p> required <code>content_key</code> <code>str</code> <p>Key for the content in each chunk.</p> required <code>order_key</code> <code>str</code> <p>Key for the order of each chunk.</p> required <code>main_chunk_start</code> <code>str</code> <p>String to mark the start of the main chunk.</p> required <code>main_chunk_end</code> <code>str</code> <p>String to mark the end of the main chunk.</p> required <code>doc_header_key</code> <code>str</code> <p>The key for the headers in the current chunk.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Renderted chunk with context and headers.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def render_chunk_with_context(\n    self,\n    chunks: List[Dict],\n    current_index: int,\n    peripheral_config: Dict,\n    content_key: str,\n    order_key: str,\n    main_chunk_start: str,\n    main_chunk_end: str,\n    doc_header_key: str,\n) -&gt; str:\n    \"\"\"\n    Render a chunk with its peripheral context and headers.\n\n    Args:\n        chunks (List[Dict]): List of all chunks in the document.\n        current_index (int): Index of the current chunk being processed.\n        peripheral_config (Dict): Configuration for peripheral chunks.\n        content_key (str): Key for the content in each chunk.\n        order_key (str): Key for the order of each chunk.\n        main_chunk_start (str): String to mark the start of the main chunk.\n        main_chunk_end (str): String to mark the end of the main chunk.\n        doc_header_key (str): The key for the headers in the current chunk.\n\n    Returns:\n        str: Renderted chunk with context and headers.\n    \"\"\"\n    combined_parts = [\"--- Previous Context ---\"]\n\n    combined_parts.extend(\n        self.process_peripheral_chunks(\n            chunks[:current_index],\n            peripheral_config.get(\"previous\", {}),\n            content_key,\n            order_key,\n        )\n    )\n    combined_parts.append(\"--- End Previous Context ---\\n\")\n\n    # Process main chunk\n    main_chunk = chunks[current_index]\n    if headers := self.render_hierarchy_headers(\n        main_chunk, chunks[: current_index + 1], doc_header_key\n    ):\n        combined_parts.append(headers)\n    combined_parts.extend(\n        (\n            f\"{main_chunk_start}\",\n            f\"{main_chunk[content_key]}\",\n            f\"{main_chunk_end}\",\n            \"\\n--- Next Context ---\",\n        )\n    )\n    combined_parts.extend(\n        self.process_peripheral_chunks(\n            chunks[current_index + 1 :],\n            peripheral_config.get(\"next\", {}),\n            content_key,\n            order_key,\n        )\n    )\n    combined_parts.append(\"--- End Next Context ---\")\n\n    return \"\\n\".join(combined_parts)\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.render_hierarchy_headers","title":"<code>render_hierarchy_headers(current_chunk, chunks, doc_header_key)</code>","text":"<p>Render headers for the current chunk's hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>current_chunk</code> <code>Dict</code> <p>The current chunk being processed.</p> required <code>chunks</code> <code>List[Dict]</code> <p>List of chunks up to and including the current chunk.</p> required <code>doc_header_key</code> <code>str</code> <p>The key for the headers in the current chunk.</p> required <p>Returns:     str: Renderted headers in the current chunk's hierarchy.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def render_hierarchy_headers(\n    self,\n    current_chunk: Dict,\n    chunks: List[Dict],\n    doc_header_key: str,\n) -&gt; str:\n    \"\"\"\n    Render headers for the current chunk's hierarchy.\n\n    Args:\n        current_chunk (Dict): The current chunk being processed.\n        chunks (List[Dict]): List of chunks up to and including the current chunk.\n        doc_header_key (str): The key for the headers in the current chunk.\n    Returns:\n        str: Renderted headers in the current chunk's hierarchy.\n    \"\"\"\n    current_hierarchy = {}\n\n    if doc_header_key is None:\n        return \"\"\n\n    # Find the largest/highest level in the current chunk\n    current_chunk_headers = current_chunk.get(doc_header_key, [])\n    highest_level = float(\"inf\")  # Initialize with positive infinity\n    for header_info in current_chunk_headers:\n        level = header_info.get(\"level\")\n        if level is not None and level &lt; highest_level:\n            highest_level = level\n\n    # If no headers found in the current chunk, set highest_level to None\n    if highest_level == float(\"inf\"):\n        highest_level = None\n\n    for chunk in chunks:\n        for header_info in chunk.get(doc_header_key, []):\n            header = header_info[\"header\"]\n            level = header_info[\"level\"]\n            if header and level:\n                current_hierarchy[level] = header\n                # Clear lower levels when a higher level header is found\n                for lower_level in range(level + 1, len(current_hierarchy) + 1):\n                    if lower_level in current_hierarchy:\n                        current_hierarchy[lower_level] = None\n\n    rendered_headers = [\n        f\"{'#' * level} {header}\"\n        for level, header in sorted(current_hierarchy.items())\n        if header is not None and (highest_level is None or level &lt; highest_level)\n    ]\n    rendered_headers = \" &gt; \".join(rendered_headers)\n    return f\"_Current Section:_ {rendered_headers}\" if rendered_headers else \"\"\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.gather.GatherOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Perform a syntax check on the operation configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing or if there are configuration errors.</p> <code>TypeError</code> <p>If main_chunk_start or main_chunk_end are not strings.</p> Source code in <code>docetl/operations/gather.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Perform a syntax check on the operation configuration.\n\n    Raises:\n        ValueError: If required keys are missing or if there are configuration errors.\n        TypeError: If main_chunk_start or main_chunk_end are not strings.\n    \"\"\"\n    required_keys = [\"content_key\", \"doc_id_key\", \"order_key\"]\n    for key in required_keys:\n        if key not in self.config:\n            raise ValueError(\n                f\"Missing required key '{key}' in GatherOperation configuration\"\n            )\n\n    if \"peripheral_chunks\" not in self.config:\n        raise ValueError(\n            \"Missing 'peripheral_chunks' configuration in GatherOperation\"\n        )\n\n    peripheral_config = self.config[\"peripheral_chunks\"]\n    for direction in [\"previous\", \"next\"]:\n        if direction not in peripheral_config:\n            continue\n        for section in [\"head\", \"middle\", \"tail\"]:\n            if section in peripheral_config[direction]:\n                section_config = peripheral_config[direction][section]\n                if section != \"middle\" and \"count\" not in section_config:\n                    raise ValueError(\n                        f\"Missing 'count' in {direction}.{section} configuration\"\n                    )\n\n    if \"main_chunk_start\" in self.config and not isinstance(\n        self.config[\"main_chunk_start\"], str\n    ):\n        raise TypeError(\"'main_chunk_start' must be a string\")\n    if \"main_chunk_end\" in self.config and not isinstance(\n        self.config[\"main_chunk_end\"], str\n    ):\n        raise TypeError(\"'main_chunk_end' must be a string\")\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.unnest.UnnestOperation","title":"<code>docetl.operations.unnest.UnnestOperation</code>","text":"<p>               Bases: <code>BaseOperation</code></p> <p>A class that represents an operation to unnest a list-like or dictionary value in a dictionary into multiple dictionaries.</p> <p>This operation takes a list of dictionaries and a specified key, and creates new dictionaries based on the value type: - For list-like values: Creates a new dictionary for each element in the list, copying all other key-value pairs. - For dictionary values: Expands specified fields from the nested dictionary into the parent dictionary.</p> Inherits from <p>BaseOperation</p> <p>Usage: <pre><code>from docetl.operations import UnnestOperation\n\n# Unnesting a list\nconfig_list = {\"unnest_key\": \"tags\"}\ninput_data_list = [\n    {\"id\": 1, \"tags\": [\"a\", \"b\", \"c\"]},\n    {\"id\": 2, \"tags\": [\"d\", \"e\"]}\n]\n\nunnest_op_list = UnnestOperation(config_list)\nresult_list, _ = unnest_op_list.execute(input_data_list)\n\n# Result will be:\n# [\n#     {\"id\": 1, \"tags\": \"a\"},\n#     {\"id\": 1, \"tags\": \"b\"},\n#     {\"id\": 1, \"tags\": \"c\"},\n#     {\"id\": 2, \"tags\": \"d\"},\n#     {\"id\": 2, \"tags\": \"e\"}\n# ]\n\n# Unnesting a dictionary\nconfig_dict = {\"unnest_key\": \"user\", \"expand_fields\": [\"name\", \"age\"]}\ninput_data_dict = [\n    {\"id\": 1, \"user\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}},\n    {\"id\": 2, \"user\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}}\n]\n\nunnest_op_dict = UnnestOperation(config_dict)\nresult_dict, _ = unnest_op_dict.execute(input_data_dict)\n\n# Result will be:\n# [\n#     {\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"user\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}},\n#     {\"id\": 2, \"name\": \"Bob\", \"age\": 25, \"user\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}}\n# ]\n</code></pre></p> Source code in <code>docetl/operations/unnest.py</code> <pre><code>class UnnestOperation(BaseOperation):\n    \"\"\"\n    A class that represents an operation to unnest a list-like or dictionary value in a dictionary into multiple dictionaries.\n\n    This operation takes a list of dictionaries and a specified key, and creates new dictionaries based on the value type:\n    - For list-like values: Creates a new dictionary for each element in the list, copying all other key-value pairs.\n    - For dictionary values: Expands specified fields from the nested dictionary into the parent dictionary.\n\n    Inherits from:\n        BaseOperation\n\n    Usage:\n    ```python\n    from docetl.operations import UnnestOperation\n\n    # Unnesting a list\n    config_list = {\"unnest_key\": \"tags\"}\n    input_data_list = [\n        {\"id\": 1, \"tags\": [\"a\", \"b\", \"c\"]},\n        {\"id\": 2, \"tags\": [\"d\", \"e\"]}\n    ]\n\n    unnest_op_list = UnnestOperation(config_list)\n    result_list, _ = unnest_op_list.execute(input_data_list)\n\n    # Result will be:\n    # [\n    #     {\"id\": 1, \"tags\": \"a\"},\n    #     {\"id\": 1, \"tags\": \"b\"},\n    #     {\"id\": 1, \"tags\": \"c\"},\n    #     {\"id\": 2, \"tags\": \"d\"},\n    #     {\"id\": 2, \"tags\": \"e\"}\n    # ]\n\n    # Unnesting a dictionary\n    config_dict = {\"unnest_key\": \"user\", \"expand_fields\": [\"name\", \"age\"]}\n    input_data_dict = [\n        {\"id\": 1, \"user\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}},\n        {\"id\": 2, \"user\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}}\n    ]\n\n    unnest_op_dict = UnnestOperation(config_dict)\n    result_dict, _ = unnest_op_dict.execute(input_data_dict)\n\n    # Result will be:\n    # [\n    #     {\"id\": 1, \"name\": \"Alice\", \"age\": 30, \"user\": {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}},\n    #     {\"id\": 2, \"name\": \"Bob\", \"age\": 25, \"user\": {\"name\": \"Bob\", \"age\": 25, \"email\": \"bob@example.com\"}}\n    # ]\n    ```\n    \"\"\"\n\n    def syntax_check(self) -&gt; None:\n        \"\"\"\n        Checks if the required configuration key is present in the operation's config.\n\n        Raises:\n            ValueError: If the required 'unnest_key' is missing from the configuration.\n        \"\"\"\n\n        required_keys = [\"unnest_key\"]\n        for key in required_keys:\n            if key not in self.config:\n                raise ValueError(\n                    f\"Missing required key '{key}' in UnnestOperation configuration\"\n                )\n\n    def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n        \"\"\"\n        Executes the unnest operation on the input data.\n\n        Args:\n            input_data (List[Dict]): A list of dictionaries to process.\n\n        Returns:\n            Tuple[List[Dict], float]: A tuple containing the processed list of dictionaries\n            and a float value (always 0 in this implementation).\n\n        Raises:\n            KeyError: If the specified unnest_key is not found in an input dictionary.\n            TypeError: If the value of the unnest_key is not iterable (list, tuple, set, or dict).\n            ValueError: If unnesting a dictionary and 'expand_fields' is not provided in the config.\n\n        The operation supports unnesting of both list-like values and dictionary values:\n\n        1. For list-like values (list, tuple, set):\n           Each element in the list becomes a separate dictionary in the output.\n\n        2. For dictionary values:\n           The operation expands specified fields from the nested dictionary into the parent dictionary.\n           The 'expand_fields' config parameter must be provided to specify which fields to expand.\n\n        Examples:\n        ```python\n        # Unnesting a list\n        unnest_op = UnnestOperation({\"unnest_key\": \"colors\"})\n        input_data = [\n            {\"id\": 1, \"colors\": [\"red\", \"blue\"]},\n            {\"id\": 2, \"colors\": [\"green\"]}\n        ]\n        result, _ = unnest_op.execute(input_data)\n        # Result will be:\n        # [\n        #     {\"id\": 1, \"colors\": \"red\"},\n        #     {\"id\": 1, \"colors\": \"blue\"},\n        #     {\"id\": 2, \"colors\": \"green\"}\n        # ]\n\n        # Unnesting a dictionary\n        unnest_op = UnnestOperation({\"unnest_key\": \"details\", \"expand_fields\": [\"color\", \"size\"]})\n        input_data = [\n            {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}},\n            {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}}\n        ]\n        result, _ = unnest_op.execute(input_data)\n        # Result will be:\n        # [\n        #     {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}, \"color\": \"red\", \"size\": \"large\"},\n        #     {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}, \"color\": \"blue\", \"size\": \"medium\"}\n        # ]\n        ```\n\n        Note: When unnesting dictionaries, the original nested dictionary is preserved in the output,\n        and the specified fields are expanded into the parent dictionary.\n        \"\"\"\n\n        unnest_key = self.config[\"unnest_key\"]\n        recursive = self.config.get(\"recursive\", False)\n        depth = self.config.get(\"depth\", None)\n        if not depth:\n            depth = 1 if not recursive else float(\"inf\")\n        results = []\n\n        def unnest_recursive(item, key, level=0):\n            if level == 0 and not isinstance(item[key], (list, tuple, set, dict)):\n                raise TypeError(f\"Value of unnest key '{key}' is not iterable\")\n\n            if level &gt; 0 and not isinstance(item[key], (list, tuple, set, dict)):\n                return [item]\n\n            if level &gt;= depth:\n                return [item]\n\n            if isinstance(item[key], dict):\n                expand_fields = self.config.get(\"expand_fields\")\n                if expand_fields is None:\n                    expand_fields = item[key].keys()\n                new_item = copy.deepcopy(item)\n                for field in expand_fields:\n                    if field in new_item[key]:\n                        new_item[field] = new_item[key][field]\n                    else:\n                        new_item[field] = None\n                return [new_item]\n            else:\n                nested_results = []\n                for value in item[key]:\n                    new_item = copy.deepcopy(item)\n                    new_item[key] = value\n                    if recursive and isinstance(value, (list, tuple, set, dict)):\n                        nested_results.extend(\n                            unnest_recursive(new_item, key, level + 1)\n                        )\n                    else:\n                        nested_results.append(new_item)\n                return nested_results\n\n        for item in input_data:\n            if unnest_key not in item:\n                raise KeyError(\n                    f\"Unnest key '{unnest_key}' not found in item. Other keys are {item.keys()}\"\n                )\n\n            results.extend(unnest_recursive(item, unnest_key))\n\n            if not item[unnest_key] and self.config.get(\"keep_empty\", False):\n                expand_fields = self.config.get(\"expand_fields\")\n                new_item = copy.deepcopy(item)\n                if isinstance(item[unnest_key], dict):\n                    if expand_fields is None:\n                        expand_fields = item[unnest_key].keys()\n                    for field in expand_fields:\n                        new_item[field] = None\n                else:\n                    new_item[unnest_key] = None\n                results.append(new_item)\n\n        # Assert that no keys are missing after the operation\n        if results:\n            original_keys = set(input_data[0].keys())\n            assert original_keys.issubset(\n                set(results[0].keys())\n            ), \"Keys lost during unnest operation\"\n\n        return results, 0\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.unnest.UnnestOperation.execute","title":"<code>execute(input_data)</code>","text":"<p>Executes the unnest operation on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>List[Dict]</code> <p>A list of dictionaries to process.</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>Tuple[List[Dict], float]: A tuple containing the processed list of dictionaries</p> <code>float</code> <p>and a float value (always 0 in this implementation).</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the specified unnest_key is not found in an input dictionary.</p> <code>TypeError</code> <p>If the value of the unnest_key is not iterable (list, tuple, set, or dict).</p> <code>ValueError</code> <p>If unnesting a dictionary and 'expand_fields' is not provided in the config.</p> <p>The operation supports unnesting of both list-like values and dictionary values:</p> <ol> <li> <p>For list-like values (list, tuple, set):    Each element in the list becomes a separate dictionary in the output.</p> </li> <li> <p>For dictionary values:    The operation expands specified fields from the nested dictionary into the parent dictionary.    The 'expand_fields' config parameter must be provided to specify which fields to expand.</p> </li> </ol> <p>Examples: <pre><code># Unnesting a list\nunnest_op = UnnestOperation({\"unnest_key\": \"colors\"})\ninput_data = [\n    {\"id\": 1, \"colors\": [\"red\", \"blue\"]},\n    {\"id\": 2, \"colors\": [\"green\"]}\n]\nresult, _ = unnest_op.execute(input_data)\n# Result will be:\n# [\n#     {\"id\": 1, \"colors\": \"red\"},\n#     {\"id\": 1, \"colors\": \"blue\"},\n#     {\"id\": 2, \"colors\": \"green\"}\n# ]\n\n# Unnesting a dictionary\nunnest_op = UnnestOperation({\"unnest_key\": \"details\", \"expand_fields\": [\"color\", \"size\"]})\ninput_data = [\n    {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}},\n    {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}}\n]\nresult, _ = unnest_op.execute(input_data)\n# Result will be:\n# [\n#     {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}, \"color\": \"red\", \"size\": \"large\"},\n#     {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}, \"color\": \"blue\", \"size\": \"medium\"}\n# ]\n</code></pre></p> <p>Note: When unnesting dictionaries, the original nested dictionary is preserved in the output, and the specified fields are expanded into the parent dictionary.</p> Source code in <code>docetl/operations/unnest.py</code> <pre><code>def execute(self, input_data: List[Dict]) -&gt; Tuple[List[Dict], float]:\n    \"\"\"\n    Executes the unnest operation on the input data.\n\n    Args:\n        input_data (List[Dict]): A list of dictionaries to process.\n\n    Returns:\n        Tuple[List[Dict], float]: A tuple containing the processed list of dictionaries\n        and a float value (always 0 in this implementation).\n\n    Raises:\n        KeyError: If the specified unnest_key is not found in an input dictionary.\n        TypeError: If the value of the unnest_key is not iterable (list, tuple, set, or dict).\n        ValueError: If unnesting a dictionary and 'expand_fields' is not provided in the config.\n\n    The operation supports unnesting of both list-like values and dictionary values:\n\n    1. For list-like values (list, tuple, set):\n       Each element in the list becomes a separate dictionary in the output.\n\n    2. For dictionary values:\n       The operation expands specified fields from the nested dictionary into the parent dictionary.\n       The 'expand_fields' config parameter must be provided to specify which fields to expand.\n\n    Examples:\n    ```python\n    # Unnesting a list\n    unnest_op = UnnestOperation({\"unnest_key\": \"colors\"})\n    input_data = [\n        {\"id\": 1, \"colors\": [\"red\", \"blue\"]},\n        {\"id\": 2, \"colors\": [\"green\"]}\n    ]\n    result, _ = unnest_op.execute(input_data)\n    # Result will be:\n    # [\n    #     {\"id\": 1, \"colors\": \"red\"},\n    #     {\"id\": 1, \"colors\": \"blue\"},\n    #     {\"id\": 2, \"colors\": \"green\"}\n    # ]\n\n    # Unnesting a dictionary\n    unnest_op = UnnestOperation({\"unnest_key\": \"details\", \"expand_fields\": [\"color\", \"size\"]})\n    input_data = [\n        {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}},\n        {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}}\n    ]\n    result, _ = unnest_op.execute(input_data)\n    # Result will be:\n    # [\n    #     {\"id\": 1, \"details\": {\"color\": \"red\", \"size\": \"large\", \"stock\": 5}, \"color\": \"red\", \"size\": \"large\"},\n    #     {\"id\": 2, \"details\": {\"color\": \"blue\", \"size\": \"medium\", \"stock\": 3}, \"color\": \"blue\", \"size\": \"medium\"}\n    # ]\n    ```\n\n    Note: When unnesting dictionaries, the original nested dictionary is preserved in the output,\n    and the specified fields are expanded into the parent dictionary.\n    \"\"\"\n\n    unnest_key = self.config[\"unnest_key\"]\n    recursive = self.config.get(\"recursive\", False)\n    depth = self.config.get(\"depth\", None)\n    if not depth:\n        depth = 1 if not recursive else float(\"inf\")\n    results = []\n\n    def unnest_recursive(item, key, level=0):\n        if level == 0 and not isinstance(item[key], (list, tuple, set, dict)):\n            raise TypeError(f\"Value of unnest key '{key}' is not iterable\")\n\n        if level &gt; 0 and not isinstance(item[key], (list, tuple, set, dict)):\n            return [item]\n\n        if level &gt;= depth:\n            return [item]\n\n        if isinstance(item[key], dict):\n            expand_fields = self.config.get(\"expand_fields\")\n            if expand_fields is None:\n                expand_fields = item[key].keys()\n            new_item = copy.deepcopy(item)\n            for field in expand_fields:\n                if field in new_item[key]:\n                    new_item[field] = new_item[key][field]\n                else:\n                    new_item[field] = None\n            return [new_item]\n        else:\n            nested_results = []\n            for value in item[key]:\n                new_item = copy.deepcopy(item)\n                new_item[key] = value\n                if recursive and isinstance(value, (list, tuple, set, dict)):\n                    nested_results.extend(\n                        unnest_recursive(new_item, key, level + 1)\n                    )\n                else:\n                    nested_results.append(new_item)\n            return nested_results\n\n    for item in input_data:\n        if unnest_key not in item:\n            raise KeyError(\n                f\"Unnest key '{unnest_key}' not found in item. Other keys are {item.keys()}\"\n            )\n\n        results.extend(unnest_recursive(item, unnest_key))\n\n        if not item[unnest_key] and self.config.get(\"keep_empty\", False):\n            expand_fields = self.config.get(\"expand_fields\")\n            new_item = copy.deepcopy(item)\n            if isinstance(item[unnest_key], dict):\n                if expand_fields is None:\n                    expand_fields = item[unnest_key].keys()\n                for field in expand_fields:\n                    new_item[field] = None\n            else:\n                new_item[unnest_key] = None\n            results.append(new_item)\n\n    # Assert that no keys are missing after the operation\n    if results:\n        original_keys = set(input_data[0].keys())\n        assert original_keys.issubset(\n            set(results[0].keys())\n        ), \"Keys lost during unnest operation\"\n\n    return results, 0\n</code></pre>"},{"location":"api-reference/operations/#docetl.operations.unnest.UnnestOperation.syntax_check","title":"<code>syntax_check()</code>","text":"<p>Checks if the required configuration key is present in the operation's config.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the required 'unnest_key' is missing from the configuration.</p> Source code in <code>docetl/operations/unnest.py</code> <pre><code>def syntax_check(self) -&gt; None:\n    \"\"\"\n    Checks if the required configuration key is present in the operation's config.\n\n    Raises:\n        ValueError: If the required 'unnest_key' is missing from the configuration.\n    \"\"\"\n\n    required_keys = [\"unnest_key\"]\n    for key in required_keys:\n        if key not in self.config:\n            raise ValueError(\n                f\"Missing required key '{key}' in UnnestOperation configuration\"\n            )\n</code></pre>"},{"location":"api-reference/optimizers/","title":"docetl.optimizers","text":""},{"location":"api-reference/optimizers/#docetl.optimizers.map_optimizer.optimizer.MapOptimizer","title":"<code>docetl.optimizers.map_optimizer.optimizer.MapOptimizer</code>","text":"<p>A class for optimizing map operations in data processing pipelines.</p> <p>This optimizer analyzes the input operation configuration and data, and generates optimized plans for executing the operation. It can create plans for chunking, metadata extraction, gleaning, chain decomposition, and parallel execution.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary for the optimizer.</p> <code>console</code> <code>Console</code> <p>A Rich console object for pretty printing.</p> <code>llm_client</code> <code>LLMClient</code> <p>A client for interacting with a language model.</p> <code>_run_operation</code> <code>Callable</code> <p>A function to execute operations.</p> <code>max_threads</code> <code>int</code> <p>The maximum number of threads to use for parallel execution.</p> <code>timeout</code> <code>int</code> <p>The timeout in seconds for operation execution.</p> Source code in <code>docetl/optimizers/map_optimizer/optimizer.py</code> <pre><code>class MapOptimizer:\n    \"\"\"\n    A class for optimizing map operations in data processing pipelines.\n\n    This optimizer analyzes the input operation configuration and data,\n    and generates optimized plans for executing the operation. It can\n    create plans for chunking, metadata extraction, gleaning, chain\n    decomposition, and parallel execution.\n\n    Attributes:\n        config (Dict[str, Any]): The configuration dictionary for the optimizer.\n        console (Console): A Rich console object for pretty printing.\n        llm_client (LLMClient): A client for interacting with a language model.\n        _run_operation (Callable): A function to execute operations.\n        max_threads (int): The maximum number of threads to use for parallel execution.\n        timeout (int): The timeout in seconds for operation execution.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        runner,\n        config: Dict[str, Any],\n        console: Console,\n        llm_client: LLMClient,\n        max_threads: int,\n        run_operation: Callable,\n        timeout: int = 10,\n        is_filter: bool = False,\n    ):\n        \"\"\"\n        Initialize the MapOptimizer.\n\n        Args:\n            config (Dict[str, Any]): The configuration dictionary for the optimizer.\n            console (Console): A Rich console object for pretty printing.\n            llm_client (LLMClient): A client for interacting with a language model.\n            max_threads (int): The maximum number of threads to use for parallel execution.\n            run_operation (Callable): A function to execute operations.\n            timeout (int, optional): The timeout in seconds for operation execution. Defaults to 10.\n            is_filter (bool, optional): If True, the operation is a filter operation. Defaults to False.\n        \"\"\"\n        self.runner = runner\n        self.config = config\n        self.console = console\n        self.llm_client = llm_client\n        self._run_operation = run_operation\n        self.max_threads = max_threads\n        self.timeout = timeout\n        self._num_plans_to_evaluate_in_parallel = 5\n        self.is_filter = is_filter\n        self.k_to_pairwise_compare = 6\n\n        self.plan_generator = PlanGenerator(\n            llm_client, console, config, run_operation, max_threads, is_filter\n        )\n        self.evaluator = Evaluator(\n            llm_client,\n            console,\n            run_operation,\n            timeout,\n            self._num_plans_to_evaluate_in_parallel,\n            is_filter,\n        )\n        self.prompt_generator = PromptGenerator(\n            llm_client, console, config, max_threads, is_filter\n        )\n\n    def optimize(\n        self, op_config: Dict[str, Any], input_data: List[Dict[str, Any]]\n    ) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]:\n        \"\"\"\n        Optimize the given operation configuration for the input data.\n        This method analyzes the operation and input data, generates various\n        optimization plans, evaluates them, and returns the best plan along\n        with its output. A key part of this process is creating a custom\n        validator prompt for evaluation. The validator prompt is generated\n        based on the specific task, input data, and output data. It serves\n        as a critical tool for assessing the quality and correctness of\n        each optimization plan's output. This custom prompt ensures that\n        the evaluation is tailored to the unique requirements and nuances\n        of the given operation. The types of optimization plans include:\n\n        1. Improved Prompt Plan: Enhances the original prompt based on evaluation, aiming to improve output quality.\n\n        2. Chunk Size Plan: Splits input data into chunks of different sizes,\n           processes each chunk separately, and then combines the results. This\n           can improve performance for large inputs.\n\n        3. Gleaning Plans: Implements an iterative refinement process where the\n           output is validated and improved over multiple rounds, enhancing accuracy.\n\n        4. Chain Decomposition Plan: Breaks down complex operations into a series\n           of simpler sub-operations, potentially improving overall performance\n           and interpretability.\n\n        5. Parallel Map Plan: Decomposes the task into subtasks that can be\n           executed in parallel, potentially speeding up processing for\n           independent operations.\n\n        The method generates these plans, evaluates their performance using\n        a custom validator, and selects the best performing plan based on\n        output quality and execution time.\n\n        Args:\n            op_config (Dict[str, Any]): The configuration of the operation to optimize.\n            input_data (List[Dict[str, Any]]): The input data for the operation.\n\n        Returns:\n            Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]: A tuple containing\n            the best optimization plan and its output. The plan is a list of\n            operation configurations that achieve the best performance.\n            The cost is the cost of the optimizer (from possibly synthesizing resolves).\n\n        \"\"\"\n        input_data = copy.deepcopy(input_data)\n        # Add id to each input_data\n        for i in range(len(input_data)):\n            input_data[i][\"_map_opt_id\"] = str(uuid.uuid4())\n\n        # Define the token limit (adjust as needed)\n        model_input_context_length = model_cost.get(\n            op_config.get(\"model\", self.config.get(\"default_model\")), {}\n        ).get(\"max_input_tokens\", 8192)\n\n        # Render the prompt with all sample inputs and count tokens\n        total_tokens = 0\n        exceed_count = 0\n        for sample in input_data:\n            rendered_prompt = Template(op_config[\"prompt\"]).render(input=sample)\n            prompt_tokens = count_tokens(\n                rendered_prompt,\n                op_config.get(\"model\", self.config.get(\"default_model\")),\n            )\n            total_tokens += prompt_tokens\n\n            if prompt_tokens &gt; model_input_context_length:\n                exceed_count += 1\n\n        # Calculate average tokens and percentage of samples exceeding limit\n        avg_tokens = total_tokens / len(input_data)\n        exceed_percentage = (exceed_count / len(input_data)) * 100\n\n        data_exceeds_limit = exceed_count &gt; 0\n        if exceed_count &gt; 0:\n            self.console.log(\n                f\"[yellow]Warning: {exceed_percentage:.2f}% of prompts exceed token limit. \"\n                f\"Average token count: {avg_tokens:.2f}. \"\n                f\"Truncating input data when generating validators.[/yellow]\"\n            )\n\n        # Execute the original operation on the sample data\n        no_change_start = time.time()\n        output_data = self._run_operation(op_config, input_data, is_build=True)\n        no_change_runtime = time.time() - no_change_start\n\n        # Generate custom validator prompt\n        validator_prompt = self.prompt_generator._generate_validator_prompt(\n            op_config, input_data, output_data\n        )\n\n        # Log the validator prompt\n        self.console.log(\"[bold]Validator Prompt:[/bold]\")\n        self.console.log(validator_prompt)\n        self.console.log(\"\\n\")  # Add a newline for better readability\n\n        # Step 2: Use the validator prompt to assess the operation's performance\n        assessment = self.evaluator._assess_operation(\n            op_config, input_data, output_data, validator_prompt\n        )\n\n        # Print out the assessment\n        self.console.log(\n            f\"[bold]Assessment for whether we should improve operation {op_config['name']}:[/bold]\"\n        )\n        for key, value in assessment.items():\n            self.console.print(\n                f\"[bold cyan]{key}:[/bold cyan] [yellow]{value}[/yellow]\"\n            )\n        self.console.log(\"\\n\")  # Add a newline for better readability\n\n        # Check if improvement is needed based on the assessment\n        if not data_exceeds_limit and not assessment.get(\"needs_improvement\", True):\n            self.console.log(\n                f\"[green]No improvement needed for operation {op_config['name']}[/green]\"\n            )\n            return [op_config], output_data, self.plan_generator.reduce_optimizer_cost\n\n        candidate_plans = {}\n\n        # Generate improved prompt plan\n        if not data_exceeds_limit:\n            #     improved_prompt_plan = self.prompt_generator._get_improved_prompt(\n            #         op_config, assessment, input_data\n            #     )\n            #     candidate_plans[\"improved_instructions\"] = improved_prompt_plan\n            candidate_plans[\"no_change\"] = [op_config]\n\n        # Generate chunk size plans\n        self.console.log(\"[bold magenta]Generating chunking plans...[/bold magenta]\")\n        chunk_size_plans = self.plan_generator._generate_chunk_size_plans(\n            op_config, input_data, validator_prompt, model_input_context_length\n        )\n        for pname, plan in chunk_size_plans.items():\n            candidate_plans[pname] = plan\n\n        # Generate gleaning plans\n        if not data_exceeds_limit:\n            self.console.log(\n                \"[bold magenta]Generating gleaning plans...[/bold magenta]\"\n            )\n            gleaning_plans = self.plan_generator._generate_gleaning_plans(\n                op_config, validator_prompt\n            )\n            for pname, plan in gleaning_plans.items():\n                candidate_plans[pname] = plan\n\n        # Generate chain decomposition plans\n        if not data_exceeds_limit:\n            if not self.is_filter:\n                self.console.log(\n                    \"[bold magenta]Generating chain projection synthesis plans...[/bold magenta]\"\n                )\n                chain_plans = self.plan_generator._generate_chain_plans(\n                    op_config, input_data\n                )\n                for pname, plan in chain_plans.items():\n                    candidate_plans[pname] = plan\n\n                # Generate parallel map plans\n                self.console.log(\n                    \"[bold magenta]Generating independent projection synthesis plans...[/bold magenta]\"\n                )\n                parallel_plans = self.plan_generator._generate_parallel_plans(\n                    op_config, input_data\n                )\n                for pname, plan in parallel_plans.items():\n                    candidate_plans[pname] = plan\n\n        # Select consistent evaluation samples\n        num_evaluations = min(5, len(input_data))\n        evaluation_samples = select_evaluation_samples(input_data, num_evaluations)\n\n        results = {}\n        plans_list = list(candidate_plans.items())\n\n        self.console.log(\n            f\"[bold magenta]Evaluating {len(plans_list)} plans...[/bold magenta]\"\n        )\n        for i in range(0, len(plans_list), self._num_plans_to_evaluate_in_parallel):\n            batch = plans_list[i : i + self._num_plans_to_evaluate_in_parallel]\n            with ThreadPoolExecutor(\n                max_workers=self._num_plans_to_evaluate_in_parallel\n            ) as executor:\n                futures = {\n                    executor.submit(\n                        self.evaluator._evaluate_plan,\n                        plan_name,\n                        op_config,\n                        plan,\n                        copy.deepcopy(evaluation_samples),\n                        validator_prompt,\n                    ): plan_name\n                    for plan_name, plan in batch\n                }\n                for future in as_completed(futures):\n                    plan_name = futures[future]\n                    try:\n                        score, runtime, output = future.result(timeout=self.timeout)\n                        results[plan_name] = (score, runtime, output)\n                    except concurrent.futures.TimeoutError:\n                        self.console.log(\n                            f\"[yellow]Plan {plan_name} timed out and will be skipped.[/yellow]\"\n                        )\n                    except Exception as e:\n                        # TODO: raise this error if the error is related to a Jinja error\n                        self.console.log(\n                            f\"[red]Error in plan {plan_name}: {str(e)}[/red]\"\n                        )\n                        import traceback\n\n                        print(traceback.format_exc())\n\n        # Add no change plan\n        if not data_exceeds_limit:\n            results[\"no_change\"] = (\n                results[\"no_change\"][0],\n                no_change_runtime,\n                results[\"no_change\"][2],\n            )\n\n        # Create a table of scores sorted in descending order\n        scores = sorted(\n            [(score, runtime, plan) for plan, (score, runtime, _) in results.items()],\n            reverse=True,\n        )\n\n        # Sort results by score in descending order\n        sorted_results = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n\n        # Take the top 6 plans\n        top_plans = sorted_results[: self.k_to_pairwise_compare]\n\n        # Check if there are no top plans\n        if len(top_plans) == 0:\n            raise ValueError(\n                \"Agent did not generate any plans. Unable to proceed with optimization. Try again.\"\n            )\n\n        # Include any additional plans that are tied with the last plan\n        tail_score = (\n            top_plans[-1][1][0]\n            if len(top_plans) == self.k_to_pairwise_compare\n            else float(\"-inf\")\n        )\n        filtered_results = dict(\n            top_plans\n            + [\n                item\n                for item in sorted_results[len(top_plans) :]\n                if item[1][0] == tail_score\n            ]\n        )\n\n        # Perform pairwise comparisons on filtered plans\n        if len(filtered_results) &gt; 1:\n            pairwise_rankings = self.evaluator._pairwise_compare_plans(\n                filtered_results, validator_prompt, op_config, evaluation_samples\n            )\n            best_plan_name = max(pairwise_rankings, key=pairwise_rankings.get)\n        else:\n            pairwise_rankings = {k: 0 for k in results.keys()}\n            best_plan_name = (\n                next(iter(filtered_results))\n                if filtered_results\n                else max(results, key=lambda x: results[x][0])\n            )\n\n        self.console.log(\n            f\"\\n[bold]Plan Evaluation Results for {op_config['name']} ({op_config['type']}, {len(scores)} plans, {num_evaluations} samples):[/bold]\"\n        )\n        table = Table(show_header=True, header_style=\"bold magenta\")\n        table.add_column(\"Plan\", style=\"dim\")\n        table.add_column(\"Score\", justify=\"right\", width=10)\n        table.add_column(\"Runtime\", justify=\"right\", width=10)\n        table.add_column(\"Pairwise Wins\", justify=\"right\", width=10)\n\n        for score, runtime, plan in scores:\n            table.add_row(\n                plan,\n                f\"{score:.2f}\",\n                f\"{runtime:.2f}s\",\n                f\"{pairwise_rankings.get(plan, 0)}\",\n            )\n\n        self.console.log(table)\n        self.console.log(\"\\n\")\n\n        _, _, best_output = results[best_plan_name]\n        self.console.log(\n            f\"[green]Choosing {best_plan_name} for operation {op_config['name']} (Score: {results[best_plan_name][0]:.2f}, Runtime: {results[best_plan_name][1]:.2f}s)[/green]\"\n        )\n\n        return (\n            candidate_plans[best_plan_name],\n            best_output,\n            self.plan_generator.reduce_optimizer_cost,\n        )\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.map_optimizer.optimizer.MapOptimizer.__init__","title":"<code>__init__(runner, config, console, llm_client, max_threads, run_operation, timeout=10, is_filter=False)</code>","text":"<p>Initialize the MapOptimizer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The configuration dictionary for the optimizer.</p> required <code>console</code> <code>Console</code> <p>A Rich console object for pretty printing.</p> required <code>llm_client</code> <code>LLMClient</code> <p>A client for interacting with a language model.</p> required <code>max_threads</code> <code>int</code> <p>The maximum number of threads to use for parallel execution.</p> required <code>run_operation</code> <code>Callable</code> <p>A function to execute operations.</p> required <code>timeout</code> <code>int</code> <p>The timeout in seconds for operation execution. Defaults to 10.</p> <code>10</code> <code>is_filter</code> <code>bool</code> <p>If True, the operation is a filter operation. Defaults to False.</p> <code>False</code> Source code in <code>docetl/optimizers/map_optimizer/optimizer.py</code> <pre><code>def __init__(\n    self,\n    runner,\n    config: Dict[str, Any],\n    console: Console,\n    llm_client: LLMClient,\n    max_threads: int,\n    run_operation: Callable,\n    timeout: int = 10,\n    is_filter: bool = False,\n):\n    \"\"\"\n    Initialize the MapOptimizer.\n\n    Args:\n        config (Dict[str, Any]): The configuration dictionary for the optimizer.\n        console (Console): A Rich console object for pretty printing.\n        llm_client (LLMClient): A client for interacting with a language model.\n        max_threads (int): The maximum number of threads to use for parallel execution.\n        run_operation (Callable): A function to execute operations.\n        timeout (int, optional): The timeout in seconds for operation execution. Defaults to 10.\n        is_filter (bool, optional): If True, the operation is a filter operation. Defaults to False.\n    \"\"\"\n    self.runner = runner\n    self.config = config\n    self.console = console\n    self.llm_client = llm_client\n    self._run_operation = run_operation\n    self.max_threads = max_threads\n    self.timeout = timeout\n    self._num_plans_to_evaluate_in_parallel = 5\n    self.is_filter = is_filter\n    self.k_to_pairwise_compare = 6\n\n    self.plan_generator = PlanGenerator(\n        llm_client, console, config, run_operation, max_threads, is_filter\n    )\n    self.evaluator = Evaluator(\n        llm_client,\n        console,\n        run_operation,\n        timeout,\n        self._num_plans_to_evaluate_in_parallel,\n        is_filter,\n    )\n    self.prompt_generator = PromptGenerator(\n        llm_client, console, config, max_threads, is_filter\n    )\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.map_optimizer.optimizer.MapOptimizer.optimize","title":"<code>optimize(op_config, input_data)</code>","text":"<p>Optimize the given operation configuration for the input data. This method analyzes the operation and input data, generates various optimization plans, evaluates them, and returns the best plan along with its output. A key part of this process is creating a custom validator prompt for evaluation. The validator prompt is generated based on the specific task, input data, and output data. It serves as a critical tool for assessing the quality and correctness of each optimization plan's output. This custom prompt ensures that the evaluation is tailored to the unique requirements and nuances of the given operation. The types of optimization plans include:</p> <ol> <li> <p>Improved Prompt Plan: Enhances the original prompt based on evaluation, aiming to improve output quality.</p> </li> <li> <p>Chunk Size Plan: Splits input data into chunks of different sizes,    processes each chunk separately, and then combines the results. This    can improve performance for large inputs.</p> </li> <li> <p>Gleaning Plans: Implements an iterative refinement process where the    output is validated and improved over multiple rounds, enhancing accuracy.</p> </li> <li> <p>Chain Decomposition Plan: Breaks down complex operations into a series    of simpler sub-operations, potentially improving overall performance    and interpretability.</p> </li> <li> <p>Parallel Map Plan: Decomposes the task into subtasks that can be    executed in parallel, potentially speeding up processing for    independent operations.</p> </li> </ol> <p>The method generates these plans, evaluates their performance using a custom validator, and selects the best performing plan based on output quality and execution time.</p> <p>Parameters:</p> Name Type Description Default <code>op_config</code> <code>Dict[str, Any]</code> <p>The configuration of the operation to optimize.</p> required <code>input_data</code> <code>List[Dict[str, Any]]</code> <p>The input data for the operation.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]: A tuple containing</p> <code>List[Dict[str, Any]]</code> <p>the best optimization plan and its output. The plan is a list of</p> <code>float</code> <p>operation configurations that achieve the best performance.</p> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]</code> <p>The cost is the cost of the optimizer (from possibly synthesizing resolves).</p> Source code in <code>docetl/optimizers/map_optimizer/optimizer.py</code> <pre><code>def optimize(\n    self, op_config: Dict[str, Any], input_data: List[Dict[str, Any]]\n) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]:\n    \"\"\"\n    Optimize the given operation configuration for the input data.\n    This method analyzes the operation and input data, generates various\n    optimization plans, evaluates them, and returns the best plan along\n    with its output. A key part of this process is creating a custom\n    validator prompt for evaluation. The validator prompt is generated\n    based on the specific task, input data, and output data. It serves\n    as a critical tool for assessing the quality and correctness of\n    each optimization plan's output. This custom prompt ensures that\n    the evaluation is tailored to the unique requirements and nuances\n    of the given operation. The types of optimization plans include:\n\n    1. Improved Prompt Plan: Enhances the original prompt based on evaluation, aiming to improve output quality.\n\n    2. Chunk Size Plan: Splits input data into chunks of different sizes,\n       processes each chunk separately, and then combines the results. This\n       can improve performance for large inputs.\n\n    3. Gleaning Plans: Implements an iterative refinement process where the\n       output is validated and improved over multiple rounds, enhancing accuracy.\n\n    4. Chain Decomposition Plan: Breaks down complex operations into a series\n       of simpler sub-operations, potentially improving overall performance\n       and interpretability.\n\n    5. Parallel Map Plan: Decomposes the task into subtasks that can be\n       executed in parallel, potentially speeding up processing for\n       independent operations.\n\n    The method generates these plans, evaluates their performance using\n    a custom validator, and selects the best performing plan based on\n    output quality and execution time.\n\n    Args:\n        op_config (Dict[str, Any]): The configuration of the operation to optimize.\n        input_data (List[Dict[str, Any]]): The input data for the operation.\n\n    Returns:\n        Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]: A tuple containing\n        the best optimization plan and its output. The plan is a list of\n        operation configurations that achieve the best performance.\n        The cost is the cost of the optimizer (from possibly synthesizing resolves).\n\n    \"\"\"\n    input_data = copy.deepcopy(input_data)\n    # Add id to each input_data\n    for i in range(len(input_data)):\n        input_data[i][\"_map_opt_id\"] = str(uuid.uuid4())\n\n    # Define the token limit (adjust as needed)\n    model_input_context_length = model_cost.get(\n        op_config.get(\"model\", self.config.get(\"default_model\")), {}\n    ).get(\"max_input_tokens\", 8192)\n\n    # Render the prompt with all sample inputs and count tokens\n    total_tokens = 0\n    exceed_count = 0\n    for sample in input_data:\n        rendered_prompt = Template(op_config[\"prompt\"]).render(input=sample)\n        prompt_tokens = count_tokens(\n            rendered_prompt,\n            op_config.get(\"model\", self.config.get(\"default_model\")),\n        )\n        total_tokens += prompt_tokens\n\n        if prompt_tokens &gt; model_input_context_length:\n            exceed_count += 1\n\n    # Calculate average tokens and percentage of samples exceeding limit\n    avg_tokens = total_tokens / len(input_data)\n    exceed_percentage = (exceed_count / len(input_data)) * 100\n\n    data_exceeds_limit = exceed_count &gt; 0\n    if exceed_count &gt; 0:\n        self.console.log(\n            f\"[yellow]Warning: {exceed_percentage:.2f}% of prompts exceed token limit. \"\n            f\"Average token count: {avg_tokens:.2f}. \"\n            f\"Truncating input data when generating validators.[/yellow]\"\n        )\n\n    # Execute the original operation on the sample data\n    no_change_start = time.time()\n    output_data = self._run_operation(op_config, input_data, is_build=True)\n    no_change_runtime = time.time() - no_change_start\n\n    # Generate custom validator prompt\n    validator_prompt = self.prompt_generator._generate_validator_prompt(\n        op_config, input_data, output_data\n    )\n\n    # Log the validator prompt\n    self.console.log(\"[bold]Validator Prompt:[/bold]\")\n    self.console.log(validator_prompt)\n    self.console.log(\"\\n\")  # Add a newline for better readability\n\n    # Step 2: Use the validator prompt to assess the operation's performance\n    assessment = self.evaluator._assess_operation(\n        op_config, input_data, output_data, validator_prompt\n    )\n\n    # Print out the assessment\n    self.console.log(\n        f\"[bold]Assessment for whether we should improve operation {op_config['name']}:[/bold]\"\n    )\n    for key, value in assessment.items():\n        self.console.print(\n            f\"[bold cyan]{key}:[/bold cyan] [yellow]{value}[/yellow]\"\n        )\n    self.console.log(\"\\n\")  # Add a newline for better readability\n\n    # Check if improvement is needed based on the assessment\n    if not data_exceeds_limit and not assessment.get(\"needs_improvement\", True):\n        self.console.log(\n            f\"[green]No improvement needed for operation {op_config['name']}[/green]\"\n        )\n        return [op_config], output_data, self.plan_generator.reduce_optimizer_cost\n\n    candidate_plans = {}\n\n    # Generate improved prompt plan\n    if not data_exceeds_limit:\n        #     improved_prompt_plan = self.prompt_generator._get_improved_prompt(\n        #         op_config, assessment, input_data\n        #     )\n        #     candidate_plans[\"improved_instructions\"] = improved_prompt_plan\n        candidate_plans[\"no_change\"] = [op_config]\n\n    # Generate chunk size plans\n    self.console.log(\"[bold magenta]Generating chunking plans...[/bold magenta]\")\n    chunk_size_plans = self.plan_generator._generate_chunk_size_plans(\n        op_config, input_data, validator_prompt, model_input_context_length\n    )\n    for pname, plan in chunk_size_plans.items():\n        candidate_plans[pname] = plan\n\n    # Generate gleaning plans\n    if not data_exceeds_limit:\n        self.console.log(\n            \"[bold magenta]Generating gleaning plans...[/bold magenta]\"\n        )\n        gleaning_plans = self.plan_generator._generate_gleaning_plans(\n            op_config, validator_prompt\n        )\n        for pname, plan in gleaning_plans.items():\n            candidate_plans[pname] = plan\n\n    # Generate chain decomposition plans\n    if not data_exceeds_limit:\n        if not self.is_filter:\n            self.console.log(\n                \"[bold magenta]Generating chain projection synthesis plans...[/bold magenta]\"\n            )\n            chain_plans = self.plan_generator._generate_chain_plans(\n                op_config, input_data\n            )\n            for pname, plan in chain_plans.items():\n                candidate_plans[pname] = plan\n\n            # Generate parallel map plans\n            self.console.log(\n                \"[bold magenta]Generating independent projection synthesis plans...[/bold magenta]\"\n            )\n            parallel_plans = self.plan_generator._generate_parallel_plans(\n                op_config, input_data\n            )\n            for pname, plan in parallel_plans.items():\n                candidate_plans[pname] = plan\n\n    # Select consistent evaluation samples\n    num_evaluations = min(5, len(input_data))\n    evaluation_samples = select_evaluation_samples(input_data, num_evaluations)\n\n    results = {}\n    plans_list = list(candidate_plans.items())\n\n    self.console.log(\n        f\"[bold magenta]Evaluating {len(plans_list)} plans...[/bold magenta]\"\n    )\n    for i in range(0, len(plans_list), self._num_plans_to_evaluate_in_parallel):\n        batch = plans_list[i : i + self._num_plans_to_evaluate_in_parallel]\n        with ThreadPoolExecutor(\n            max_workers=self._num_plans_to_evaluate_in_parallel\n        ) as executor:\n            futures = {\n                executor.submit(\n                    self.evaluator._evaluate_plan,\n                    plan_name,\n                    op_config,\n                    plan,\n                    copy.deepcopy(evaluation_samples),\n                    validator_prompt,\n                ): plan_name\n                for plan_name, plan in batch\n            }\n            for future in as_completed(futures):\n                plan_name = futures[future]\n                try:\n                    score, runtime, output = future.result(timeout=self.timeout)\n                    results[plan_name] = (score, runtime, output)\n                except concurrent.futures.TimeoutError:\n                    self.console.log(\n                        f\"[yellow]Plan {plan_name} timed out and will be skipped.[/yellow]\"\n                    )\n                except Exception as e:\n                    # TODO: raise this error if the error is related to a Jinja error\n                    self.console.log(\n                        f\"[red]Error in plan {plan_name}: {str(e)}[/red]\"\n                    )\n                    import traceback\n\n                    print(traceback.format_exc())\n\n    # Add no change plan\n    if not data_exceeds_limit:\n        results[\"no_change\"] = (\n            results[\"no_change\"][0],\n            no_change_runtime,\n            results[\"no_change\"][2],\n        )\n\n    # Create a table of scores sorted in descending order\n    scores = sorted(\n        [(score, runtime, plan) for plan, (score, runtime, _) in results.items()],\n        reverse=True,\n    )\n\n    # Sort results by score in descending order\n    sorted_results = sorted(results.items(), key=lambda x: x[1][0], reverse=True)\n\n    # Take the top 6 plans\n    top_plans = sorted_results[: self.k_to_pairwise_compare]\n\n    # Check if there are no top plans\n    if len(top_plans) == 0:\n        raise ValueError(\n            \"Agent did not generate any plans. Unable to proceed with optimization. Try again.\"\n        )\n\n    # Include any additional plans that are tied with the last plan\n    tail_score = (\n        top_plans[-1][1][0]\n        if len(top_plans) == self.k_to_pairwise_compare\n        else float(\"-inf\")\n    )\n    filtered_results = dict(\n        top_plans\n        + [\n            item\n            for item in sorted_results[len(top_plans) :]\n            if item[1][0] == tail_score\n        ]\n    )\n\n    # Perform pairwise comparisons on filtered plans\n    if len(filtered_results) &gt; 1:\n        pairwise_rankings = self.evaluator._pairwise_compare_plans(\n            filtered_results, validator_prompt, op_config, evaluation_samples\n        )\n        best_plan_name = max(pairwise_rankings, key=pairwise_rankings.get)\n    else:\n        pairwise_rankings = {k: 0 for k in results.keys()}\n        best_plan_name = (\n            next(iter(filtered_results))\n            if filtered_results\n            else max(results, key=lambda x: results[x][0])\n        )\n\n    self.console.log(\n        f\"\\n[bold]Plan Evaluation Results for {op_config['name']} ({op_config['type']}, {len(scores)} plans, {num_evaluations} samples):[/bold]\"\n    )\n    table = Table(show_header=True, header_style=\"bold magenta\")\n    table.add_column(\"Plan\", style=\"dim\")\n    table.add_column(\"Score\", justify=\"right\", width=10)\n    table.add_column(\"Runtime\", justify=\"right\", width=10)\n    table.add_column(\"Pairwise Wins\", justify=\"right\", width=10)\n\n    for score, runtime, plan in scores:\n        table.add_row(\n            plan,\n            f\"{score:.2f}\",\n            f\"{runtime:.2f}s\",\n            f\"{pairwise_rankings.get(plan, 0)}\",\n        )\n\n    self.console.log(table)\n    self.console.log(\"\\n\")\n\n    _, _, best_output = results[best_plan_name]\n    self.console.log(\n        f\"[green]Choosing {best_plan_name} for operation {op_config['name']} (Score: {results[best_plan_name][0]:.2f}, Runtime: {results[best_plan_name][1]:.2f}s)[/green]\"\n    )\n\n    return (\n        candidate_plans[best_plan_name],\n        best_output,\n        self.plan_generator.reduce_optimizer_cost,\n    )\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.reduce_optimizer.ReduceOptimizer","title":"<code>docetl.optimizers.reduce_optimizer.ReduceOptimizer</code>","text":"<p>A class that optimizes reduce operations in data processing pipelines.</p> <p>This optimizer analyzes the input and output of a reduce operation, creates and evaluates multiple reduce plans, and selects the best plan for optimizing the operation's performance.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict[str, Any]</code> <p>Configuration dictionary for the optimizer.</p> <code>console</code> <code>Console</code> <p>Rich console object for pretty printing.</p> <code>llm_client</code> <code>LLMClient</code> <p>Client for interacting with a language model.</p> <code>_run_operation</code> <code>Callable</code> <p>Function to run an operation.</p> <code>max_threads</code> <code>int</code> <p>Maximum number of threads to use for parallel processing.</p> <code>num_fold_prompts</code> <code>int</code> <p>Number of fold prompts to generate.</p> <code>num_samples_in_validation</code> <code>int</code> <p>Number of samples to use in validation.</p> Source code in <code>docetl/optimizers/reduce_optimizer.py</code> <pre><code>class ReduceOptimizer:\n    \"\"\"\n    A class that optimizes reduce operations in data processing pipelines.\n\n    This optimizer analyzes the input and output of a reduce operation, creates and evaluates\n    multiple reduce plans, and selects the best plan for optimizing the operation's performance.\n\n    Attributes:\n        config (Dict[str, Any]): Configuration dictionary for the optimizer.\n        console (Console): Rich console object for pretty printing.\n        llm_client (LLMClient): Client for interacting with a language model.\n        _run_operation (Callable): Function to run an operation.\n        max_threads (int): Maximum number of threads to use for parallel processing.\n        num_fold_prompts (int): Number of fold prompts to generate.\n        num_samples_in_validation (int): Number of samples to use in validation.\n    \"\"\"\n\n    def __init__(\n        self,\n        runner,\n        config: Dict[str, Any],\n        console: Console,\n        llm_client: LLMClient,\n        max_threads: int,\n        run_operation: Callable,\n        num_fold_prompts: int = 1,\n        num_samples_in_validation: int = 10,\n        status: Optional[Status] = None,\n    ):\n        \"\"\"\n        Initialize the ReduceOptimizer.\n\n        Args:\n            config (Dict[str, Any]): Configuration dictionary for the optimizer.\n            console (Console): Rich console object for pretty printing.\n            llm_client (LLMClient): Client for interacting with a language model.\n            max_threads (int): Maximum number of threads to use for parallel processing.\n            run_operation (Callable): Function to run an operation.\n            num_fold_prompts (int, optional): Number of fold prompts to generate. Defaults to 1.\n            num_samples_in_validation (int, optional): Number of samples to use in validation. Defaults to 10.\n        \"\"\"\n        self.runner = runner\n        self.config = config\n        self.console = console\n        self.llm_client = llm_client\n        self._run_operation = run_operation\n        self.max_threads = max_threads\n        self.num_fold_prompts = num_fold_prompts\n        self.num_samples_in_validation = num_samples_in_validation\n        self.status = status\n\n    def optimize(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        level: int = 1,\n    ) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]:\n        \"\"\"\n        Optimize the reduce operation based on the given configuration and input data.\n\n        This method performs the following steps:\n        1. Run the original operation\n        2. Generate a validator prompt\n        3. Validate the output\n        4. If improvement is needed:\n           a. Evaluate if decomposition is beneficial\n           b. If decomposition is beneficial, recursively optimize each sub-operation\n           c. If not, proceed with single operation optimization\n        5. Run the optimized operation(s)\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n\n        Returns:\n            Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]: A tuple containing the list of optimized configurations\n            and the list of outputs from the optimized operation(s), and the cost of the operation due to synthesizing any resolve operations.\n        \"\"\"\n        # Check if we're running out of token limits for the reduce prompt\n        model = op_config.get(\"model\", self.config.get(\"default_model\", \"gpt-4o-mini\"))\n        model_input_context_length = model_cost.get(model, {}).get(\n            \"max_input_tokens\", 4096\n        )\n\n        # Find the key with the longest value\n        longest_key = max(\n            op_config[\"reduce_key\"], key=lambda k: len(str(input_data[0][k]))\n        )\n        sample_key = tuple(\n            input_data[0][k] if k == longest_key else input_data[0][k]\n            for k in op_config[\"reduce_key\"]\n        )\n\n        # Render the prompt with a sample input\n        prompt_template = Template(op_config[\"prompt\"])\n        sample_prompt = prompt_template.render(\n            reduce_key=dict(zip(op_config[\"reduce_key\"], sample_key)),\n            inputs=[input_data[0]],\n        )\n\n        # Count tokens in the sample prompt\n        prompt_tokens = count_tokens(sample_prompt, model)\n\n        add_map_op = False\n        if prompt_tokens * 2 &gt; model_input_context_length:\n            add_map_op = True\n            self.console.log(\n                f\"[yellow]Warning: The reduce prompt exceeds the token limit for model {model}. \"\n                f\"Token count: {prompt_tokens}, Limit: {model_input_context_length}. \"\n                f\"Add a map operation to the pipeline.[/yellow]\"\n            )\n\n        # # Also query an agent to look at a sample of the inputs and see if they think a map operation would be helpful\n        # preprocessing_steps = \"\"\n        # should_use_map, preprocessing_steps = self._should_use_map(\n        #     op_config, input_data\n        # )\n        # if should_use_map or add_map_op:\n        #     # Synthesize a map operation\n        #     map_prompt, map_output_schema = self._synthesize_map_operation(\n        #         op_config, preprocessing_steps, input_data\n        #     )\n        #     # Change the reduce operation prompt to use the map schema\n        #     new_reduce_prompt = self._change_reduce_prompt_to_use_map_schema(\n        #         op_config[\"prompt\"], map_output_schema\n        #     )\n        #     op_config[\"prompt\"] = new_reduce_prompt\n\n        #     # Return unoptimized map and reduce operations\n        #     return [map_prompt, op_config], input_data, 0.0\n\n        original_output = self._run_operation(op_config, input_data)\n\n        # Step 1: Synthesize a validator prompt\n        validator_prompt = self._generate_validator_prompt(\n            op_config, input_data, original_output\n        )\n\n        # Log the validator prompt\n        self.console.log(\"[bold]Validator Prompt:[/bold]\")\n        self.console.log(validator_prompt)\n        self.console.log(\"\\n\")  # Add a newline for better readability\n\n        # Step 2: validate the output\n        validator_inputs = self._create_validation_inputs(\n            input_data, op_config[\"reduce_key\"]\n        )\n        validation_results = self._validate_reduce_output(\n            op_config, validator_inputs, original_output, validator_prompt\n        )\n\n        # Print the validation results\n        self.console.log(\"[bold]Validation Results on Initial Sample:[/bold]\")\n        if validation_results[\"needs_improvement\"]:\n            self.console.log(\n                \"\\n\".join(\n                    [\n                        f\"Issues: {result['issues']} Suggestions: {result['suggestions']}\"\n                        for result in validation_results[\"validation_results\"]\n                    ]\n                )\n            )\n\n            # Step 3: Evaluate if decomposition is beneficial\n            decomposition_result = self._evaluate_decomposition(\n                op_config, input_data, level\n            )\n\n            if decomposition_result[\"should_decompose\"]:\n                return self._optimize_decomposed_reduce(\n                    decomposition_result, op_config, input_data, level\n                )\n\n            return self._optimize_single_reduce(op_config, input_data, validator_prompt)\n        else:\n            self.console.log(\"No improvements identified.\")\n            return [op_config], original_output, 0.0\n\n    def _should_use_map(\n        self, op_config: Dict[str, Any], input_data: List[Dict[str, Any]]\n    ) -&gt; Tuple[bool, str]:\n        \"\"\"\n        Determine if a map operation should be used based on the input data.\n        \"\"\"\n        # Sample a random input item\n        sample_input = random.choice(input_data)\n\n        # Format the prompt with the sample input\n        prompt_template = Template(op_config[\"prompt\"])\n        formatted_prompt = prompt_template.render(\n            reduce_key=dict(\n                zip(op_config[\"reduce_key\"], sample_input[op_config[\"reduce_key\"]])\n            ),\n            inputs=[sample_input],\n        )\n\n        # Prepare the message for the LLM\n        messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n\n        # Truncate the messages to fit the model's context window\n        truncated_messages = truncate_messages(\n            messages, self.config.get(\"model\", self.default_model)\n        )\n\n        # Query the LLM for preprocessing suggestions\n        preprocessing_prompt = (\n            \"Based on the following reduce operation prompt, should we do any preprocessing on the input data? \"\n            \"Consider if we need to remove unnecessary context, or logically construct an output that will help in the task. \"\n            \"If preprocessing would be beneficial, explain why and suggest specific steps. If not, explain why preprocessing isn't necessary.\\n\\n\"\n            f\"Reduce operation prompt:\\n{truncated_messages[0]['content']}\"\n        )\n\n        preprocessing_response = self.llm_client.generate(\n            model=self.config.get(\"model\", self.default_model),\n            messages=[{\"role\": \"user\", \"content\": preprocessing_prompt}],\n            response_format={\n                \"type\": \"json_object\",\n                \"schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"preprocessing_needed\": {\"type\": \"boolean\"},\n                        \"rationale\": {\"type\": \"string\"},\n                        \"suggested_steps\": {\"type\": \"string\"},\n                    },\n                    \"required\": [\n                        \"preprocessing_needed\",\n                        \"rationale\",\n                        \"suggested_steps\",\n                    ],\n                },\n            },\n        )\n\n        preprocessing_result = preprocessing_response.choices[0].message.content\n\n        should_preprocess = preprocessing_result[\"preprocessing_needed\"]\n        preprocessing_rationale = preprocessing_result[\"rationale\"]\n\n        self.console.log(f\"[bold]Map-Reduce Decomposition Analysis:[/bold]\")\n        self.console.log(f\"Should write a map operation: {should_preprocess}\")\n        self.console.log(f\"Rationale: {preprocessing_rationale}\")\n\n        if should_preprocess:\n            self.console.log(\n                f\"Suggested steps: {preprocessing_result['suggested_steps']}\"\n            )\n\n        return should_preprocess, preprocessing_result[\"suggested_steps\"]\n\n    def _optimize_single_reduce(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        validator_prompt: str,\n    ) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]:\n        \"\"\"\n        Optimize a single reduce operation.\n\n        This method performs the following steps:\n        1. Determine and configure value sampling\n        2. Determine if the reduce operation is associative\n        3. Create and evaluate multiple reduce plans\n        4. Run the best reduce plan\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n            validator_prompt (str): The validator prompt for evaluating reduce plans.\n\n        Returns:\n            Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]: A tuple containing a single-item list with the optimized configuration\n            and a single-item list with the output from the optimized operation, and the cost of the operation due to synthesizing any resolve operations.\n        \"\"\"\n        # Step 1: Determine and configure value sampling (TODO: re-enable this when the agent is more reliable)\n        # value_sampling_config = self._determine_value_sampling(op_config, input_data)\n        # if value_sampling_config[\"enabled\"]:\n        #     op_config[\"value_sampling\"] = value_sampling_config\n        #     self.console.log(\"[bold]Value Sampling Configuration:[/bold]\")\n        #     self.console.log(json.dumps(value_sampling_config, indent=2))\n\n        # Step 2: Determine if the reduce operation is associative\n        is_associative = self._is_associative(op_config, input_data)\n\n        # Step 3: Create and evaluate multiple reduce plans\n        self.console.log(\"[bold magenta]Generating batched plans...[/bold magenta]\")\n        reduce_plans = self._create_reduce_plans(op_config, input_data, is_associative)\n\n        # Create gleaning plans\n        self.console.log(\"[bold magenta]Generating gleaning plans...[/bold magenta]\")\n        gleaning_plans = self._generate_gleaning_plans(reduce_plans, validator_prompt)\n\n        self.console.log(\"[bold magenta]Evaluating plans...[/bold magenta]\")\n        best_plan = self._evaluate_reduce_plans(\n            op_config, reduce_plans + gleaning_plans, input_data, validator_prompt\n        )\n\n        # Step 4: Run the best reduce plan\n        optimized_output = self._run_operation(best_plan, input_data)\n\n        return [best_plan], optimized_output, 0.0\n\n    def _generate_gleaning_plans(\n        self,\n        plans: List[Dict[str, Any]],\n        validation_prompt: str,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Generate plans that use gleaning for the given operation.\n\n        Gleaning involves iteratively refining the output of an operation\n        based on validation feedback. This method creates plans with different\n        numbers of gleaning rounds.\n\n        Args:\n            plans (List[Dict[str, Any]]): The list of plans to use for gleaning.\n            validation_prompt (str): The prompt used for validating the operation's output.\n\n        Returns:\n            Dict[str, List[Dict[str, Any]]]: A dictionary of gleaning plans, where each key\n            is a plan name and each value is a list containing a single operation configuration\n            with gleaning parameters.\n\n        \"\"\"\n        # Generate an op with gleaning num_rounds and validation_prompt\n        gleaning_plans = []\n        gleaning_rounds = [1]\n        biggest_batch_size = max([plan[\"fold_batch_size\"] for plan in plans])\n        for plan in plans:\n            if plan[\"fold_batch_size\"] != biggest_batch_size:\n                continue\n            for gleaning_round in gleaning_rounds:\n                plan_copy = copy.deepcopy(plan)\n                plan_copy[\"gleaning\"] = {\n                    \"num_rounds\": gleaning_round,\n                    \"validation_prompt\": validation_prompt,\n                }\n                plan_name = f\"gleaning_{gleaning_round}_rounds_{plan['name']}\"\n                plan_copy[\"name\"] = plan_name\n                gleaning_plans.append(plan_copy)\n        return gleaning_plans\n\n    def _optimize_decomposed_reduce(\n        self,\n        decomposition_result: Dict[str, Any],\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        level: int,\n    ) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]:\n        \"\"\"\n        Optimize a decomposed reduce operation.\n\n        This method performs the following steps:\n        1. Group the input data by the sub-group key.\n        2. Optimize the first reduce operation.\n        3. Run the optimized first reduce operation on all groups.\n        4. Optimize the second reduce operation using the results of the first.\n        5. Run the optimized second reduce operation.\n\n        Args:\n            decomposition_result (Dict[str, Any]): The result of the decomposition evaluation.\n            op_config (Dict[str, Any]): The original reduce operation configuration.\n            input_data (List[Dict[str, Any]]): The input data for the reduce operation.\n            level (int): The current level of decomposition.\n        Returns:\n            Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]: A tuple containing the list of optimized configurations\n            for both reduce operations and the final output of the second reduce operation, and the cost of the operation due to synthesizing any resolve operations.\n        \"\"\"\n        sub_group_key = decomposition_result[\"sub_group_key\"]\n        first_reduce_prompt = decomposition_result[\"first_reduce_prompt\"]\n        second_reduce_prompt = decomposition_result[\"second_reduce_prompt\"]\n        pipeline = []\n        all_cost = 0.0\n\n        first_reduce_config = op_config.copy()\n        first_reduce_config[\"prompt\"] = first_reduce_prompt\n        if isinstance(op_config[\"reduce_key\"], list):\n            first_reduce_config[\"reduce_key\"] = [sub_group_key] + op_config[\n                \"reduce_key\"\n            ]\n        else:\n            first_reduce_config[\"reduce_key\"] = [sub_group_key, op_config[\"reduce_key\"]]\n        first_reduce_config[\"pass_through\"] = True\n\n        if first_reduce_config.get(\"synthesize_resolve\", True):\n            resolve_config = {\n                \"type\": \"resolve\",\n                \"empty\": True,\n                \"embedding_model\": \"text-embedding-3-small\",\n                \"resolution_model\": self.config.get(\"default_model\", \"gpt-4o-mini\"),\n                \"comparison_model\": self.config.get(\"default_model\", \"gpt-4o-mini\"),\n                \"_intermediates\": {\n                    \"map_prompt\": op_config.get(\"_intermediates\", {}).get(\n                        \"last_map_prompt\"\n                    ),\n                    \"reduce_key\": first_reduce_config[\"reduce_key\"],\n                },\n            }\n            optimized_resolve_config, resolve_cost = JoinOptimizer(\n                self.config,\n                resolve_config,\n                self.console,\n                self.llm_client,\n                self.max_threads,\n            ).optimize_resolve(input_data)\n            all_cost += resolve_cost\n\n            if not optimized_resolve_config.get(\"empty\", False):\n                # Add this to the pipeline\n                pipeline += [optimized_resolve_config]\n\n                # Run the resolver\n                optimized_output = self._run_operation(\n                    optimized_resolve_config, input_data\n                )\n                input_data = optimized_output\n\n        first_optimized_configs, first_outputs, first_cost = self.optimize(\n            first_reduce_config, input_data, level + 1\n        )\n        pipeline += first_optimized_configs\n        all_cost += first_cost\n\n        # Optimize second reduce operation\n        second_reduce_config = op_config.copy()\n        second_reduce_config[\"prompt\"] = second_reduce_prompt\n        second_reduce_config[\"pass_through\"] = True\n\n        second_optimized_configs, second_outputs, second_cost = self.optimize(\n            second_reduce_config, first_outputs, level + 1\n        )\n\n        # Combine optimized configs and return with final output\n        pipeline += second_optimized_configs\n        all_cost += second_cost\n\n        return pipeline, second_outputs, all_cost\n\n    def _evaluate_decomposition(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        level: int = 1,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate whether decomposing the reduce operation would be beneficial.\n\n        This method first determines if decomposition would be helpful, and if so,\n        it then determines the sub-group key and prompts for the decomposed operations.\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n            level (int): The current level of decomposition.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the decomposition decision and details.\n        \"\"\"\n        should_decompose = self._should_decompose(op_config, input_data, level)\n\n        # Log the decomposition decision\n        if should_decompose[\"should_decompose\"]:\n            self.console.log(\n                f\"[bold green]Decomposition recommended:[/bold green] {should_decompose['explanation']}\"\n            )\n        else:\n            self.console.log(\n                f\"[bold yellow]Decomposition not recommended:[/bold yellow] {should_decompose['explanation']}\"\n            )\n\n        # Return early if decomposition is not recommended\n        if not should_decompose[\"should_decompose\"]:\n            return should_decompose\n\n        # Temporarily stop the status\n        if self.status:\n            self.status.stop()\n\n        # Ask user if they agree with the decomposition assessment\n        user_agrees = Confirm.ask(\n            f\"Do you agree with the decomposition assessment? \"\n            f\"[bold]{'Recommended' if should_decompose['should_decompose'] else 'Not recommended'}[/bold]\"\n        )\n\n        # If user disagrees, invert the decomposition decision\n        if not user_agrees:\n            should_decompose[\"should_decompose\"] = not should_decompose[\n                \"should_decompose\"\n            ]\n            should_decompose[\"explanation\"] = (\n                \"User disagreed with the initial assessment.\"\n            )\n\n        # Restart the status\n        if self.status:\n            self.status.start()\n\n        # Return if decomposition is not recommended\n        if not should_decompose[\"should_decompose\"]:\n            return should_decompose\n\n        decomposition_details = self._get_decomposition_details(op_config, input_data)\n        result = {**should_decompose, **decomposition_details}\n        if decomposition_details[\"sub_group_key\"] in op_config[\"reduce_key\"]:\n            result[\"should_decompose\"] = False\n            result[\n                \"explanation\"\n            ] += \" However, the suggested sub-group key is already part of the current reduce key(s), so decomposition is not recommended.\"\n            result[\"sub_group_key\"] = \"\"\n\n        return result\n\n    def _should_decompose(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        level: int = 1,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Determine if decomposing the reduce operation would be beneficial.\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n            level (int): The current level of decomposition.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the decomposition decision and explanation.\n        \"\"\"\n        # TODO: we have not enabled recursive decomposition yet\n        if level &gt; 1 and not op_config.get(\"recursively_optimize\", False):\n            return {\n                \"should_decompose\": False,\n                \"explanation\": \"Recursive decomposition is not enabled.\",\n            }\n\n        system_prompt = (\n            \"You are an AI assistant tasked with optimizing data processing pipelines.\"\n        )\n\n        # Sample a subset of input data for analysis\n        sample_size = min(10, len(input_data))\n        sample_input = random.sample(input_data, sample_size)\n\n        # Get all keys from the input data\n        all_keys = set().union(*(item.keys() for item in sample_input))\n        reduce_key = op_config[\"reduce_key\"]\n        reduce_keys = [reduce_key] if isinstance(reduce_key, str) else reduce_key\n        other_keys = [key for key in all_keys if key not in reduce_keys]\n\n        # See if there's an input schema and constrain the sample_input to that schema\n        input_schema = op_config.get(\"input\", {}).get(\"schema\", {})\n        if input_schema:\n            sample_input = [\n                {key: item[key] for key in input_schema} for item in sample_input\n            ]\n\n        # Create a sample of values for other keys\n        sample_values = {\n            key: list(set(str(item.get(key))[:50] for item in sample_input))[:5]\n            for key in other_keys\n        }\n\n        prompt = f\"\"\"Analyze the following reduce operation and determine if it should be decomposed into two reduce operations chained together:\n\n        Reduce Operation Prompt:\n        ```\n        {op_config['prompt']}\n        ```\n\n        Current Reduce Key(s): {reduce_keys}\n        Other Available Keys: {', '.join(other_keys)}\n\n        Sample values for other keys:\n        {json.dumps(sample_values, indent=2)}\n\n        Based on this information, determine if it would be beneficial to decompose this reduce operation into a sub-reduce operation followed by a final reduce operation. Consider the following:\n\n        1. Is there a natural hierarchy in the data (e.g., country -&gt; state -&gt; city) among the other available keys, with a key at a finer level of granularity than the current reduce key(s)?\n        2. Are the current reduce key(s) some form of ID, and are there many different types of inputs for that ID among the other available keys?\n        3. Does the prompt implicitly ask for sub-grouping based on the other available keys (e.g., \"summarize policies by state, then by country\")?\n        4. Would splitting the operation improve accuracy (i.e., make sure information isn't lost when reducing)?\n        5. Are all the keys of the potential hierarchy provided in the other available keys? If not, we should not decompose.\n        6. Importantly, do not suggest decomposition using any key that is already part of the current reduce key(s). We are looking for a new key from the other available keys to use for sub-grouping.\n\n        Provide your analysis in the following format:\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"should_decompose\": {\"type\": \"boolean\"},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"should_decompose\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        return json.loads(response.choices[0].message.content)\n\n    def _get_decomposition_details(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Determine the sub-group key and prompts for decomposed reduce operations.\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing the sub-group key and prompts for decomposed operations.\n        \"\"\"\n        system_prompt = (\n            \"You are an AI assistant tasked with optimizing data processing pipelines.\"\n        )\n\n        # Sample a subset of input data for analysis\n        sample_size = min(10, len(input_data))\n        sample_input = random.sample(input_data, sample_size)\n\n        # Get all keys from the input data\n        all_keys = set().union(*(item.keys() for item in sample_input))\n        reduce_key = op_config[\"reduce_key\"]\n        reduce_keys = [reduce_key] if isinstance(reduce_key, str) else reduce_key\n        other_keys = [key for key in all_keys if key not in reduce_keys]\n\n        prompt = f\"\"\"Given that we've decided to decompose the following reduce operation, suggest a two-step reduce process:\n\n        Reduce Operation Prompt:\n        ```\n        {op_config['prompt']}\n        ```\n\n        Reduce Key(s): {reduce_key}\n        Other Keys: {', '.join(other_keys)}\n\n        Provide the following:\n        1. A sub-group key to use for the first reduce operation\n        2. A prompt for the first reduce operation\n        3. A prompt for the second (final) reduce operation\n\n        For the reduce operation prompts, you should only minimally modify the original prompt. The prompts should be Jinja templates, and the only variables they can access are the `reduce_key` and `inputs` variables.\n\n        Provide your suggestions in the following format:\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sub_group_key\": {\"type\": \"string\"},\n                \"first_reduce_prompt\": {\"type\": \"string\"},\n                \"second_reduce_prompt\": {\"type\": \"string\"},\n            },\n            \"required\": [\n                \"sub_group_key\",\n                \"first_reduce_prompt\",\n                \"second_reduce_prompt\",\n            ],\n        }\n\n        response = self.llm_client.generate(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        return json.loads(response.choices[0].message.content)\n\n    def _determine_value_sampling(\n        self, op_config: Dict[str, Any], input_data: List[Dict[str, Any]]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Determine whether value sampling should be enabled and configure its parameters.\n        \"\"\"\n        system_prompt = (\n            \"You are an AI assistant helping to optimize data processing pipelines.\"\n        )\n\n        # Sample a subset of input data for analysis\n        sample_size = min(100, len(input_data))\n        sample_input = random.sample(input_data, sample_size)\n\n        prompt = f\"\"\"\n        Analyze the following reduce operation and determine if value sampling should be enabled:\n\n        Reduce Operation Prompt:\n        {op_config['prompt']}\n\n        Sample Input Data (first 2 items):\n        {json.dumps(sample_input[:2], indent=2)}\n\n        Value sampling is appropriate for reduce operations that don't need to look at all the values for each key to produce a good result, such as generic summarization tasks.\n\n        Based on the reduce operation prompt and the sample input data, determine if value sampling should be enabled.\n        Answer with 'yes' if value sampling should be enabled or 'no' if it should not be enabled. Explain your reasoning briefly.\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"enable_sampling\": {\"type\": \"boolean\"},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"enable_sampling\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        result = json.loads(response.choices[0].message.content)\n\n        if not result[\"enable_sampling\"]:\n            return {\"enabled\": False}\n\n        # Print the explanation for enabling value sampling\n        self.console.log(f\"Value sampling enabled: {result['explanation']}\")\n\n        # Determine sampling method\n        prompt = f\"\"\"\n        We are optimizing a reduce operation in a data processing pipeline. The reduce operation is defined by the following prompt:\n\n        Reduce Operation Prompt:\n        {op_config['prompt']}\n\n        Sample Input Data (first 2 items):\n        {json.dumps(sample_input[:2], indent=2)}\n\n        We have determined that value sampling should be enabled for this reduce operation. Value sampling is a technique used to process only a subset of the input data for each reduce key, rather than processing all items. This can significantly reduce processing time and costs for very large datasets, especially when the reduce operation doesn't require looking at every single item to produce a good result (e.g., summarization tasks).\n\n        Now we need to choose the most appropriate sampling method. The available methods are:\n\n        1. \"random\": Randomly select a subset of values.\n        Example: In a customer review analysis task, randomly selecting a subset of reviews to summarize the overall sentiment.\n\n        2. \"cluster\": Use K-means clustering to select representative samples.\n        Example: In a document categorization task, clustering documents based on their content and selecting representative documents from each cluster to determine the overall categories.\n\n        3. \"sem_sim\": Use semantic similarity to select the most relevant samples to a query text.\n        Example: In a news article summarization task, selecting articles that are semantically similar to a query like \"Major economic events of {{reduce_key}}\" to produce a focused summary.\n\n        Based on the reduce operation prompt, the nature of the task, and the sample input data, which sampling method would be most appropriate?\n\n        Provide your answer as either \"random\", \"cluster\", or \"sem_sim\", and explain your reasoning in detail. Consider the following in your explanation:\n        - The nature of the reduce task (e.g., summarization, aggregation, analysis)\n        - The structure and content of the input data\n        - The potential benefits and drawbacks of each sampling method for this specific task\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"method\": {\"type\": \"string\", \"enum\": [\"random\", \"cluster\", \"sem_sim\"]},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"method\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        result = json.loads(response.choices[0].message.content)\n        method = result[\"method\"]\n\n        value_sampling_config = {\n            \"enabled\": True,\n            \"method\": method,\n            \"sample_size\": 100,  # Default sample size\n            \"embedding_model\": \"text-embedding-3-small\",\n        }\n\n        if method in [\"cluster\", \"sem_sim\"]:\n            # Determine embedding keys\n            prompt = f\"\"\"\n            For the {method} sampling method, we need to determine which keys from the input data should be used for generating embeddings.\n\n            Input data keys:\n            {', '.join(sample_input[0].keys())}\n\n            Sample Input Data:\n            {json.dumps(sample_input[0], indent=2)[:1000]}...\n\n            Based on the reduce operation prompt and the sample input data, which keys should be used for generating embeddings? Use keys that will create meaningful embeddings (i.e., not id-related keys).\n            Provide your answer as a list of key names that is a subset of the input data keys. You should pick only the 1-3 keys that are necessary for generating meaningful embeddings, that have relatively short values.\n            \"\"\"\n\n            parameters = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"embedding_keys\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                    \"explanation\": {\"type\": \"string\"},\n                },\n                \"required\": [\"embedding_keys\", \"explanation\"],\n            }\n\n            response = self.llm_client.generate(\n                [{\"role\": \"user\", \"content\": prompt}],\n                system_prompt,\n                parameters,\n            )\n            result = json.loads(response.choices[0].message.content)\n            # TODO: validate that these exist\n            embedding_keys = result[\"embedding_keys\"]\n            for key in result[\"embedding_keys\"]:\n                if key not in sample_input[0]:\n                    embedding_keys.remove(key)\n\n            if not embedding_keys:\n                # Select the reduce key\n                self.console.log(\n                    \"No embedding keys found, selecting reduce key for embedding key\"\n                )\n                embedding_keys = (\n                    op_config[\"reduce_key\"]\n                    if isinstance(op_config[\"reduce_key\"], list)\n                    else [op_config[\"reduce_key\"]]\n                )\n\n            value_sampling_config[\"embedding_keys\"] = embedding_keys\n\n        if method == \"sem_sim\":\n            # Determine query text\n            prompt = f\"\"\"\n            For the semantic similarity (sem_sim) sampling method, we need to determine the query text to compare against when selecting samples.\n\n            Reduce Operation Prompt:\n            {op_config['prompt']}\n\n            The query text should be a Jinja template with access to the `reduce_key` variable.\n            Based on the reduce operation prompt, what would be an appropriate query text for selecting relevant samples?\n            \"\"\"\n\n            parameters = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query_text\": {\"type\": \"string\"},\n                    \"explanation\": {\"type\": \"string\"},\n                },\n                \"required\": [\"query_text\", \"explanation\"],\n            }\n\n            response = self.llm_client.generate(\n                [{\"role\": \"user\", \"content\": prompt}],\n                system_prompt,\n                parameters,\n            )\n            result = json.loads(response.choices[0].message.content)\n            value_sampling_config[\"query_text\"] = result[\"query_text\"]\n\n        return value_sampling_config\n\n    def _is_associative(\n        self, op_config: Dict[str, Any], input_data: List[Dict[str, Any]]\n    ) -&gt; bool:\n        \"\"\"\n        Determine if the reduce operation is associative.\n\n        This method analyzes the reduce operation configuration and a sample of the input data\n        to determine if the operation is associative (i.e., the order of combining elements\n        doesn't affect the final result).\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n\n        Returns:\n            bool: True if the operation is determined to be associative, False otherwise.\n        \"\"\"\n        system_prompt = (\n            \"You are an AI assistant helping to optimize data processing pipelines.\"\n        )\n\n        # Sample a subset of input data for analysis\n        sample_size = min(5, len(input_data))\n        sample_input = random.sample(input_data, sample_size)\n\n        prompt = f\"\"\"\n        Analyze the following reduce operation and determine if it is associative:\n\n        Reduce Operation Prompt:\n        {op_config['prompt']}\n\n        Sample Input Data:\n        {json.dumps(sample_input, indent=2)[:1000]}...\n\n        Based on the reduce operation prompt, determine whether the order in which we process data matters.\n        Answer with 'yes' if order matters or 'no' if order doesn't matter.\n        Explain your reasoning briefly.\n\n        For example:\n        - Merging extracted key-value pairs from documents does not require order: combining {{\"name\": \"John\", \"age\": 30}} with {{\"city\": \"New York\", \"job\": \"Engineer\"}} yields the same result regardless of order\n        - Generating a timeline of events requires order: the order of events matters for maintaining chronological accuracy.\n\n        Consider these examples when determining whether the order in which we process data matters. You might also have to consider the specific data.\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"order_matters\": {\"type\": \"boolean\"},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"order_matters\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        result = json.loads(response.choices[0].message.content)\n        result[\"is_associative\"] = not result[\"order_matters\"]\n\n        self.console.log(\n            f\"[yellow]Reduce operation {'is associative' if result['is_associative'] else 'is not associative'}.[/yellow] Analysis: {result['explanation']}\"\n        )\n        return result[\"is_associative\"]\n\n    def _generate_validator_prompt(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        original_output: List[Dict[str, Any]],\n    ) -&gt; str:\n        \"\"\"\n        Generate a custom validator prompt for assessing the quality of the reduce operation output.\n\n        This method creates a prompt that will be used to validate the output of the reduce operation.\n        It includes specific questions about the quality and completeness of the output.\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n            original_output (List[Dict[str, Any]]): Original output of the reduce operation.\n\n        Returns:\n            str: A custom validator prompt as a string.\n        \"\"\"\n        system_prompt = \"You are an AI assistant tasked with creating custom validation prompts for reduce operations in data processing pipelines.\"\n\n        sample_input = random.choice(input_data)\n        input_keys = op_config.get(\"input\", {}).get(\"schema\", {})\n        if input_keys:\n            sample_input = {k: sample_input[k] for k in input_keys}\n\n        reduce_key = op_config.get(\"reduce_key\")\n        if reduce_key and original_output:\n            if isinstance(reduce_key, list):\n                key = next(\n                    (\n                        tuple(item[k] for k in reduce_key)\n                        for item in original_output\n                        if all(k in item for k in reduce_key)\n                    ),\n                    tuple(None for _ in reduce_key),\n                )\n                sample_output = next(\n                    (\n                        item\n                        for item in original_output\n                        if all(item.get(k) == v for k, v in zip(reduce_key, key))\n                    ),\n                    {},\n                )\n            else:\n                key = next(\n                    (\n                        item[reduce_key]\n                        for item in original_output\n                        if reduce_key in item\n                    ),\n                    None,\n                )\n                sample_output = next(\n                    (item for item in original_output if item.get(reduce_key) == key),\n                    {},\n                )\n        else:\n            sample_output = original_output[0] if original_output else {}\n\n        output_keys = op_config.get(\"output\", {}).get(\"schema\", {})\n        sample_output = {k: sample_output[k] for k in output_keys}\n\n        prompt = f\"\"\"\n        Analyze the following reduce operation and its input/output:\n\n        Reduce Operation Prompt:\n        {op_config[\"prompt\"]}\n\n        Sample Input (just one item):\n        {json.dumps(sample_input, indent=2)}\n\n        Sample Output:\n        {json.dumps(sample_output, indent=2)}\n\n        Create a custom validator prompt that will assess how well the reduce operation performed its intended task. The prompt should ask specific 2-3 questions about the quality of the output, such as:\n        1. Does the output accurately reflect the aggregation method specified in the task? For example, if finding anomalies, are the identified anomalies actually anomalies?\n        2. Are there any missing fields, unexpected null values, or data type mismatches in the output compared to the expected schema?\n        3. Does the output maintain the key information from the input while appropriately condensing or summarizing it? For instance, in a text summarization task, are the main points preserved?\n        4. How well does the output adhere to any specific formatting requirements mentioned in the original prompt, such as character limits for summaries or specific data types for aggregated values?\n\n        Note that the output may reflect more than just the input provided, since we only provide a one-item sample input. Provide your response as a single string containing the custom validator prompt. The prompt should be tailored to the task and avoid generic criteria. The prompt should not reference a specific value in the sample input, but rather a general property.\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\"validator_prompt\": {\"type\": \"string\"}},\n            \"required\": [\"validator_prompt\"],\n        }\n\n        response = self.llm_client.generate(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        return json.loads(response.choices[0].message.content)[\"validator_prompt\"]\n\n    def _validate_reduce_output(\n        self,\n        op_config: Dict[str, Any],\n        validation_inputs: Dict[Any, List[Dict[str, Any]]],\n        output_data: List[Dict[str, Any]],\n        validator_prompt: str,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Validate the output of the reduce operation using the generated validator prompt.\n\n        This method assesses the quality of the reduce operation output by applying the validator prompt\n        to multiple samples of the input and output data.\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            validation_inputs (Dict[Any, List[Dict[str, Any]]]): Validation inputs for the reduce operation.\n            output_data (List[Dict[str, Any]]): Output data from the reduce operation.\n            validator_prompt (str): The validator prompt generated earlier.\n\n        Returns:\n            Dict[str, Any]: A dictionary containing validation results and a flag indicating if improvement is needed.\n        \"\"\"\n        system_prompt = \"You are an AI assistant tasked with validating the output of reduce operations in data processing pipelines.\"\n\n        validation_results = []\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = []\n            for reduce_key, inputs in validation_inputs.items():\n                if isinstance(op_config[\"reduce_key\"], list):\n                    sample_output = next(\n                        (\n                            item\n                            for item in output_data\n                            if all(\n                                item[key] == reduce_key[i]\n                                for i, key in enumerate(op_config[\"reduce_key\"])\n                            )\n                        ),\n                        None,\n                    )\n                else:\n                    sample_output = next(\n                        (\n                            item\n                            for item in output_data\n                            if item[op_config[\"reduce_key\"]] == reduce_key\n                        ),\n                        None,\n                    )\n\n                if sample_output is None:\n                    self.console.log(\n                        f\"Warning: No output found for reduce key {reduce_key}\"\n                    )\n                    continue\n\n                input_str = json.dumps(inputs, indent=2)\n                # truncate input_str to 40,000 words\n                input_str = input_str.split()[:40000]\n                input_str = \" \".join(input_str) + \"...\"\n\n                prompt = f\"\"\"{validator_prompt}\n\n                Reduce Operation Task:\n                {op_config[\"prompt\"]}\n\n                Input Data Samples:\n                {input_str}\n\n                Output Data Sample:\n                {json.dumps(sample_output, indent=2)}\n\n                Based on the validator prompt and the input/output samples, assess the quality (e.g., correctness, completeness) of the reduce operation output.\n                Provide your assessment in the following format:\n                \"\"\"\n\n                parameters = {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"is_valid\": {\"type\": \"boolean\"},\n                        \"issues\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"suggestions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                    },\n                    \"required\": [\"is_valid\", \"issues\", \"suggestions\"],\n                }\n\n                futures.append(\n                    executor.submit(\n                        self.llm_client.generate,\n                        [{\"role\": \"user\", \"content\": prompt}],\n                        system_prompt,\n                        parameters,\n                    )\n                )\n\n            for future, (reduce_key, inputs) in zip(futures, validation_inputs.items()):\n                response = future.result()\n                result = json.loads(response.choices[0].message.content)\n                validation_results.append(result)\n\n        # Determine if optimization is needed based on validation results\n        invalid_count = sum(\n            1 for result in validation_results if not result[\"is_valid\"]\n        )\n        needs_improvement = invalid_count &gt; 1\n\n        return {\n            \"needs_improvement\": needs_improvement,\n            \"validation_results\": validation_results,\n        }\n\n    def _create_validation_inputs(\n        self, input_data: List[Dict[str, Any]], reduce_key: Union[str, List[str]]\n    ) -&gt; Dict[Any, List[Dict[str, Any]]]:\n        # Group input data by reduce_key\n        grouped_data = {}\n        for item in input_data:\n            if isinstance(reduce_key, list):\n                key = tuple(item[k] for k in reduce_key)\n            else:\n                key = item[reduce_key]\n            if key not in grouped_data:\n                grouped_data[key] = []\n            grouped_data[key].append(item)\n\n        # Select a fixed number of reduce keys\n        selected_keys = random.sample(\n            list(grouped_data.keys()),\n            min(self.num_samples_in_validation, len(grouped_data)),\n        )\n\n        # Create a new dict with only the selected keys\n        validation_inputs = {key: grouped_data[key] for key in selected_keys}\n\n        return validation_inputs\n\n    def _create_reduce_plans(\n        self,\n        op_config: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        is_associative: bool,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Create multiple reduce plans based on the input data and operation configuration.\n\n        This method generates various reduce plans by varying batch sizes and fold prompts.\n        It takes into account the LLM's context window size to determine appropriate batch sizes.\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n            is_associative (bool): Flag indicating whether the reduce operation is associative.\n\n        Returns:\n            List[Dict[str, Any]]: A list of reduce plans, each with different batch sizes and fold prompts.\n        \"\"\"\n        model = op_config.get(\"model\", \"gpt-4o-mini\")\n        model_input_context_length = model_cost.get(model, {}).get(\n            \"max_input_tokens\", 8192\n        )\n\n        # Estimate tokens for prompt, input, and output\n        prompt_tokens = count_tokens(op_config[\"prompt\"], model)\n        sample_input = input_data[:100]\n        sample_output = self._run_operation(op_config, input_data[:100])\n\n        prompt_vars = extract_jinja_variables(op_config[\"prompt\"])\n        prompt_vars = [var.split(\".\")[-1] for var in prompt_vars]\n        avg_input_tokens = mean(\n            [\n                count_tokens(\n                    json.dumps({k: item[k] for k in prompt_vars if k in item}), model\n                )\n                for item in sample_input\n            ]\n        )\n        avg_output_tokens = mean(\n            [\n                count_tokens(\n                    json.dumps({k: item[k] for k in prompt_vars if k in item}), model\n                )\n                for item in sample_output\n            ]\n        )\n\n        # Calculate max batch size that fits in context window\n        max_batch_size = (\n            model_input_context_length - prompt_tokens - avg_output_tokens\n        ) // avg_input_tokens\n\n        # Generate 6 candidate batch sizes\n        batch_sizes = [\n            max(1, int(max_batch_size * ratio))\n            for ratio in [0.1, 0.2, 0.4, 0.6, 0.75, 0.9]\n        ]\n        # Log the generated batch sizes\n        self.console.log(\"[cyan]Generating plans for batch sizes:[/cyan]\")\n        for size in batch_sizes:\n            self.console.log(f\"  - {size}\")\n        batch_sizes = sorted(set(batch_sizes))  # Remove duplicates and sort\n\n        plans = []\n\n        # Generate multiple fold prompts\n        max_retries = 5\n        retry_count = 0\n        fold_prompts = []\n\n        while retry_count &lt; max_retries and not fold_prompts:\n            try:\n                fold_prompts = self._synthesize_fold_prompts(\n                    op_config,\n                    sample_input,\n                    sample_output,\n                    num_prompts=self.num_fold_prompts,\n                )\n                if not fold_prompts:\n                    raise ValueError(\"No fold prompts generated\")\n            except Exception as e:\n                retry_count += 1\n                if retry_count == max_retries:\n                    raise RuntimeError(\n                        f\"Failed to generate fold prompts after {max_retries} attempts: {str(e)}\"\n                    )\n                self.console.log(\n                    f\"Retry {retry_count}/{max_retries}: Failed to generate fold prompts. Retrying...\"\n                )\n\n        for batch_size in batch_sizes:\n            for fold_idx, fold_prompt in enumerate(fold_prompts):\n                plan = op_config.copy()\n                plan[\"fold_prompt\"] = fold_prompt\n                plan[\"fold_batch_size\"] = batch_size\n                plan[\"associative\"] = is_associative\n                plan[\"name\"] = f\"{op_config['name']}_bs_{batch_size}_fp_{fold_idx}\"\n                plans.append(plan)\n\n        return plans\n\n    def _calculate_compression_ratio(\n        self,\n        op_config: Dict[str, Any],\n        sample_input: List[Dict[str, Any]],\n        sample_output: List[Dict[str, Any]],\n    ) -&gt; float:\n        \"\"\"\n        Calculate the compression ratio of the reduce operation.\n\n        This method compares the size of the input data to the size of the output data\n        to determine how much the data is being compressed by the reduce operation.\n\n        Args:\n            op_config (Dict[str, Any]): Configuration for the reduce operation.\n            sample_input (List[Dict[str, Any]]): Sample input data.\n            sample_output (List[Dict[str, Any]]): Sample output data.\n\n        Returns:\n            float: The calculated compression ratio.\n        \"\"\"\n        reduce_key = op_config[\"reduce_key\"]\n        input_schema = op_config.get(\"input\", {}).get(\"schema\", {})\n        output_schema = op_config[\"output\"][\"schema\"]\n        model = op_config.get(\"model\", \"gpt-4o-mini\")\n\n        compression_ratios = {}\n\n        # Handle both single key and list of keys\n        if isinstance(reduce_key, list):\n            distinct_keys = set(\n                tuple(item[k] for k in reduce_key) for item in sample_input\n            )\n        else:\n            distinct_keys = set(item[reduce_key] for item in sample_input)\n\n        for key in distinct_keys:\n            if isinstance(reduce_key, list):\n                key_input = [\n                    item\n                    for item in sample_input\n                    if tuple(item[k] for k in reduce_key) == key\n                ]\n                key_output = [\n                    item\n                    for item in sample_output\n                    if tuple(item[k] for k in reduce_key) == key\n                ]\n            else:\n                key_input = [item for item in sample_input if item[reduce_key] == key]\n                key_output = [item for item in sample_output if item[reduce_key] == key]\n\n            if input_schema:\n                key_input_tokens = sum(\n                    count_tokens(\n                        json.dumps({k: item[k] for k in input_schema if k in item}),\n                        model,\n                    )\n                    for item in key_input\n                )\n            else:\n                key_input_tokens = sum(\n                    count_tokens(json.dumps(item), model) for item in key_input\n                )\n\n            key_output_tokens = sum(\n                count_tokens(\n                    json.dumps({k: item[k] for k in output_schema if k in item}), model\n                )\n                for item in key_output\n            )\n\n            compression_ratios[key] = (\n                key_output_tokens / key_input_tokens if key_input_tokens &gt; 0 else 1\n            )\n\n        if not compression_ratios:\n            return 1\n\n        # Calculate importance weights based on the number of items for each key\n        total_items = len(sample_input)\n        if isinstance(reduce_key, list):\n            importance_weights = {\n                key: len(\n                    [\n                        item\n                        for item in sample_input\n                        if tuple(item[k] for k in reduce_key) == key\n                    ]\n                )\n                / total_items\n                for key in compression_ratios\n            }\n        else:\n            importance_weights = {\n                key: len([item for item in sample_input if item[reduce_key] == key])\n                / total_items\n                for key in compression_ratios\n            }\n\n        # Calculate weighted average of compression ratios\n        weighted_sum = sum(\n            compression_ratios[key] * importance_weights[key]\n            for key in compression_ratios\n        )\n        return weighted_sum\n\n    def _synthesize_fold_prompts(\n        self,\n        op_config: Dict[str, Any],\n        sample_input: List[Dict[str, Any]],\n        sample_output: List[Dict[str, Any]],\n        num_prompts: int = 2,\n    ) -&gt; List[str]:\n        \"\"\"\n        Synthesize fold prompts for the reduce operation. We generate multiple\n        fold prompts in case one is bad.\n\n        A fold operation is a higher-order function that iterates through a data structure,\n        accumulating the results of applying a given combining operation to its elements.\n        In the context of reduce operations, folding allows processing of data in batches,\n        which can significantly improve performance for large datasets.\n\n        This method generates multiple fold prompts that can be used to optimize the reduce operation\n        by allowing it to run on batches of inputs. It uses the language model to create prompts\n        that are variations of the original reduce prompt, adapted for folding operations.\n\n        Args:\n            op_config (Dict[str, Any]): The configuration of the reduce operation.\n            sample_input (List[Dict[str, Any]]): A sample of the input data.\n            sample_output (List[Dict[str, Any]]): A sample of the output data.\n            num_prompts (int, optional): The number of fold prompts to generate. Defaults to 2.\n\n        Returns:\n            List[str]: A list of synthesized fold prompts.\n\n        The method performs the following steps:\n        1. Sets up the system prompt and parameters for the language model.\n        2. Defines a function to get random examples from the sample data.\n        3. Creates a prompt template for generating fold prompts.\n        4. Uses multi-threading to generate multiple fold prompts in parallel.\n        5. Returns the list of generated fold prompts.\n        \"\"\"\n        system_prompt = \"You are an AI assistant tasked with creating a fold prompt for reduce operations in data processing pipelines.\"\n        original_prompt = op_config[\"prompt\"]\n\n        input_schema = op_config.get(\"input\", {}).get(\"schema\", {})\n        output_schema = op_config[\"output\"][\"schema\"]\n        reduce_key = op_config[\"reduce_key\"]\n\n        def get_random_examples():\n            if isinstance(reduce_key, list):\n                random_key = tuple(\n                    random.choice(\n                        [\n                            tuple(item[k] for k in reduce_key if k in item)\n                            for item in sample_input\n                            if all(k in item for k in reduce_key)\n                        ]\n                    )\n                )\n                input_example = random.choice(\n                    [\n                        item\n                        for item in sample_input\n                        if all(item.get(k) == v for k, v in zip(reduce_key, random_key))\n                    ]\n                )\n                output_example = random.choice(\n                    [\n                        item\n                        for item in sample_output\n                        if all(item.get(k) == v for k, v in zip(reduce_key, random_key))\n                    ]\n                )\n            else:\n                random_key = random.choice(\n                    [item[reduce_key] for item in sample_input if reduce_key in item]\n                )\n                input_example = random.choice(\n                    [item for item in sample_input if item[reduce_key] == random_key]\n                )\n                output_example = random.choice(\n                    [item for item in sample_output if item[reduce_key] == random_key]\n                )\n\n            if input_schema:\n                input_example = {\n                    k: input_example[k] for k in input_schema if k in input_example\n                }\n            output_example = {\n                k: output_example[k] for k in output_schema if k in output_example\n            }\n            return input_example, output_example\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"fold_prompt\": {\n                    \"type\": \"string\",\n                }\n            },\n            \"required\": [\"fold_prompt\"],\n        }\n\n        def generate_single_prompt():\n            input_example, output_example = get_random_examples()\n            prompt = f\"\"\"\n            Original Reduce Operation Prompt:\n            {original_prompt}\n\n            Sample Input:\n            {json.dumps(input_example, indent=2)}\n\n            Sample Output:\n            {json.dumps(output_example, indent=2)}\n\n            Create a fold prompt for the reduce operation to run on batches of inputs. The fold prompt should:\n            1. Minimally modify the original reduce prompt\n            2. Describe how to combine the new values with the current reduced value\n            3. Be designed to work iteratively, allowing for multiple fold operations. The first iteration will use the original prompt, and all successive iterations will use the fold prompt.\n\n            The fold prompt should be a Jinja2 template with the following variables available:\n            - {{ output }}: The current reduced value (a dictionary with the current output schema)\n            - {{ inputs }}: A list of new values to be folded in\n            - {{ reduce_key }}: The key used for grouping in the reduce operation\n\n            Provide the fold prompt as a string.\n            \"\"\"\n            response = self.llm_client.generate(\n                [{\"role\": \"user\", \"content\": prompt}],\n                system_prompt,\n                parameters,\n            )\n            fold_prompt = json.loads(response.choices[0].message.content)[\"fold_prompt\"]\n\n            # Run the operation with the fold prompt\n            # Create a temporary plan with the fold prompt\n            temp_plan = op_config.copy()\n            temp_plan[\"fold_prompt\"] = fold_prompt\n            temp_plan[\"fold_batch_size\"] = min(\n                len(sample_input), 2\n            )  # Use a small batch size for testing\n\n            # Run the operation with the fold prompt\n            self._run_operation(temp_plan, sample_input[: temp_plan[\"fold_batch_size\"]])\n\n            # If the operation runs successfully, return the fold prompt\n            return fold_prompt\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            fold_prompts = list(\n                executor.map(lambda _: generate_single_prompt(), range(num_prompts))\n            )\n\n        return fold_prompts\n\n    def _evaluate_reduce_plans(\n        self,\n        op_config: Dict[str, Any],\n        plans: List[Dict[str, Any]],\n        input_data: List[Dict[str, Any]],\n        validator_prompt: str,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Evaluate multiple reduce plans and select the best one.\n\n        This method takes a list of reduce plans, evaluates each one using the input data\n        and a validator prompt, and selects the best plan based on the evaluation scores.\n        It also attempts to create and evaluate a merged plan that enhances the runtime performance\n        of the best plan.\n\n        A merged plan is an optimization technique applied to the best-performing plan\n        that uses the fold operation. It allows the best plan to run even faster by\n        executing parallel folds and then merging the results of these individual folds\n        together. We default to a merge batch size of 2, but one can increase this.\n\n        Args:\n            op_config (Dict[str, Any]): The configuration of the reduce operation.\n            plans (List[Dict[str, Any]]): A list of reduce plans to evaluate.\n            input_data (List[Dict[str, Any]]): The input data to use for evaluation.\n            validator_prompt (str): The prompt to use for validating the output of each plan.\n\n        Returns:\n            Dict[str, Any]: The best reduce plan, either the top-performing original plan\n                            or a merged plan if it performs well enough.\n\n        The method performs the following steps:\n        1. Evaluates each plan using multi-threading.\n        2. Sorts the plans based on their evaluation scores.\n        3. Selects the best plan and attempts to create a merged plan.\n        4. Evaluates the merged plan and compares it to the best original plan.\n        5. Returns either the merged plan or the best original plan based on their scores.\n        \"\"\"\n        self.console.log(\"\\n[bold]Evaluating Reduce Plans:[/bold]\")\n        for i, plan in enumerate(plans):\n            self.console.log(f\"Plan {i+1} (batch size: {plan['fold_batch_size']})\")\n\n        plan_scores = []\n        plan_outputs = {}\n\n        # Create a fixed random sample for evaluation\n        sample_size = min(100, len(input_data))\n        evaluation_sample = random.sample(input_data, sample_size)\n\n        # Create a fixed set of validation samples\n        validation_inputs = self._create_validation_inputs(\n            evaluation_sample, plan[\"reduce_key\"]\n        )\n\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(\n                    self._evaluate_single_plan,\n                    plan,\n                    evaluation_sample,\n                    validator_prompt,\n                    validation_inputs,\n                )\n                for plan in plans\n            ]\n            for future in as_completed(futures):\n                plan, score, output = future.result()\n                plan_scores.append((plan, score))\n                plan_outputs[id(plan)] = output\n\n        # Sort plans by score in descending order, then by fold_batch_size in descending order\n        sorted_plans = sorted(\n            plan_scores, key=lambda x: (x[1], x[0][\"fold_batch_size\"]), reverse=True\n        )\n\n        self.console.log(\"\\n[bold]Reduce Plan Scores:[/bold]\")\n        for i, (plan, score) in enumerate(sorted_plans):\n            self.console.log(\n                f\"Plan {i+1} (batch size: {plan['fold_batch_size']}): {score:.2f}\"\n            )\n\n        best_plan, best_score = sorted_plans[0]\n        self.console.log(\n            f\"\\n[green]Selected best plan with score: {best_score:.2f} and batch size: {best_plan['fold_batch_size']}[/green]\"\n        )\n\n        if op_config.get(\"synthesize_merge\", False):\n            # Create a new plan with merge prompt and updated parameters\n            merged_plan = best_plan.copy()\n\n            # Synthesize merge prompt if it doesn't exist\n            if \"merge_prompt\" not in merged_plan:\n                merged_plan[\"merge_prompt\"] = self._synthesize_merge_prompt(\n                    merged_plan, plan_outputs[id(best_plan)]\n                )\n                # Print the synthesized merge prompt\n                self.console.log(\"\\n[bold]Synthesized Merge Prompt:[/bold]\")\n                self.console.log(merged_plan[\"merge_prompt\"])\n\n            # Set merge_batch_size to 2 and num_parallel_folds to 5\n            merged_plan[\"merge_batch_size\"] = 2\n\n            # Evaluate the merged plan\n            _, merged_plan_score, _, operation_instance = self._evaluate_single_plan(\n                merged_plan,\n                evaluation_sample,\n                validator_prompt,\n                validation_inputs,\n                return_instance=True,\n            )\n\n            # Get the merge and fold times from the operation instance\n            merge_times = operation_instance.merge_times\n            fold_times = operation_instance.fold_times\n            merge_avg_time = mean(merge_times) if merge_times else None\n            fold_avg_time = mean(fold_times) if fold_times else None\n\n            self.console.log(\"\\n[bold]Scores:[/bold]\")\n            self.console.log(f\"Original plan: {best_score:.2f}\")\n            self.console.log(f\"Merged plan: {merged_plan_score:.2f}\")\n\n            # Compare scores and decide which plan to use\n            if merged_plan_score &gt;= best_score * 0.75:\n                self.console.log(\n                    f\"\\n[green]Using merged plan with score: {merged_plan_score:.2f}[/green]\"\n                )\n                if merge_avg_time and fold_avg_time:\n                    merged_plan[\"merge_time\"] = merge_avg_time\n                    merged_plan[\"fold_time\"] = fold_avg_time\n                return merged_plan\n            else:\n                self.console.log(\n                    f\"\\n[yellow]Merged plan quality too low. Using original plan with score: {best_score:.2f}[/yellow]\"\n                )\n                return best_plan\n        else:\n            return best_plan\n\n    def _evaluate_single_plan(\n        self,\n        plan: Dict[str, Any],\n        input_data: List[Dict[str, Any]],\n        validator_prompt: str,\n        validation_inputs: List[Dict[str, Any]],\n        return_instance: bool = False,\n    ) -&gt; Union[\n        Tuple[Dict[str, Any], float, List[Dict[str, Any]]],\n        Tuple[Dict[str, Any], float, List[Dict[str, Any]], BaseOperation],\n    ]:\n        \"\"\"\n        Evaluate a single reduce plan using the provided input data and validator prompt.\n\n        This method runs the reduce operation with the given plan, validates the output,\n        and calculates a score based on the validation results. The scoring works as follows:\n        1. It counts the number of valid results from the validation.\n        2. The score is calculated as the ratio of valid results to the total number of validation results.\n        3. This produces a score between 0 and 1, where 1 indicates all results were valid, and 0 indicates none were valid.\n\n        TODO: We should come up with a better scoring method here, maybe pairwise comparisons.\n\n        Args:\n            plan (Dict[str, Any]): The reduce plan to evaluate.\n            input_data (List[Dict[str, Any]]): The input data to use for evaluation.\n            validator_prompt (str): The prompt to use for validating the output.\n            return_instance (bool, optional): Whether to return the operation instance. Defaults to False.\n\n        Returns:\n            Union[\n                Tuple[Dict[str, Any], float, List[Dict[str, Any]]],\n                Tuple[Dict[str, Any], float, List[Dict[str, Any]], BaseOperation],\n            ]: A tuple containing the plan, its score, the output data, and optionally the operation instance.\n\n        The method performs the following steps:\n        1. Runs the reduce operation with the given plan on the input data.\n        2. Validates the output using the validator prompt.\n        3. Calculates a score based on the validation results.\n        4. Returns the plan, score, output data, and optionally the operation instance.\n        \"\"\"\n        output = self._run_operation(plan, input_data, return_instance)\n        if return_instance:\n            output, operation_instance = output\n\n        validation_result = self._validate_reduce_output(\n            plan, validation_inputs, output, validator_prompt\n        )\n\n        # Calculate a score based on validation results\n        valid_count = sum(\n            1\n            for result in validation_result[\"validation_results\"]\n            if result[\"is_valid\"]\n        )\n        score = valid_count / len(validation_result[\"validation_results\"])\n\n        if return_instance:\n            return plan, score, output, operation_instance\n        else:\n            return plan, score, output\n\n    def _synthesize_merge_prompt(\n        self, plan: Dict[str, Any], sample_outputs: List[Dict[str, Any]]\n    ) -&gt; str:\n        \"\"\"\n        Synthesize a merge prompt for combining multiple folded outputs in a reduce operation.\n\n        This method generates a merge prompt that can be used to combine the results of multiple\n        parallel fold operations into a single output. It uses the language model to create a prompt\n        that is consistent with the original reduce and fold prompts while addressing the specific\n        requirements of merging multiple outputs.\n\n        Args:\n            plan (Dict[str, Any]): The reduce plan containing the original prompt and fold prompt.\n            sample_outputs (List[Dict[str, Any]]): Sample outputs from the fold operation to use as examples.\n\n        Returns:\n            str: The synthesized merge prompt as a string.\n\n        The method performs the following steps:\n        1. Sets up the system prompt for the language model.\n        2. Prepares a random sample output to use as an example.\n        3. Creates a detailed prompt for the language model, including the original reduce prompt,\n           fold prompt, sample output, and instructions for creating the merge prompt.\n        4. Uses the language model to generate the merge prompt.\n        5. Returns the generated merge prompt.\n        \"\"\"\n        system_prompt = \"You are an AI assistant tasked with creating a merge prompt for reduce operations in data processing pipelines. The pipeline has a reduce operation, and incrementally folds inputs into a single output. We want to optimize the pipeline for speed by running multiple folds on different inputs in parallel, and then merging the fold outputs into a single output.\"\n\n        output_schema = plan[\"output\"][\"schema\"]\n        random_output = random.choice(sample_outputs)\n        random_output = {\n            k: random_output[k] for k in output_schema if k in random_output\n        }\n\n        prompt = f\"\"\"Reduce Operation Prompt (runs on the first batch of inputs):\n        {plan[\"prompt\"]}\n\n        Fold Prompt (runs on the second and subsequent batches of inputs):\n        {plan[\"fold_prompt\"]}\n\n        Sample output of the fold operation (an input to the merge operation):\n        {json.dumps(random_output, indent=2)}\n\n        Create a merge prompt for the reduce operation to combine 2+ folded outputs. The merge prompt should:\n        1. Give context on the task &amp; fold operations, describing that the prompt will be used to combine multiple outputs from the fold operation (as if the original prompt was run on all inputs at once)\n        2. Describe how to combine multiple folded outputs into a single output\n        3. Minimally deviate from the reduce and fold prompts\n\n        The merge prompt should be a Jinja2 template with the following variables available:\n        - {{ outputs }}: A list of reduced outputs to be merged (each following the output schema). You can access the first output with {{ outputs[0] }} and the second with {{ outputs[1] }}\n\n        Output Schema:\n        {json.dumps(output_schema, indent=2)}\n\n        Provide the merge prompt as a string.\n        \"\"\"\n\n        parameters = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"merge_prompt\": {\n                    \"type\": \"string\",\n                }\n            },\n            \"required\": [\"merge_prompt\"],\n        }\n\n        response = self.llm_client.generate(\n            [{\"role\": \"user\", \"content\": prompt}],\n            system_prompt,\n            parameters,\n        )\n        return json.loads(response.choices[0].message.content)[\"merge_prompt\"]\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.reduce_optimizer.ReduceOptimizer.__init__","title":"<code>__init__(runner, config, console, llm_client, max_threads, run_operation, num_fold_prompts=1, num_samples_in_validation=10, status=None)</code>","text":"<p>Initialize the ReduceOptimizer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Configuration dictionary for the optimizer.</p> required <code>console</code> <code>Console</code> <p>Rich console object for pretty printing.</p> required <code>llm_client</code> <code>LLMClient</code> <p>Client for interacting with a language model.</p> required <code>max_threads</code> <code>int</code> <p>Maximum number of threads to use for parallel processing.</p> required <code>run_operation</code> <code>Callable</code> <p>Function to run an operation.</p> required <code>num_fold_prompts</code> <code>int</code> <p>Number of fold prompts to generate. Defaults to 1.</p> <code>1</code> <code>num_samples_in_validation</code> <code>int</code> <p>Number of samples to use in validation. Defaults to 10.</p> <code>10</code> Source code in <code>docetl/optimizers/reduce_optimizer.py</code> <pre><code>def __init__(\n    self,\n    runner,\n    config: Dict[str, Any],\n    console: Console,\n    llm_client: LLMClient,\n    max_threads: int,\n    run_operation: Callable,\n    num_fold_prompts: int = 1,\n    num_samples_in_validation: int = 10,\n    status: Optional[Status] = None,\n):\n    \"\"\"\n    Initialize the ReduceOptimizer.\n\n    Args:\n        config (Dict[str, Any]): Configuration dictionary for the optimizer.\n        console (Console): Rich console object for pretty printing.\n        llm_client (LLMClient): Client for interacting with a language model.\n        max_threads (int): Maximum number of threads to use for parallel processing.\n        run_operation (Callable): Function to run an operation.\n        num_fold_prompts (int, optional): Number of fold prompts to generate. Defaults to 1.\n        num_samples_in_validation (int, optional): Number of samples to use in validation. Defaults to 10.\n    \"\"\"\n    self.runner = runner\n    self.config = config\n    self.console = console\n    self.llm_client = llm_client\n    self._run_operation = run_operation\n    self.max_threads = max_threads\n    self.num_fold_prompts = num_fold_prompts\n    self.num_samples_in_validation = num_samples_in_validation\n    self.status = status\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.reduce_optimizer.ReduceOptimizer.optimize","title":"<code>optimize(op_config, input_data, level=1)</code>","text":"<p>Optimize the reduce operation based on the given configuration and input data.</p> <p>This method performs the following steps: 1. Run the original operation 2. Generate a validator prompt 3. Validate the output 4. If improvement is needed:    a. Evaluate if decomposition is beneficial    b. If decomposition is beneficial, recursively optimize each sub-operation    c. If not, proceed with single operation optimization 5. Run the optimized operation(s)</p> <p>Parameters:</p> Name Type Description Default <code>op_config</code> <code>Dict[str, Any]</code> <p>Configuration for the reduce operation.</p> required <code>input_data</code> <code>List[Dict[str, Any]]</code> <p>Input data for the reduce operation.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]: A tuple containing the list of optimized configurations</p> <code>List[Dict[str, Any]]</code> <p>and the list of outputs from the optimized operation(s), and the cost of the operation due to synthesizing any resolve operations.</p> Source code in <code>docetl/optimizers/reduce_optimizer.py</code> <pre><code>def optimize(\n    self,\n    op_config: Dict[str, Any],\n    input_data: List[Dict[str, Any]],\n    level: int = 1,\n) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]:\n    \"\"\"\n    Optimize the reduce operation based on the given configuration and input data.\n\n    This method performs the following steps:\n    1. Run the original operation\n    2. Generate a validator prompt\n    3. Validate the output\n    4. If improvement is needed:\n       a. Evaluate if decomposition is beneficial\n       b. If decomposition is beneficial, recursively optimize each sub-operation\n       c. If not, proceed with single operation optimization\n    5. Run the optimized operation(s)\n\n    Args:\n        op_config (Dict[str, Any]): Configuration for the reduce operation.\n        input_data (List[Dict[str, Any]]): Input data for the reduce operation.\n\n    Returns:\n        Tuple[List[Dict[str, Any]], List[Dict[str, Any]], float]: A tuple containing the list of optimized configurations\n        and the list of outputs from the optimized operation(s), and the cost of the operation due to synthesizing any resolve operations.\n    \"\"\"\n    # Check if we're running out of token limits for the reduce prompt\n    model = op_config.get(\"model\", self.config.get(\"default_model\", \"gpt-4o-mini\"))\n    model_input_context_length = model_cost.get(model, {}).get(\n        \"max_input_tokens\", 4096\n    )\n\n    # Find the key with the longest value\n    longest_key = max(\n        op_config[\"reduce_key\"], key=lambda k: len(str(input_data[0][k]))\n    )\n    sample_key = tuple(\n        input_data[0][k] if k == longest_key else input_data[0][k]\n        for k in op_config[\"reduce_key\"]\n    )\n\n    # Render the prompt with a sample input\n    prompt_template = Template(op_config[\"prompt\"])\n    sample_prompt = prompt_template.render(\n        reduce_key=dict(zip(op_config[\"reduce_key\"], sample_key)),\n        inputs=[input_data[0]],\n    )\n\n    # Count tokens in the sample prompt\n    prompt_tokens = count_tokens(sample_prompt, model)\n\n    add_map_op = False\n    if prompt_tokens * 2 &gt; model_input_context_length:\n        add_map_op = True\n        self.console.log(\n            f\"[yellow]Warning: The reduce prompt exceeds the token limit for model {model}. \"\n            f\"Token count: {prompt_tokens}, Limit: {model_input_context_length}. \"\n            f\"Add a map operation to the pipeline.[/yellow]\"\n        )\n\n    # # Also query an agent to look at a sample of the inputs and see if they think a map operation would be helpful\n    # preprocessing_steps = \"\"\n    # should_use_map, preprocessing_steps = self._should_use_map(\n    #     op_config, input_data\n    # )\n    # if should_use_map or add_map_op:\n    #     # Synthesize a map operation\n    #     map_prompt, map_output_schema = self._synthesize_map_operation(\n    #         op_config, preprocessing_steps, input_data\n    #     )\n    #     # Change the reduce operation prompt to use the map schema\n    #     new_reduce_prompt = self._change_reduce_prompt_to_use_map_schema(\n    #         op_config[\"prompt\"], map_output_schema\n    #     )\n    #     op_config[\"prompt\"] = new_reduce_prompt\n\n    #     # Return unoptimized map and reduce operations\n    #     return [map_prompt, op_config], input_data, 0.0\n\n    original_output = self._run_operation(op_config, input_data)\n\n    # Step 1: Synthesize a validator prompt\n    validator_prompt = self._generate_validator_prompt(\n        op_config, input_data, original_output\n    )\n\n    # Log the validator prompt\n    self.console.log(\"[bold]Validator Prompt:[/bold]\")\n    self.console.log(validator_prompt)\n    self.console.log(\"\\n\")  # Add a newline for better readability\n\n    # Step 2: validate the output\n    validator_inputs = self._create_validation_inputs(\n        input_data, op_config[\"reduce_key\"]\n    )\n    validation_results = self._validate_reduce_output(\n        op_config, validator_inputs, original_output, validator_prompt\n    )\n\n    # Print the validation results\n    self.console.log(\"[bold]Validation Results on Initial Sample:[/bold]\")\n    if validation_results[\"needs_improvement\"]:\n        self.console.log(\n            \"\\n\".join(\n                [\n                    f\"Issues: {result['issues']} Suggestions: {result['suggestions']}\"\n                    for result in validation_results[\"validation_results\"]\n                ]\n            )\n        )\n\n        # Step 3: Evaluate if decomposition is beneficial\n        decomposition_result = self._evaluate_decomposition(\n            op_config, input_data, level\n        )\n\n        if decomposition_result[\"should_decompose\"]:\n            return self._optimize_decomposed_reduce(\n                decomposition_result, op_config, input_data, level\n            )\n\n        return self._optimize_single_reduce(op_config, input_data, validator_prompt)\n    else:\n        self.console.log(\"No improvements identified.\")\n        return [op_config], original_output, 0.0\n</code></pre>"},{"location":"api-reference/optimizers/#docetl.optimizers.join_optimizer.JoinOptimizer","title":"<code>docetl.optimizers.join_optimizer.JoinOptimizer</code>","text":"Source code in <code>docetl/optimizers/join_optimizer.py</code> <pre><code>class JoinOptimizer:\n    def __init__(\n        self,\n        runner,\n        config: Dict[str, Any],\n        op_config: Dict[str, Any],\n        console: Console,\n        llm_client: Any,\n        max_threads: int,\n        target_recall: float = 0.95,\n        sample_size: int = 500,\n        sampling_weight: float = 20,\n        agent_max_retries: int = 5,\n        estimated_selectivity: float = None,\n        status: Status = None,\n    ):\n        self.runner = runner\n        self.config = config\n        self.op_config = op_config\n        self.llm_client = llm_client\n        self.max_threads = max_threads\n        self.console = console\n        self.target_recall = target_recall\n        self.sample_size = sample_size\n        self.sampling_weight = sampling_weight\n        self.agent_max_retries = agent_max_retries\n        self.estimated_selectivity = estimated_selectivity\n        self.console.log(f\"Target Recall: {self.target_recall}\")\n        self.status = status\n        # if self.estimated_selectivity is not None:\n        #     self.console.log(\n        #         f\"[yellow]Using estimated selectivity of {self.estimated_selectivity}[/yellow]\"\n        #     )\n\n    def _analyze_map_prompt_categorization(self, map_prompt: str) -&gt; bool:\n        \"\"\"\n        Analyze the map prompt to determine if it's explicitly categorical.\n\n        Args:\n            map_prompt (str): The map prompt to analyze.\n\n        Returns:\n            bool: True if the prompt is explicitly categorical, False otherwise.\n        \"\"\"\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an AI assistant tasked with analyzing prompts for data processing operations.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze the following map operation prompt and determine if it is explicitly categorical,\n                meaning it details a specific set of possible outputs:\n\n                {map_prompt}\n\n                Respond with 'Yes' if the prompt is explicitly categorical, detailing a finite set of possible outputs.\n                Respond with 'No' if the prompt allows for open-ended or non-categorical responses.\n                Provide a brief explanation for your decision.\"\"\",\n            },\n        ]\n\n        response = self.llm_client.generate(\n            messages,\n            \"You are an expert in analyzing natural language prompts for data processing tasks.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"is_categorical\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"Yes\", \"No\"],\n                        \"description\": \"Whether the prompt is explicitly categorical\",\n                    },\n                    \"explanation\": {\n                        \"type\": \"string\",\n                        \"description\": \"Brief explanation for the decision\",\n                    },\n                },\n                \"required\": [\"is_categorical\", \"explanation\"],\n            },\n        )\n\n        analysis = json.loads(response.choices[0].message.content)\n\n        self.console.log(\"[bold]Map Prompt Analysis:[/bold]\")\n        self.console.log(f\"Is Categorical: {analysis['is_categorical']}\")\n        self.console.log(f\"Explanation: {analysis['explanation']}\")\n\n        return analysis[\"is_categorical\"].lower() == \"yes\"\n\n    def _determine_duplicate_keys(\n        self,\n        input_data: List[Dict[str, Any]],\n        reduce_key: List[str],\n        map_prompt: Optional[str] = None,\n    ) -&gt; bool:\n        # Prepare a sample of the input data for analysis\n        sample_size = min(10, len(input_data))\n        data_sample = random.sample(\n            [{rk: item[rk] for rk in reduce_key} for item in input_data], sample_size\n        )\n\n        context_prefix = \"\"\n        if map_prompt:\n            context_prefix = f\"For context, these values came out of a pipeline with the following prompt:\\n\\n{map_prompt}\\n\\n\"\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"{context_prefix}I want to do a reduce operation on these values, and I need to determine if there are semantic duplicates in the data, where the strings are different but they technically belong in the same group. Note that exact string duplicates should not be considered here.\\n\\nHere's a sample of the data (showing the '{reduce_key}' field(s)): {data_sample}\\n\\nBased on this {'context and ' if map_prompt else ''}sample, are there likely to be such semantic duplicates (not exact string matches) in the dataset? Respond with 'yes' only if you think there are semantic duplicates, or 'no' if you don't see evidence of semantic duplicates or if you only see exact string duplicates.\",\n            },\n        ]\n        response = self.llm_client.generate(\n            messages,\n            \"You are an expert data analyst. Analyze the given data sample and determine if there are likely to be semantic duplicate values that belong in the same group, even if the strings are different.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"likely_duplicates\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"Yes\", \"No\"],\n                        \"description\": \"Whether duplicates are likely to exist in the full dataset\",\n                    },\n                    \"explanation\": {\n                        \"type\": \"string\",\n                        \"description\": \"Brief explanation for the decision\",\n                    },\n                },\n                \"required\": [\"likely_duplicates\", \"explanation\"],\n            },\n        )\n\n        analysis = json.loads(response.choices[0].message.content)\n\n        self.console.log(f\"[bold]Duplicate Analysis for '{reduce_key}':[/bold]\")\n        self.console.log(f\"Likely Duplicates: {analysis['likely_duplicates']}\")\n        self.console.log(f\"Explanation: {analysis['explanation']}\")\n\n        if analysis[\"likely_duplicates\"].lower() == \"yes\":\n            self.console.log(\n                \"[yellow]Duplicates are likely. Consider using a deduplication strategy in the resolution step.[/yellow]\"\n            )\n            return True\n        return False\n\n    def _sample_random_pairs(\n        self, input_data: List[Dict[str, Any]], n: int\n    ) -&gt; List[Tuple[int, int]]:\n        \"\"\"Sample random pairs of indices, excluding exact matches.\"\"\"\n        pairs = set()\n        max_attempts = n * 10  # Avoid infinite loop\n        attempts = 0\n\n        while len(pairs) &lt; n and attempts &lt; max_attempts:\n            i, j = random.sample(range(len(input_data)), 2)\n            if i != j and input_data[i] != input_data[j]:\n                pairs.add((min(i, j), max(i, j)))  # Ensure ordered pairs\n            attempts += 1\n\n        return list(pairs)\n\n    def _check_duplicates_with_llm(\n        self,\n        input_data: List[Dict[str, Any]],\n        pairs: List[Tuple[int, int]],\n        reduce_key: List[str],\n        map_prompt: Optional[str],\n    ) -&gt; bool:\n        \"\"\"Use LLM to check if any pairs are duplicates.\"\"\"\n\n        content = \"Analyze the following pairs of entries and determine if any of them are likely duplicates. Respond with 'Yes' if you find any likely duplicates, or 'No' if none of the pairs seem to be duplicates. Provide a brief explanation for your decision.\\n\\n\"\n\n        if map_prompt:\n            content = (\n                f\"For reference, here is the map prompt used earlier in the pipeline: {map_prompt}\\n\\n\"\n                + content\n            )\n\n        for i, (idx1, idx2) in enumerate(pairs, 1):\n            content += f\"Pair {i}:\\n\"\n            content += \"Entry 1:\\n\"\n            for key in reduce_key:\n                content += f\"{key}: {json.dumps(input_data[idx1][key], indent=2)}\\n\"\n            content += \"\\nEntry 2:\\n\"\n            for key in reduce_key:\n                content += f\"{key}: {json.dumps(input_data[idx2][key], indent=2)}\\n\"\n            content += \"\\n\"\n\n        messages = [{\"role\": \"user\", \"content\": content}]\n\n        system_prompt = \"You are an AI assistant tasked with identifying potential duplicate entries in a dataset.\"\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"duplicates_found\": {\"type\": \"string\", \"enum\": [\"Yes\", \"No\"]},\n                \"explanation\": {\"type\": \"string\"},\n            },\n            \"required\": [\"duplicates_found\", \"explanation\"],\n        }\n\n        response = self.llm_client.generate(messages, system_prompt, response_schema)\n\n        # Print the duplicates_found and explanation\n        self.console.log(\n            f\"[bold]Duplicates in keys found:[/bold] {response['duplicates_found']}\\n\"\n            f\"[bold]Explanation:[/bold] {response['explanation']}\"\n        )\n\n        return response[\"duplicates_found\"].lower() == \"yes\"\n\n    def synthesize_compare_prompt(\n        self, map_prompt: Optional[str], reduce_key: List[str]\n    ) -&gt; str:\n\n        system_prompt = f\"You are an AI assistant tasked with creating a comparison prompt for LLM-assisted entity resolution. Your task is to create a comparison prompt that will be used to compare two entities, referred to as input1 and input2, to see if they are likely the same entity based on the following reduce key(s): {', '.join(reduce_key)}.\"\n        if map_prompt:\n            system_prompt += f\"\\n\\nFor context, here is the prompt used earlier in the pipeline to create the inputs to resolve: {map_prompt}\"\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n    Create a comparison prompt for entity resolution: The prompt should:\n    1. Be tailored to the specific domain and type of data being compared ({reduce_key}), based on the context provided.\n    2. Instruct to compare two entities, referred to as input1 and input2.\n    3. Specifically mention comparing each reduce key in input1 and input2 (e.g., input1.{{key}} and input2.{{key}} for each key in {reduce_key}). You can reference other fields in the input as well, as long as they are short.\n    4. Include instructions to consider relevant attributes or characteristics for comparison.\n    5. Ask to respond with \"True\" if the entities are likely the same, or \"False\" if they are likely different.\n\n    Example structure:\n    ```\n    Compare the following two {reduce_key} from [entity or document type]:\n\n    [Entity 1]:\n    {{{{ input1.key1 }}}}\n    {{{{ input1.optional_key2 }}}}\n\n    [Entity 2]:\n    {{{{ input2.key1 }}}}\n    {{{{ input2.optional_key2 }}}}\n\n    Are these [entities] likely referring to the same [entity type]? Consider [list relevant attributes or characteristics to compare]. Respond with \"True\" if they are likely the same [entity type], or \"False\" if they are likely different [entity types].\n    ```\n\n    Please generate the comparison prompt:\n    \"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate(\n            messages,\n            system_prompt,\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"comparison_prompt\": {\n                        \"type\": \"string\",\n                        \"description\": \"Detailed comparison prompt for entity resolution\",\n                    }\n                },\n                \"required\": [\"comparison_prompt\"],\n            },\n        )\n\n        comparison_prompt = json.loads(response.choices[0].message.content)[\n            \"comparison_prompt\"\n        ]\n\n        # Log the synthesized comparison prompt\n        self.console.log(\"[green]Synthesized comparison prompt:[/green]\")\n        self.console.log(comparison_prompt)\n\n        if not comparison_prompt:\n            raise ValueError(\n                \"Could not synthesize a comparison prompt. Please provide a comparison prompt in the config.\"\n            )\n\n        return comparison_prompt\n\n    def synthesize_resolution_prompt(\n        self,\n        map_prompt: Optional[str],\n        reduce_key: List[str],\n        output_schema: Dict[str, str],\n    ) -&gt; str:\n        system_prompt = f\"\"\"You are an AI assistant tasked with creating a resolution prompt for LLM-assisted entity resolution.\n        Your task is to create a prompt that will be used to merge multiple duplicate keys into a single, consolidated key.\n        The key(s) being resolved (known as the reduce_key) are {', '.join(reduce_key)}.\n        The duplicate keys will be provided in a list called 'inputs' in a Jinja2 template.\n        \"\"\"\n\n        if map_prompt:\n            system_prompt += f\"\\n\\nFor context, here is the prompt used earlier in the pipeline to create the inputs to resolve: {map_prompt}\"\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n    Create a resolution prompt for merging duplicate keys into a single key. The prompt should:\n    1. Be tailored to the specific domain and type of data being merged, based on the context provided.\n    2. Use a Jinja2 template to iterate over the duplicate keys (accessed as 'inputs', where each item is a dictionary containing the reduce_key fields, which you can access as entry.reduce_key for each reduce_key in {reduce_key}).\n    3. Instruct to create a single, consolidated key from the duplicate keys.\n    4. Include guidelines for resolving conflicts (e.g., choosing the most recent, most complete, or most reliable information).\n    5. Specify that the output of the resolution prompt should conform to the given output schema: {json.dumps(output_schema, indent=2)}\n\n    Example structure:\n    ```\n    Analyze the following duplicate entries for the {reduce_key} key:\n\n    {{% for key in inputs %}}\n    Entry {{{{ loop.index }}}}:\n    {{ % for key in reduce_key %}}\n    {{{{ key }}}}: {{{{ key[reduce_key] }}}}\n    {{% endfor %}}\n\n    {{% endfor %}}\n\n    Merge these into a single key.\n    When merging, follow these guidelines:\n    1. [Provide specific merging instructions relevant to the data type]\n    2. [Do not make the prompt too long]\n\n    Ensure that the merged key conforms to the following schema:\n    {json.dumps(output_schema, indent=2)}\n\n    Return the consolidated key as a single [appropriate data type] value.\n    ```\n\n    Please generate the resolution prompt:\n    \"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate(\n            messages,\n            system_prompt,\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"resolution_prompt\": {\n                        \"type\": \"string\",\n                        \"description\": \"Detailed resolution prompt for merging duplicate keys\",\n                    }\n                },\n                \"required\": [\"resolution_prompt\"],\n            },\n        )\n\n        resolution_prompt = json.loads(response.choices[0].message.content)[\n            \"resolution_prompt\"\n        ]\n\n        # Log the synthesized resolution prompt\n        self.console.log(\"[green]Synthesized resolution prompt:[/green]\")\n        self.console.log(resolution_prompt)\n\n        if not resolution_prompt:\n            raise ValueError(\n                \"Could not synthesize a resolution prompt. Please provide a resolution prompt in the config.\"\n            )\n\n        return resolution_prompt\n\n    def optimize_resolve(\n        self, input_data: List[Dict[str, Any]]\n    ) -&gt; Tuple[Dict[str, Any], float]:\n\n        # Check if the operation is marked as empty\n        if self.op_config.get(\"empty\", False):\n            # Extract the map prompt from the intermediates\n            map_prompt = self.op_config[\"_intermediates\"][\"map_prompt\"]\n            reduce_key = self.op_config[\"_intermediates\"][\"reduce_key\"]\n\n            if reduce_key is None:\n                raise ValueError(\n                    \"[yellow]Warning: No reduce key found in intermediates for synthesized resolve operation.[/yellow]\"\n                )\n\n            dedup = True\n\n            if map_prompt:\n                # Analyze the map prompt\n                analysis = self._analyze_map_prompt_categorization(map_prompt)\n\n                if analysis:\n                    dedup = False\n            else:\n                self.console.log(\n                    \"[yellow]No map prompt found in intermediates for analysis.[/yellow]\"\n                )\n\n            # TODO: figure out why this would ever be the case\n            if not map_prompt:\n                map_prompt = \"N/A\"\n\n            if dedup is False:\n                dedup = self._determine_duplicate_keys(\n                    input_data, reduce_key, map_prompt\n                )\n\n            # Now do the last attempt of pairwise comparisons\n            if dedup is False:\n                # Sample up to 20 random pairs of keys for duplicate analysis\n                sampled_pairs = self._sample_random_pairs(input_data, 20)\n\n                # Use LLM to check for duplicates\n                duplicates_found = self._check_duplicates_with_llm(\n                    input_data, sampled_pairs, reduce_key, map_prompt\n                )\n\n                if duplicates_found:\n                    dedup = True\n\n            if dedup is False:\n                # If no deduplication is needed, return the same config with 0 cost\n                return self.op_config, 0.0\n\n            # Add the reduce key to the output schema in the config\n            self.op_config[\"output\"] = {\"schema\": {rk: \"string\" for rk in reduce_key}}\n            for attempt in range(2):  # Try up to 2 times\n                self.op_config[\"comparison_prompt\"] = self.synthesize_compare_prompt(\n                    map_prompt, reduce_key\n                )\n                if (\n                    \"input1\" in self.op_config[\"comparison_prompt\"]\n                    and \"input2\" in self.op_config[\"comparison_prompt\"]\n                ):\n                    break\n                elif attempt == 0:\n                    self.console.log(\n                        \"[yellow]Warning: 'input1' or 'input2' not found in comparison prompt. Retrying...[/yellow]\"\n                    )\n            if (\n                \"input1\" not in self.op_config[\"comparison_prompt\"]\n                or \"input2\" not in self.op_config[\"comparison_prompt\"]\n            ):\n                self.console.log(\n                    \"[red]Error: Failed to generate comparison prompt with 'input1' and 'input2'. Using last generated prompt.[/red]\"\n                )\n            for attempt in range(2):  # Try up to 2 times\n                self.op_config[\"resolution_prompt\"] = self.synthesize_resolution_prompt(\n                    map_prompt, reduce_key, self.op_config[\"output\"][\"schema\"]\n                )\n                if \"inputs\" in self.op_config[\"resolution_prompt\"]:\n                    break\n                elif attempt == 0:\n                    self.console.log(\n                        \"[yellow]Warning: 'inputs' not found in resolution prompt. Retrying...[/yellow]\"\n                    )\n            if \"inputs\" not in self.op_config[\"resolution_prompt\"]:\n                self.console.log(\n                    \"[red]Error: Failed to generate resolution prompt with 'inputs'. Using last generated prompt.[/red]\"\n                )\n\n            # Pop off the empty flag\n            self.op_config.pop(\"empty\")\n\n        embeddings, blocking_keys, embedding_cost = self._compute_embeddings(input_data)\n        self.console.log(\n            f\"[bold]Cost of creating embeddings on the sample: ${embedding_cost:.4f}[/bold]\"\n        )\n\n        similarities = self._calculate_cosine_similarities(embeddings)\n\n        sampled_pairs = self._sample_pairs(similarities)\n        comparison_results, comparison_cost = self._perform_comparisons_resolve(\n            input_data, sampled_pairs\n        )\n\n        self._print_similarity_histogram(similarities, comparison_results)\n\n        threshold, estimated_selectivity = self._find_optimal_threshold(\n            comparison_results, similarities\n        )\n\n        blocking_rules = self._generate_blocking_rules(\n            blocking_keys, input_data, comparison_results\n        )\n\n        if blocking_rules:\n            false_negatives, rule_selectivity = self._verify_blocking_rule(\n                input_data,\n                blocking_rules[0],\n                blocking_keys,\n                comparison_results,\n            )\n            if not false_negatives and rule_selectivity &lt;= estimated_selectivity:\n                self.console.log(\n                    \"[green]Blocking rule verified. No false negatives detected in the sample and selectivity is within estimated selectivity.[/green]\"\n                )\n            else:\n                if false_negatives:\n                    self.console.log(\n                        f\"[red]Blocking rule rejected. {len(false_negatives)} false negatives detected in the sample.[/red]\"\n                    )\n                    for i, j in false_negatives[:5]:  # Show up to 5 examples\n                        self.console.log(\n                            f\"  Filtered pair: {{ {blocking_keys[0]}: {input_data[i][blocking_keys[0]]} }} and {{ {blocking_keys[0]}: {input_data[j][blocking_keys[0]]} }}\"\n                        )\n                    if len(false_negatives) &gt; 5:\n                        self.console.log(f\"  ... and {len(false_negatives) - 5} more.\")\n                if rule_selectivity &gt; estimated_selectivity:\n                    self.console.log(\n                        f\"[red]Blocking rule rejected. Rule selectivity ({rule_selectivity:.4f}) is higher than the estimated selectivity ({estimated_selectivity:.4f}).[/red]\"\n                    )\n                blocking_rules = (\n                    []\n                )  # Clear the blocking rule if it introduces false negatives or is too selective\n\n        optimized_config = self._update_config(threshold, blocking_keys, blocking_rules)\n        return optimized_config, embedding_cost + comparison_cost\n\n    def optimize_equijoin(\n        self, left_data: List[Dict[str, Any]], right_data: List[Dict[str, Any]]\n    ) -&gt; Tuple[Dict[str, Any], float, Dict[str, Any]]:\n        left_keys = self.op_config.get(\"blocking_keys\", {}).get(\"left\", [])\n        right_keys = self.op_config.get(\"blocking_keys\", {}).get(\"right\", [])\n\n        if not left_keys and not right_keys:\n            # Ask the LLM agent if it would be beneficial to do a map operation on\n            # one of the datasets before doing an equijoin\n            apply_transformation, dataset_to_transform, reason = (\n                self._should_apply_map_transformation(\n                    left_keys, right_keys, left_data, right_data\n                )\n            )\n\n            if apply_transformation:\n                self.console.log(\n                    f\"LLM agent suggested applying a map transformation to {dataset_to_transform} dataset because: {reason}\"\n                )\n                extraction_prompt, output_key, new_comparison_prompt = (\n                    self._generate_map_and_new_join_transformation(\n                        dataset_to_transform, reason, left_data, right_data\n                    )\n                )\n                self.console.log(\n                    f\"Generated map transformation prompt: {extraction_prompt}\"\n                )\n                self.console.log(f\"\\nNew output key: {output_key}\")\n                self.console.log(\n                    f\"\\nNew equijoin comparison prompt: {new_comparison_prompt}\"\n                )\n\n                # Update the comparison prompt\n                self.op_config[\"comparison_prompt\"] = new_comparison_prompt\n\n                # Add the output key to the left_keys or right_keys\n                if dataset_to_transform == \"left\":\n                    left_keys.append(output_key)\n                else:\n                    right_keys.append(output_key)\n\n                # Reset the blocking keys in the config\n                self.op_config[\"blocking_keys\"] = {\n                    \"left\": left_keys,\n                    \"right\": right_keys,\n                }\n\n                # Bubble up this config and return the transformation prompt, so we can optimize the map operation\n                return (\n                    self.op_config,\n                    0.0,\n                    {\n                        \"optimize_map\": True,\n                        \"map_prompt\": extraction_prompt,\n                        \"output_key\": output_key,\n                        \"dataset_to_transform\": dataset_to_transform,\n                    },\n                )\n\n            # Print the reason for not applying a map transformation\n            self.console.log(\n                f\"Reason for not synthesizing a map transformation for either left or right dataset: {reason}\"\n            )\n\n        # If there are no blocking keys, generate them\n        if not left_keys or not right_keys:\n            generated_left_keys, generated_right_keys = (\n                self._generate_blocking_keys_equijoin(left_data, right_data)\n            )\n            left_keys.extend(generated_left_keys)\n            right_keys.extend(generated_right_keys)\n            left_keys = list(set(left_keys))\n            right_keys = list(set(right_keys))\n\n            # Log the generated blocking keys\n            self.console.log(\n                f\"[bold]Generated blocking keys (for embeddings-based blocking):[/bold]\"\n            )\n            self.console.log(f\"Left keys: {left_keys}\")\n            self.console.log(f\"Right keys: {right_keys}\")\n\n        left_embeddings, _, left_embedding_cost = self._compute_embeddings(\n            left_data, keys=left_keys\n        )\n        right_embeddings, _, right_embedding_cost = self._compute_embeddings(\n            right_data, keys=right_keys\n        )\n        self.console.log(\n            f\"[bold]Cost of creating embeddings on the sample: ${left_embedding_cost + right_embedding_cost:.4f}[/bold]\"\n        )\n\n        similarities = self._calculate_cross_similarities(\n            left_embeddings, right_embeddings\n        )\n\n        sampled_pairs = self._sample_pairs(similarities)\n        comparison_results, comparison_cost = self._perform_comparisons_equijoin(\n            left_data, right_data, sampled_pairs\n        )\n        self._print_similarity_histogram(similarities, comparison_results)\n        while not any(result[2] for result in comparison_results):\n            self.console.log(\n                \"[yellow]No matches found in the current sample. Resampling pairs to compare...[/yellow]\"\n            )\n            sampled_pairs = self._sample_pairs(similarities)\n            comparison_results, current_cost = self._perform_comparisons_equijoin(\n                left_data, right_data, sampled_pairs\n            )\n            comparison_cost += current_cost\n            self._print_similarity_histogram(similarities, comparison_results)\n\n        threshold, estimated_selectivity = self._find_optimal_threshold(\n            comparison_results, similarities\n        )\n        self.estimated_selectivity = estimated_selectivity\n\n        blocking_rules = self._generate_blocking_rules_equijoin(\n            left_keys, right_keys, left_data, right_data, comparison_results\n        )\n\n        if blocking_rules:\n            false_negatives, rule_selectivity = self._verify_blocking_rule_equijoin(\n                left_data,\n                right_data,\n                blocking_rules[0],\n                left_keys,\n                right_keys,\n                comparison_results,\n            )\n            if not false_negatives and rule_selectivity &lt;= estimated_selectivity:\n                self.console.log(\n                    \"[green]Blocking rule verified. No false negatives detected in the sample and selectivity is within bounds.[/green]\"\n                )\n            else:\n                if false_negatives:\n                    self.console.log(\n                        f\"[red]Blocking rule rejected. {len(false_negatives)} false negatives detected in the sample.[/red]\"\n                    )\n                    for i, j in false_negatives[:5]:  # Show up to 5 examples\n                        self.console.log(\n                            f\"  Filtered pair: Left: {{{', '.join(f'{key}: {left_data[i][key]}' for key in left_keys)}}} and Right: {{{', '.join(f'{key}: {right_data[j][key]}' for key in right_keys)}}}\"\n                        )\n                    if len(false_negatives) &gt; 5:\n                        self.console.log(f\"  ... and {len(false_negatives) - 5} more.\")\n                if rule_selectivity &gt; estimated_selectivity:\n                    self.console.log(\n                        f\"[red]Blocking rule rejected. Rule selectivity ({rule_selectivity:.4f}) is higher than the estimated selectivity ({estimated_selectivity:.4f}).[/red]\"\n                    )\n                blocking_rules = (\n                    []\n                )  # Clear the blocking rule if it introduces false negatives or is too selective\n\n        containment_rules = self._generate_containment_rules_equijoin(\n            left_data, right_data\n        )\n        self.console.log(\n            f\"[bold]Generated {len(containment_rules)} containment rules. Please select which ones to use as blocking conditions:[/bold]\"\n        )\n        selected_containment_rules = []\n        for rule in containment_rules:\n            self.console.log(f\"[green]{rule}[/green]\")\n            # Temporarily stop the status\n            if self.status:\n                self.status.stop()\n            # Use Rich's Confirm for input\n            if Confirm.ask(\"Use this rule?\"):\n                selected_containment_rules.append(rule)\n            # Restart the status\n            if self.status:\n                self.status.start()\n\n        if len(containment_rules) &gt; 0:\n            self.console.log(\n                f\"[bold]Selected {len(selected_containment_rules)} containment rules for blocking.[/bold]\"\n            )\n        blocking_rules.extend(selected_containment_rules)\n\n        optimized_config = self._update_config_equijoin(\n            threshold, left_keys, right_keys, blocking_rules\n        )\n        return (\n            optimized_config,\n            left_embedding_cost + right_embedding_cost + comparison_cost,\n            {},\n        )\n\n    def _should_apply_map_transformation(\n        self,\n        left_keys: List[str],\n        right_keys: List[str],\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n        sample_size: int = 5,\n    ) -&gt; Tuple[bool, str, str]:\n        # Sample data\n        left_sample = random.sample(left_data, min(sample_size, len(left_data)))\n        right_sample = random.sample(right_data, min(sample_size, len(right_data)))\n\n        # Get keys and their average lengths\n        all_left_keys = {\n            k: sum(len(str(d[k])) for d in left_sample) / len(left_sample)\n            for k in left_sample[0].keys()\n        }\n        all_right_keys = {\n            k: sum(len(str(d[k])) for d in right_sample) / len(right_sample)\n            for k in right_sample[0].keys()\n        }\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze the following datasets and determine if an additional LLM transformation should be applied to generate a new key-value pair for easier joining:\n\n                Comparison prompt for the join operation: {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n                Left dataset keys and average lengths: {json.dumps(all_left_keys, indent=2)}\n                Right dataset keys and average lengths: {json.dumps(all_right_keys, indent=2)}\n\n                Left dataset sample:\n                {json.dumps(left_sample, indent=2)}\n\n                Right dataset sample:\n                {json.dumps(right_sample, indent=2)}\n\n                Current keys used for embedding-based ranking of likely matches:\n                Left keys: {left_keys}\n                Right keys: {right_keys}\n\n                Consider the following:\n                1. Are the current keys sufficient for accurate embedding-based ranking of likely matches? We don't want to use too many keys, or keys with too much information, as this will dilute the signal in the embeddings.\n                2. Are there any keys particularly long (e.g., full text fields), containing information that is not relevant for the join operation?\n                3. Is there information spread across multiple fields that could be combined?\n                4. Would a summary or extraction of key information be beneficial?\n                5. Is there a mismatch in information representation between the datasets?\n                6. Could an additional LLM-generated field improve the accuracy of embeddings or join comparisons?\n\n                If you believe an additional LLM transformation would be beneficial, specify which dataset (left or right) should be transformed and explain why. In most cases, you should pick the dataset with the longer keys unless there is a specific reason to pick the other dataset. Otherwise, indicate that no additional transformation is needed and explain why the current blocking keys are sufficient.\"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate(\n            messages,\n            \"You are an AI expert in data analysis and entity matching.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"apply_transformation\": {\"type\": \"boolean\"},\n                    \"dataset_to_transform\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"left\", \"right\", \"none\"],\n                    },\n                    \"reason\": {\"type\": \"string\"},\n                },\n                \"required\": [\"apply_transformation\", \"dataset_to_transform\", \"reason\"],\n            },\n        )\n\n        result = json.loads(response.choices[0].message.content)\n\n        return (\n            result[\"apply_transformation\"],\n            result[\"dataset_to_transform\"],\n            result[\"reason\"],\n        )\n\n    def _generate_map_and_new_join_transformation(\n        self,\n        dataset_to_transform: str,\n        reason: str,\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n        sample_size: int = 5,\n    ) -&gt; Tuple[str, str, str]:\n        # Sample data\n        left_sample = random.sample(left_data, min(sample_size, len(left_data)))\n        right_sample = random.sample(right_data, min(sample_size, len(right_data)))\n\n        target_data = left_sample if dataset_to_transform == \"left\" else right_sample\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Generate an LLM prompt to transform the {dataset_to_transform} dataset for easier joining. The transformation should create a new key-value pair.\n\n                Current comparison prompt for the join operation: {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n                Target ({dataset_to_transform}) dataset sample:\n                {json.dumps(target_data, indent=2)}\n\n                Other ({'left' if dataset_to_transform == \"right\" else \"right\"}) dataset sample:\n                {json.dumps(right_sample if dataset_to_transform == \"left\" else left_sample, indent=2)}\n\n                Reason for transforming {dataset_to_transform} dataset: {reason}\n\n                Please provide:\n                1. An LLM prompt to extract a smaller representation of what is relevant to the join task. The prompt should be a Jinja2 template, referring to any fields in the input data as {{ input.field_name }}. The prompt should instruct the LLM to return some **non-empty** string-valued output. The transformation should be tailored to the join task if possible, not just a generic summary of the data.\n                2. A name for the new output key that will store the transformed data.\n                3. An edited comparison prompt that leverages the new attribute created by the transformation. This prompt should be a Jinja2 template, referring to any fields in the input data as {{ left.field_name }} and {{ right.field_name }}. The prompt should be the same as the current comparison prompt, but with a new instruction that leverages the new attribute created by the transformation. The prompt should instruct the LLM to return a boolean-valued output, like the current comparison prompt.\"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate(\n            messages,\n            \"You are an AI expert in data analysis and decomposing complex data processing pipelines.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"extraction_prompt\": {\"type\": \"string\"},\n                    \"output_key\": {\"type\": \"string\"},\n                    \"new_comparison_prompt\": {\"type\": \"string\"},\n                },\n                \"required\": [\n                    \"extraction_prompt\",\n                    \"output_key\",\n                    \"new_comparison_prompt\",\n                ],\n            },\n        )\n\n        result = json.loads(response.choices[0].message.content)\n\n        return (\n            result[\"extraction_prompt\"],\n            result[\"output_key\"],\n            result[\"new_comparison_prompt\"],\n        )\n\n    def _generate_blocking_keys_equijoin(\n        self,\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n        sample_size: int = 5,\n    ) -&gt; Tuple[List[str], List[str]]:\n        # Sample data\n        left_sample = random.sample(left_data, min(sample_size, len(left_data)))\n        right_sample = random.sample(right_data, min(sample_size, len(right_data)))\n\n        # Prepare sample data for LLM\n        left_keys = list(left_sample[0].keys())\n        right_keys = list(right_sample[0].keys())\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Given the following sample data from two datasets, select appropriate blocking keys for an equijoin operation.\n                The blocking process works as follows:\n                1. We create embeddings for the selected keys from both datasets.\n                2. We use cosine similarity between these embeddings to filter pairs for more detailed LLM comparison.\n                3. Pairs with high similarity will be passed to the LLM for final comparison.\n\n                The blocking keys should have relatively short values and be useful for generating embeddings that capture the essence of potential matches.\n\n                Left dataset keys: {left_keys}\n                Right dataset keys: {right_keys}\n\n                Sample from left dataset:\n                {json.dumps(left_sample, indent=2)}\n\n                Sample from right dataset:\n                {json.dumps(right_sample, indent=2)}\n\n                For context, here is the comparison prompt that will be used for the more detailed LLM comparison:\n                {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n                Please select one or more keys from each dataset that would be suitable for blocking. The keys should contain information that's likely to be similar in matching records and align with the comparison prompt's focus.\"\"\",\n            }\n        ]\n\n        response = self.llm_client.generate(\n            messages,\n            \"You are an expert in entity matching and database operations.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"left_blocking_keys\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of selected blocking keys from the left dataset\",\n                    },\n                    \"right_blocking_keys\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of selected blocking keys from the right dataset\",\n                    },\n                },\n                \"required\": [\"left_blocking_keys\", \"right_blocking_keys\"],\n            },\n        )\n\n        result = json.loads(response.choices[0].message.content)\n        left_blocking_keys = result[\"left_blocking_keys\"]\n        right_blocking_keys = result[\"right_blocking_keys\"]\n\n        return left_blocking_keys, right_blocking_keys\n\n    def _compute_embeddings(\n        self,\n        input_data: List[Dict[str, Any]],\n        keys: List[str] = None,\n        is_join: bool = True,\n    ) -&gt; Tuple[List[List[float]], List[str], float]:\n        if keys is None:\n            keys = self.op_config.get(\"blocking_keys\", [])\n            if not keys:\n                prompt_template = self.op_config.get(\"comparison_prompt\", \"\")\n                prompt_vars = extract_jinja_variables(prompt_template)\n                # Get rid of input, input1, input2\n                prompt_vars = [\n                    var\n                    for var in prompt_vars\n                    if var not in [\"input\", \"input1\", \"input2\"]\n                ]\n\n                # strip all things before . in the prompt_vars\n                keys += list(set([var.split(\".\")[-1] for var in prompt_vars]))\n            if not keys:\n                self.console.log(\n                    \"[yellow]Warning: No blocking keys found. Using all keys for blocking.[/yellow]\"\n                )\n                keys = list(input_data[0].keys())\n\n        model_input_context_length = model_cost.get(\n            self.op_config.get(\"embedding_model\", \"text-embedding-3-small\"), {}\n        ).get(\"max_input_tokens\", 8192)\n        texts = [\n            \" \".join(str(item[key]) for key in keys if key in item)[\n                :model_input_context_length\n            ]\n            for item in input_data\n        ]\n\n        embeddings = []\n        total_cost = 0\n        batch_size = 2000\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i : i + batch_size]\n            self.console.log(\n                f\"[cyan]Processing batch {i//batch_size + 1} of {len(texts)//batch_size + 1}[/cyan]\"\n            )\n            response = self.runner.api.gen_embedding(\n                model=self.op_config.get(\"embedding_model\", \"text-embedding-3-small\"),\n                input=batch,\n            )\n            embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n            total_cost += completion_cost(response)\n        embeddings = [data[\"embedding\"] for data in response[\"data\"]]\n        cost = completion_cost(response)\n        return embeddings, keys, cost\n\n    def _calculate_cosine_similarities(\n        self, embeddings: List[List[float]]\n    ) -&gt; List[Tuple[int, int, float]]:\n        embeddings_array = np.array(embeddings)\n        norms = np.linalg.norm(embeddings_array, axis=1)\n        dot_products = np.dot(embeddings_array, embeddings_array.T)\n        similarities_matrix = dot_products / np.outer(norms, norms)\n        i, j = np.triu_indices(len(embeddings), k=1)\n        similarities = list(\n            zip(i.tolist(), j.tolist(), similarities_matrix[i, j].tolist())\n        )\n        return similarities\n\n    def _print_similarity_histogram(\n        self,\n        similarities: List[Tuple[int, int, float]],\n        comparison_results: List[Tuple[int, int, bool]],\n    ):\n        flat_similarities = [sim[-1] for sim in similarities if sim[-1] != 1]\n        hist, bin_edges = np.histogram(flat_similarities, bins=20)\n        max_bar_width, max_count = 50, max(hist)\n        normalized_hist = [int(count / max_count * max_bar_width) for count in hist]\n\n        # Create a dictionary to store true labels\n        true_labels = {(i, j): is_match for i, j, is_match in comparison_results}\n\n        self.console.log(\"\\n[bold]Embedding Cosine Similarity Distribution:[/bold]\")\n        for i, count in enumerate(normalized_hist):\n            bar = \"\u2588\" * count\n            label = f\"{bin_edges[i]:.2f}-{bin_edges[i+1]:.2f}\"\n\n            # Count true matches and not matches in this bin\n            true_matches = 0\n            not_matches = 0\n            labeled_count = 0\n            for sim in similarities:\n                if bin_edges[i] &lt;= sim[2] &lt; bin_edges[i + 1]:\n                    if (sim[0], sim[1]) in true_labels:\n                        labeled_count += 1\n                        if true_labels[(sim[0], sim[1])]:\n                            true_matches += 1\n                        else:\n                            not_matches += 1\n\n            # Calculate percentages of labeled pairs\n            if labeled_count &gt; 0:\n                true_match_percent = (true_matches / labeled_count) * 100\n                not_match_percent = (not_matches / labeled_count) * 100\n            else:\n                true_match_percent = 0\n                not_match_percent = 0\n\n            self.console.log(\n                f\"{label}: {bar} \"\n                f\"(Labeled: {labeled_count}/{hist[i]}, [green]{true_match_percent:.1f}% match[/green], [red]{not_match_percent:.1f}% not match[/red])\"\n            )\n        self.console.log(\"\\n\")\n\n    def _sample_pairs(\n        self, similarities: List[Tuple[int, int, float]]\n    ) -&gt; List[Tuple[int, int]]:\n        # Sort similarities in descending order\n        sorted_similarities = sorted(similarities, key=lambda x: x[2], reverse=True)\n\n        # Calculate weights using exponential weighting with self.sampling_weight\n        similarities_array = np.array([sim[2] for sim in sorted_similarities])\n        weights = np.exp(self.sampling_weight * similarities_array)\n        weights /= weights.sum()  # Normalize weights to sum to 1\n\n        # Sample pairs based on the calculated weights\n        sampled_indices = np.random.choice(\n            len(sorted_similarities),\n            size=min(self.sample_size, len(sorted_similarities)),\n            replace=False,\n            p=weights,\n        )\n\n        sampled_pairs = [\n            (sorted_similarities[i][0], sorted_similarities[i][1])\n            for i in sampled_indices\n        ]\n        return sampled_pairs\n\n    def _calculate_cross_similarities(\n        self, left_embeddings: List[List[float]], right_embeddings: List[List[float]]\n    ) -&gt; List[Tuple[int, int, float]]:\n        left_array = np.array(left_embeddings)\n        right_array = np.array(right_embeddings)\n        dot_product = np.dot(left_array, right_array.T)\n        norm_left = np.linalg.norm(left_array, axis=1)\n        norm_right = np.linalg.norm(right_array, axis=1)\n        similarities = dot_product / np.outer(norm_left, norm_right)\n        return [\n            (i, j, sim)\n            for i, row in enumerate(similarities)\n            for j, sim in enumerate(row)\n        ]\n\n    def _perform_comparisons_resolve(\n        self, input_data: List[Dict[str, Any]], pairs: List[Tuple[int, int]]\n    ) -&gt; Tuple[List[Tuple[int, int, bool]], float]:\n        comparisons, total_cost = [], 0\n        op = ResolveOperation(\n            self,\n            self.op_config,\n            self.runner.default_model,\n            self.max_threads,\n            self.console,\n            self.status)\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(\n                    op.compare_pair,\n                    self.op_config[\"comparison_prompt\"],\n                    self.op_config.get(\n                        \"comparison_model\", self.config.get(\"model\", \"gpt-4o-mini\")\n                    ),\n                    input_data[i],\n                    input_data[j],\n                )\n                for i, j in pairs\n            ]\n            for future, (i, j) in zip(futures, pairs):\n                is_match, cost = future.result()\n                comparisons.append((i, j, is_match))\n                total_cost += cost\n\n        self.console.log(\n            f\"[bold]Cost of pairwise comparisons on the sample: ${total_cost:.4f}[/bold]\"\n        )\n        return comparisons, total_cost\n\n    def _perform_comparisons_equijoin(\n        self,\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n        pairs: List[Tuple[int, int]],\n    ) -&gt; Tuple[List[Tuple[int, int, bool]], float]:\n        comparisons, total_cost = [], 0\n        op = EquijoinOperation(\n            self,\n            self.op_config,\n            self.runner.default_model,\n            self.max_threads,\n            self.console,\n            self.status)\n        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:\n            futures = [\n                executor.submit(\n                    op.compare_pair,\n                    self.op_config[\"comparison_prompt\"],\n                    self.op_config.get(\n                        \"comparison_model\", self.config.get(\"model\", \"gpt-4o-mini\")\n                    ),\n                    left_data[i],\n                    right_data[j] if right_data else left_data[j],\n                )\n                for i, j in pairs\n            ]\n            for future, (i, j) in zip(futures, pairs):\n                is_match, cost = future.result()\n                comparisons.append((i, j, is_match))\n                total_cost += cost\n\n        self.console.log(\n            f\"[bold]Cost of pairwise comparisons on the sample: ${total_cost:.4f}[/bold]\"\n        )\n        return comparisons, total_cost\n\n    def _find_optimal_threshold(\n        self,\n        comparisons: List[Tuple[int, int, bool]],\n        similarities: List[Tuple[int, int, float]],\n    ) -&gt; Tuple[float, float, float]:\n        true_labels = np.array([comp[2] for comp in comparisons])\n        sim_dict = {(i, j): sim for i, j, sim in similarities}\n        sim_scores = np.array([sim_dict[(i, j)] for i, j, _ in comparisons])\n\n        thresholds = np.linspace(0, 1, 100)\n        precisions, recalls = [], []\n\n        for threshold in thresholds:\n            predictions = sim_scores &gt;= threshold\n            tp = np.sum(predictions &amp; true_labels)\n            fp = np.sum(predictions &amp; ~true_labels)\n            fn = np.sum(~predictions &amp; true_labels)\n\n            precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n            recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n\n            precisions.append(precision)\n            recalls.append(recall)\n\n        valid_indices = [i for i, r in enumerate(recalls) if r &gt;= self.target_recall]\n        if not valid_indices:\n            optimal_threshold = float(thresholds[np.argmax(recalls)])\n        else:\n            optimal_threshold = float(thresholds[max(valid_indices)])\n\n        # Improved selectivity estimation\n        all_similarities = np.array([s[2] for s in similarities])\n        sampled_similarities = sim_scores\n\n        # Calculate sampling probabilities\n        sampling_probs = np.exp(self.sampling_weight * sampled_similarities)\n        sampling_probs /= sampling_probs.sum()\n\n        # Estimate selectivity using importance sampling\n        weights = 1 / (len(all_similarities) * sampling_probs)\n        numerator = np.sum(weights * true_labels)\n        denominator = np.sum(weights)\n        selectivity_estimate = numerator / denominator\n\n        self.console.log(\n            \"[bold cyan]\u250c\u2500 Estimated Self-Join Selectivity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510[/bold cyan]\"\n        )\n        self.console.log(\n            f\"[bold cyan]\u2502[/bold cyan] [yellow]Target Recall:[/yellow] {self.target_recall:.0%}\"\n        )\n        self.console.log(\n            f\"[bold cyan]\u2502[/bold cyan] [yellow]Estimate:[/yellow] {selectivity_estimate:.4f}\"\n        )\n        self.console.log(\n            \"[bold cyan]\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518[/bold cyan]\"\n        )\n        self.console.log(\n            f\"[bold]Chosen similarity threshold for blocking: {optimal_threshold:.4f}[/bold]\"\n        )\n\n        return round(optimal_threshold, 4), selectivity_estimate\n\n    def _generate_blocking_rules(\n        self,\n        blocking_keys: List[str],\n        input_data: List[Dict[str, Any]],\n        comparisons: List[Tuple[int, int, bool]],\n    ) -&gt; List[str]:\n        # Sample 2 true and 2 false comparisons\n        true_comparisons = [comp for comp in comparisons if comp[2]][:2]\n        false_comparisons = [comp for comp in comparisons if not comp[2]][:2]\n        sample_datas = [\n            (\n                {key: input_data[i][key] for key in blocking_keys},\n                {key: input_data[j][key] for key in blocking_keys},\n                is_match,\n            )\n            for i, j, is_match in true_comparisons + false_comparisons\n        ]\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Given the following sample comparisons between entities, generate a single-line Python statement that acts as a blocking rule for entity resolution. This rule will be used in the form: `eval(blocking_rule, {{\"input1\": item1, \"input2\": item2}})`.\n\n    Sample comparisons (note: these are just a few examples and may not represent all possible cases):\n    {json.dumps(sample_datas, indent=2)}\n\n    For context, here is the comparison prompt that will be used for the more expensive, detailed comparison:\n    {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n    Please generate ONE one-line blocking rule that adheres to the following criteria:\n    1. The rule should evaluate to True if the entities are possibly a match and require further comparison.\n    2. The rule should evaluate to False ONLY if the entities are definitely not a match.\n    3. The rule must be a single Python expression that can be evaluated using the eval() function.\n    4. The rule should be much faster to evaluate than the full comparison prompt.\n    5. The rule should capture the essence of the comparison prompt but in a simplified manner.\n    6. The rule should be general enough to work well on the entire dataset, not just these specific examples.\n    7. The rule should handle inconsistent casing by using string methods like .lower() when comparing string values.\n    8. The rule should err on the side of inclusivity - it's better to have false positives than false negatives.\n\n    Example structure of a one-line blocking rule:\n    \"(condition1) or (condition2) or (condition3)\"\n\n    Where conditions could be comparisons like:\n    \"input1['field'].lower() == input2['field'].lower()\"\n    \"abs(len(input1['text']) - len(input2['text'])) &lt;= 5\"\n    \"any(word in input1['description'].lower() for word in input2['description'].lower().split())\"\n\n    If there's no clear rule that can be generated based on the given information, return the string \"True\" to ensure all pairs are compared.\n\n    Remember, the primary goal of the blocking rule is to safely reduce the number of comparisons by quickly identifying pairs that are definitely not matches, while keeping all potential matches for further evaluation.\"\"\",\n            }\n        ]\n\n        for attempt in range(self.agent_max_retries):  # Up to 3 attempts\n            # Generate blocking rule using the LLM\n            response = self.llm_client.generate(\n                messages,\n                \"You are an expert in entity resolution and Python programming. Your task is to generate one efficient blocking rule based on the given sample comparisons and data structure.\",\n                {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"blocking_rule\": {\n                            \"type\": \"string\",\n                            \"description\": \"One-line Python statement acting as a blocking rule\",\n                        }\n                    },\n                    \"required\": [\"blocking_rule\"],\n                },\n            )\n\n            # Extract the blocking rule from the LLM response\n            blocking_rule = response.choices[0].message.content\n            blocking_rule = json.loads(blocking_rule).get(\"blocking_rule\")\n\n            if blocking_rule:\n                self.console.log(\"\")  # Print a newline\n\n                if blocking_rule.strip() == \"True\":\n                    self.console.log(\n                        \"[yellow]No suitable blocking rule could be found. Proceeding without a blocking rule.[/yellow]\"\n                    )\n                    return []\n\n                self.console.log(\n                    f\"[bold]Generated blocking rule (Attempt {attempt + 1}):[/bold] {blocking_rule}\"\n                )\n\n                # Test the blocking rule\n                filtered_pairs = self._test_blocking_rule(\n                    input_data, blocking_keys, blocking_rule, comparisons\n                )\n\n                if not filtered_pairs:\n                    self.console.log(\n                        \"[green]Blocking rule looks good! No known matches were filtered out.[/green]\"\n                    )\n                    return [blocking_rule]\n                else:\n                    feedback = f\"The previous rule incorrectly filtered out {len(filtered_pairs)} known matches. \"\n                    feedback += (\n                        \"Here are up to 3 examples of incorrectly filtered pairs:\\n\"\n                    )\n                    for i, j in filtered_pairs[:3]:\n                        feedback += f\"Item 1: {json.dumps({key: input_data[i][key] for key in blocking_keys})}\\nItem 2: {json.dumps({key: input_data[j][key] for key in blocking_keys})}\\n\"\n                        feedback += \"These pairs are known matches but were filtered out by the rule.\\n\"\n                    feedback += \"Please generate a new rule that doesn't filter out these matches.\"\n\n                    messages.append({\"role\": \"assistant\", \"content\": blocking_rule})\n                    messages.append({\"role\": \"user\", \"content\": feedback})\n            else:\n                self.console.log(\"[yellow]No blocking rule generated.[/yellow]\")\n                return []\n\n        self.console.log(\n            f\"[yellow]Failed to generate a suitable blocking rule after {self.agent_max_retries} attempts. Proceeding without a blocking rule.[/yellow]\"\n        )\n        return []\n\n    def _test_blocking_rule(\n        self,\n        input_data: List[Dict[str, Any]],\n        blocking_keys: List[str],\n        blocking_rule: str,\n        comparisons: List[Tuple[int, int, bool]],\n    ) -&gt; List[Tuple[int, int]]:\n        def apply_blocking_rule(item1, item2):\n            try:\n                return eval(blocking_rule, {\"input1\": item1, \"input2\": item2})\n            except Exception as e:\n                self.console.log(f\"[red]Error applying blocking rule: {e}[/red]\")\n                return True  # If there's an error, we default to comparing the pair\n\n        filtered_pairs = []\n\n        for i, j, is_match in comparisons:\n            if is_match:\n                item1 = {\n                    k: input_data[i][k] for k in blocking_keys if k in input_data[i]\n                }\n                item2 = {\n                    k: input_data[j][k] for k in blocking_keys if k in input_data[j]\n                }\n\n                if not apply_blocking_rule(item1, item2):\n                    filtered_pairs.append((i, j))\n\n        if filtered_pairs:\n            self.console.log(\n                f\"[yellow italic]LLM Correction: The blocking rule incorrectly filtered out {len(filtered_pairs)} known positive matches.[/yellow italic]\"\n            )\n            for i, j in filtered_pairs[:5]:  # Show up to 5 examples\n                self.console.log(\n                    f\"  Incorrectly filtered pair 1: {json.dumps({key: input_data[i][key] for key in blocking_keys})}  and pair 2: {json.dumps({key: input_data[j][key] for key in blocking_keys})}\"\n                )\n            if len(filtered_pairs) &gt; 5:\n                self.console.log(\n                    f\"  ... and {len(filtered_pairs) - 5} more incorrect pairs.\"\n                )\n\n        return filtered_pairs\n\n    def _generate_containment_rules_equijoin(\n        self,\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n    ) -&gt; List[str]:\n        # Get all available keys from the sample data\n        left_keys = set(left_data[0].keys())\n        right_keys = set(right_data[0].keys())\n\n        # Sample a few records from each dataset\n        sample_left = random.sample(left_data, min(3, len(left_data)))\n        sample_right = random.sample(right_data, min(3, len(right_data)))\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an AI assistant tasked with generating containment-based blocking rules for an equijoin operation.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Generate multiple one-line Python statements that act as containment-based blocking rules for equijoin. These rules will be used in the form: `eval(blocking_rule, {{\"left\": item1, \"right\": item2}})`.\n\nAvailable keys in left dataset: {', '.join(left_keys)}\nAvailable keys in right dataset: {', '.join(right_keys)}\n\nSample data from left dataset:\n{json.dumps(sample_left, indent=2)}\n\nSample data from right dataset:\n{json.dumps(sample_right, indent=2)}\n\nComparison prompt used for detailed comparison:\n{self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\nPlease generate multiple one-line blocking rules that adhere to the following criteria:\n1. The rules should focus on containment relationships between fields in the left and right datasets. Containment can mean that the left field contains all the words in the right field, or the right field contains all the words in the left field.\n2. Each rule should evaluate to True if there's a potential match based on containment, False otherwise.\n3. Rules must be single Python expressions that can be evaluated using the eval() function.\n4. Rules should handle inconsistent casing by using string methods like .lower() when comparing string values.\n5. Consider the length of the fields when generating rules: for example, if the left field is much longer than the right field, it's more likely to contain all the words in the right field.\n\nExample structures of containment-based blocking rules:\n\"all(word in left['{{left_key}}'].lower() for word in right['{{right_key}}'].lower().split())\"\n\"any(word in right['{{right_key}}'].lower().split() for word in left['{{left_key}}'].lower().split())\"\n\nPlease provide 3-5 different containment-based blocking rules, based on the keys and sample data provided.\"\"\",\n            },\n        ]\n\n        response = self.llm_client.generate(\n            messages,\n            \"You are an expert in data matching and Python programming.\",\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"containment_rules\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"List of containment-based blocking rules as Python expressions\",\n                    }\n                },\n                \"required\": [\"containment_rules\"],\n            },\n        )\n\n        containment_rules = response.choices[0].message.content\n        containment_rules = json.loads(containment_rules).get(\"containment_rules\")\n        return containment_rules\n\n    def _generate_blocking_rules_equijoin(\n        self,\n        left_keys: List[str],\n        right_keys: List[str],\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n        comparisons: List[Tuple[int, int, bool]],\n    ) -&gt; List[str]:\n        if not left_keys or not right_keys:\n            left_keys = list(left_data[0].keys())\n            right_keys = list(right_data[0].keys())\n\n        # Sample 2 true and 2 false comparisons\n        true_comparisons = [comp for comp in comparisons if comp[2]][:2]\n        false_comparisons = [comp for comp in comparisons if not comp[2]][:2]\n        sample_datas = [\n            (\n                {key: left_data[i][key] for key in left_keys if key in left_data[i]},\n                {key: right_data[j][key] for key in right_keys if key in right_data[j]},\n                is_match,\n            )\n            for i, j, is_match in true_comparisons + false_comparisons\n        ]\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Given the following sample comparisons between entities, generate a single-line Python statement that acts as a blocking rule for equijoin. This rule will be used in the form: `eval(blocking_rule, {{\"left\": item1, \"right\": item2}})`.\n\n    Sample comparisons (note: these are just a few examples and may not represent all possible cases):\n    {json.dumps(sample_datas, indent=2)}\n\n    For context, here is the comparison prompt that will be used for the more expensive, detailed comparison:\n    {self.op_config.get('comparison_prompt', 'No comparison prompt provided.')}\n\n    Please generate ONE one-line blocking rule that adheres to the following criteria:\n    1. The rule should evaluate to True if the entities are possibly a match and require further comparison.\n    2. The rule should evaluate to False ONLY if the entities are definitely not a match.\n    3. The rule must be a single Python expression that can be evaluated using the eval() function.\n    4. The rule should be much faster to evaluate than the full comparison prompt.\n    5. The rule should capture the essence of the comparison prompt but in a simplified manner.\n    6. The rule should be general enough to work well on the entire dataset, not just these specific examples.\n    7. The rule should handle inconsistent casing by using string methods like .lower() when comparing string values.\n    8. The rule should err on the side of inclusivity - it's better to have false positives than false negatives.\n\n    Example structure of a one-line blocking rule:\n    \"(condition1) or (condition2) or (condition3)\"\n\n    Where conditions could be comparisons like:\n    \"left['{left_keys[0]}'].lower() == right['{right_keys[0]}'].lower()\"\n    \"abs(len(left['{left_keys[0]}']) - len(right['{right_keys[0]}'])) &lt;= 5\"\n    \"any(word in left['{left_keys[0]}'].lower() for word in right['{right_keys[0]}'].lower().split())\"\n\n    If there's no clear rule that can be generated based on the given information, return the string \"True\" to ensure all pairs are compared.\n\n    Remember, the primary goal of the blocking rule is to safely reduce the number of comparisons by quickly identifying pairs that are definitely not matches, while keeping all potential matches for further evaluation.\"\"\",\n            }\n        ]\n\n        for attempt in range(self.agent_max_retries):\n            response = self.llm_client.generate(\n                messages,\n                \"You are an expert in entity resolution and Python programming. Your task is to generate one efficient blocking rule based on the given sample comparisons and data structure.\",\n                {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"blocking_rule\": {\n                            \"type\": \"string\",\n                            \"description\": \"One-line Python statement acting as a blocking rule\",\n                        }\n                    },\n                    \"required\": [\"blocking_rule\"],\n                },\n            )\n\n            blocking_rule = response.choices[0].message.content\n            blocking_rule = json.loads(blocking_rule).get(\"blocking_rule\")\n\n            if blocking_rule:\n                self.console.log(\"\")\n\n                if blocking_rule.strip() == \"True\":\n                    self.console.log(\n                        \"[yellow]No suitable blocking rule could be found. Proceeding without a blocking rule.[/yellow]\"\n                    )\n                    return []\n\n                self.console.log(\n                    f\"[bold]Generated blocking rule (Attempt {attempt + 1}):[/bold] {blocking_rule}\"\n                )\n\n                # Test the blocking rule\n                filtered_pairs = self._test_blocking_rule_equijoin(\n                    left_data,\n                    right_data,\n                    left_keys,\n                    right_keys,\n                    blocking_rule,\n                    comparisons,\n                )\n\n                if not filtered_pairs:\n                    self.console.log(\n                        \"[green]Blocking rule looks good! No known matches were filtered out.[/green]\"\n                    )\n                    return [blocking_rule]\n                else:\n                    feedback = f\"The previous rule incorrectly filtered out {len(filtered_pairs)} known matches. \"\n                    feedback += (\n                        \"Here are up to 3 examples of incorrectly filtered pairs:\\n\"\n                    )\n                    for i, j in filtered_pairs[:3]:\n                        feedback += f\"Left: {json.dumps({key: left_data[i][key] for key in left_keys})}\\n\"\n                        feedback += f\"Right: {json.dumps({key: right_data[j][key] for key in right_keys})}\\n\"\n                        feedback += \"These pairs are known matches but were filtered out by the rule.\\n\"\n                    feedback += \"Please generate a new rule that doesn't filter out these matches.\"\n\n                    messages.append({\"role\": \"assistant\", \"content\": blocking_rule})\n                    messages.append({\"role\": \"user\", \"content\": feedback})\n            else:\n                self.console.log(\"[yellow]No blocking rule generated.[/yellow]\")\n                return []\n\n        self.console.log(\n            f\"[yellow]Failed to generate a suitable blocking rule after {self.agent_max_retries} attempts. Proceeding without a blocking rule.[/yellow]\"\n        )\n        return []\n\n    def _test_blocking_rule_equijoin(\n        self,\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n        left_keys: List[str],\n        right_keys: List[str],\n        blocking_rule: str,\n        comparisons: List[Tuple[int, int, bool]],\n    ) -&gt; List[Tuple[int, int]]:\n        def apply_blocking_rule(left, right):\n            try:\n                return eval(blocking_rule, {\"left\": left, \"right\": right})\n            except Exception as e:\n                self.console.log(f\"[red]Error applying blocking rule: {e}[/red]\")\n                return True  # If there's an error, we default to comparing the pair\n\n        filtered_pairs = []\n\n        for i, j, is_match in comparisons:\n            if is_match:\n                left = left_data[i]\n                right = right_data[j]\n                if not apply_blocking_rule(left, right):\n                    filtered_pairs.append((i, j))\n\n        if filtered_pairs:\n            self.console.log(\n                f\"[yellow italic]LLM Correction: The blocking rule incorrectly filtered out {len(filtered_pairs)} known positive matches.[/yellow italic]\"\n            )\n            for i, j in filtered_pairs[:5]:  # Show up to 5 examples\n                left_dict = {key: left_data[i][key] for key in left_keys}\n                right_dict = {key: right_data[j][key] for key in right_keys}\n                self.console.log(\n                    f\"  Incorrectly filtered pair - Left: {json.dumps(left_dict)}  Right: {json.dumps(right_dict)}\"\n                )\n            if len(filtered_pairs) &gt; 5:\n                self.console.log(\n                    f\"  ... and {len(filtered_pairs) - 5} more incorrect pairs.\"\n                )\n\n        return filtered_pairs\n\n    def _verify_blocking_rule_equijoin(\n        self,\n        left_data: List[Dict[str, Any]],\n        right_data: List[Dict[str, Any]],\n        blocking_rule: str,\n        left_keys: List[str],\n        right_keys: List[str],\n        comparison_results: List[Tuple[int, int, bool]],\n    ) -&gt; Tuple[List[Tuple[int, int]], float]:\n        def apply_blocking_rule(left, right):\n            try:\n                return eval(blocking_rule, {\"left\": left, \"right\": right})\n            except Exception as e:\n                self.console.log(f\"[red]Error applying blocking rule: {e}[/red]\")\n                return True  # If there's an error, we default to comparing the pair\n\n        false_negatives = []\n        total_pairs = 0\n        blocked_pairs = 0\n\n        for i, j, is_match in comparison_results:\n            total_pairs += 1\n            left = left_data[i]\n            right = right_data[j]\n            if apply_blocking_rule(left, right):\n                blocked_pairs += 1\n                if is_match:\n                    false_negatives.append((i, j))\n\n        rule_selectivity = blocked_pairs / total_pairs if total_pairs &gt; 0 else 0\n\n        return false_negatives, rule_selectivity\n\n    def _update_config_equijoin(\n        self,\n        threshold: float,\n        left_keys: List[str],\n        right_keys: List[str],\n        blocking_rules: List[str],\n    ) -&gt; Dict[str, Any]:\n        optimized_config = self.op_config.copy()\n        optimized_config[\"blocking_keys\"] = {\n            \"left\": left_keys,\n            \"right\": right_keys,\n        }\n        optimized_config[\"blocking_threshold\"] = threshold\n        if blocking_rules:\n            optimized_config[\"blocking_conditions\"] = blocking_rules\n        if \"embedding_model\" not in optimized_config:\n            optimized_config[\"embedding_model\"] = \"text-embedding-3-small\"\n        return optimized_config\n\n    def _verify_blocking_rule(\n        self,\n        input_data: List[Dict[str, Any]],\n        blocking_rule: str,\n        blocking_keys: List[str],\n        comparison_results: List[Tuple[int, int, bool]],\n    ) -&gt; Tuple[List[Tuple[int, int]], float]:\n        def apply_blocking_rule(item1, item2):\n            try:\n                return eval(blocking_rule, {\"input1\": item1, \"input2\": item2})\n            except Exception as e:\n                self.console.log(f\"[red]Error applying blocking rule: {e}[/red]\")\n                return True  # If there's an error, we default to comparing the pair\n\n        false_negatives = []\n        total_pairs = 0\n        blocked_pairs = 0\n\n        for i, j, is_match in comparison_results:\n            total_pairs += 1\n            item1 = {k: input_data[i][k] for k in blocking_keys if k in input_data[i]}\n            item2 = {k: input_data[j][k] for k in blocking_keys if k in input_data[j]}\n\n            if apply_blocking_rule(item1, item2):\n                blocked_pairs += 1\n                if is_match:\n                    false_negatives.append((i, j))\n\n        rule_selectivity = blocked_pairs / total_pairs if total_pairs &gt; 0 else 0\n\n        return false_negatives, rule_selectivity\n\n    def _update_config(\n        self, threshold: float, blocking_keys: List[str], blocking_rules: List[str]\n    ) -&gt; Dict[str, Any]:\n        optimized_config = self.op_config.copy()\n        optimized_config[\"blocking_keys\"] = blocking_keys\n        optimized_config[\"blocking_threshold\"] = threshold\n        if blocking_rules:\n            optimized_config[\"blocking_conditions\"] = blocking_rules\n        if \"embedding_model\" not in optimized_config:\n            optimized_config[\"embedding_model\"] = \"text-embedding-3-small\"\n        return optimized_config\n</code></pre>"},{"location":"api-reference/python/","title":"Python API","text":""},{"location":"api-reference/python/#operations","title":"Operations","text":""},{"location":"api-reference/python/#docetl.schemas.BaseOp","title":"<code>docetl.schemas.BaseOp</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class BaseOp(BaseModel):\n    name: str\n    type: str\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.MapOp","title":"<code>docetl.schemas.MapOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class MapOp(BaseOp):\n    type: str = \"map\"\n    output: Optional[Dict[str, Any]] = None\n    prompt: Optional[str] = None\n    model: Optional[str] = None\n    optimize: Optional[bool] = None\n    recursively_optimize: Optional[bool] = None\n    sample_size: Optional[int] = None\n    tools: Optional[List[Dict[str, Any]]] = None\n    validation_rules: Optional[List[str]] = Field(None, alias=\"validate\")\n    num_retries_on_validate_failure: Optional[int] = None\n    gleaning: Optional[Dict[str, Any]] = None\n    drop_keys: Optional[List[str]] = None\n    timeout: Optional[int] = None\n    batch_size: Optional[int] = None\n    clustering_method: Optional[str] = None\n\n    @field_validator(\"drop_keys\")\n    def validate_drop_keys(cls, v):\n        if isinstance(v, str):\n            return [v]\n        return v\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.ResolveOp","title":"<code>docetl.schemas.ResolveOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class ResolveOp(BaseOp):\n    type: str = \"resolve\"\n    comparison_prompt: str\n    resolution_prompt: str\n    output: Optional[Dict[str, Any]] = None\n    embedding_model: Optional[str] = None\n    resolution_model: Optional[str] = None\n    comparison_model: Optional[str] = None\n    blocking_keys: Optional[List[str]] = None\n    blocking_threshold: Optional[float] = None\n    blocking_conditions: Optional[List[str]] = None\n    input: Optional[Dict[str, Any]] = None\n    embedding_batch_size: Optional[int] = None\n    compare_batch_size: Optional[int] = None\n    limit_comparisons: Optional[int] = None\n    optimize: Optional[bool] = None\n    timeout: Optional[int] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.ReduceOp","title":"<code>docetl.schemas.ReduceOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class ReduceOp(BaseOp):\n    type: str = \"reduce\"\n    reduce_key: Union[str, List[str]]\n    output: Optional[Dict[str, Any]] = None\n    prompt: Optional[str] = None\n    optimize: Optional[bool] = None\n    synthesize_resolve: Optional[bool] = None\n    model: Optional[str] = None\n    input: Optional[Dict[str, Any]] = None\n    pass_through: Optional[bool] = None\n    associative: Optional[bool] = None\n    fold_prompt: Optional[str] = None\n    fold_batch_size: Optional[int] = None\n    value_sampling: Optional[Dict[str, Any]] = None\n    verbose: Optional[bool] = None\n    timeout: Optional[int] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.ParallelMapOp","title":"<code>docetl.schemas.ParallelMapOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class ParallelMapOp(BaseOp):\n    type: str = \"parallel_map\"\n    prompts: List[Dict[str, Any]]\n    output: Optional[Dict[str, Any]] = None\n    model: Optional[str] = None\n    optimize: Optional[bool] = None\n    recursively_optimize: Optional[bool] = None\n    sample_size: Optional[int] = None\n    drop_keys: Optional[List[str]] = None\n    timeout: Optional[int] = None\n    batch_size: Optional[int] = None\n    clustering_method: Optional[str] = None\n\n    @field_validator(\"drop_keys\")\n    def validate_drop_keys(cls, v):\n        if isinstance(v, str):\n            return [v]\n        return v\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.FilterOp","title":"<code>docetl.schemas.FilterOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class FilterOp(BaseOp):\n    type: str = \"filter\"\n    output: Optional[Dict[str, Any]] = None\n    prompt: Optional[str] = None\n    model: Optional[str] = None\n    optimize: Optional[bool] = None\n    recursively_optimize: Optional[bool] = None\n    sample_size: Optional[int] = None\n    validation_rules: Optional[List[str]] = Field(None, alias=\"validate\")\n    num_retries_on_validate_failure: Optional[int] = None\n    timeout: Optional[int] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.EquijoinOp","title":"<code>docetl.schemas.EquijoinOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class EquijoinOp(BaseOp):\n    type: str = \"equijoin\"\n    left: str\n    right: str\n    comparison_prompt: str\n    output: Optional[Dict[str, Any]] = None\n    blocking_threshold: Optional[float] = None\n    blocking_conditions: Optional[Dict[str, List[str]]] = None\n    limits: Optional[Dict[str, int]] = None\n    comparison_model: Optional[str] = None\n    optimize: Optional[bool] = None\n    embedding_model: Optional[str] = None\n    embedding_batch_size: Optional[int] = None\n    compare_batch_size: Optional[int] = None\n    limit_comparisons: Optional[int] = None\n    blocking_keys: Optional[Dict[str, List[str]]] = None\n    timeout: Optional[int] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.SplitOp","title":"<code>docetl.schemas.SplitOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class SplitOp(BaseOp):\n    type: str = \"split\"\n    split_key: str\n    method: str\n    method_kwargs: Dict[str, Any]\n    model: Optional[str] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.GatherOp","title":"<code>docetl.schemas.GatherOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class GatherOp(BaseOp):\n    type: str = \"gather\"\n    content_key: str\n    doc_id_key: str\n    order_key: str\n    peripheral_chunks: Dict[str, Any]\n    doc_header_key: Optional[str] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.UnnestOp","title":"<code>docetl.schemas.UnnestOp</code>","text":"<p>               Bases: <code>BaseOp</code></p> Source code in <code>docetl/schemas.py</code> <pre><code>class UnnestOp(BaseOp):\n    type: str = \"unnest\"\n    unnest_key: str\n    keep_empty: Optional[bool] = None\n    expand_fields: Optional[List[str]] = None\n    recursive: Optional[bool] = None\n    depth: Optional[int] = None\n</code></pre>"},{"location":"api-reference/python/#dataset-and-pipeline","title":"Dataset and Pipeline","text":""},{"location":"api-reference/python/#docetl.schemas.Dataset","title":"<code>docetl.schemas.Dataset</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a dataset configuration in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of the dataset. Must be either 'file' or 'memory'.</p> <code>path</code> <code>str</code> <p>The path to the dataset file or the in-memory data, depending on the type.</p> <code>source</code> <code>str</code> <p>The source of the dataset. Currently, only 'local' is supported. Defaults to 'local'.</p> <code>parsing</code> <code>Optional[List[Dict[str, str]]]</code> <p>A list of parsing tools to apply to the data. Each parsing tool                                       is represented by a dictionary with 'input_key', 'function', and                                       'output_key' keys. Defaults to None.</p> Example <pre><code>datasets:\n  my_dataset:\n    type: file\n    path: input.json\n    parsing:\n      - input_key: file_path\n        function: txt_to_string\n        output_key: content\n</code></pre> Note <p>The parsing tools are applied in the order they are listed. Each parsing tool takes the output of the previous tool as its input, allowing for chained processing of the data.</p> Source code in <code>docetl/schemas.py</code> <pre><code>class Dataset(BaseModel):\n    \"\"\"\n    Represents a dataset configuration in the pipeline.\n\n    Attributes:\n        type (str): The type of the dataset. Must be either 'file' or 'memory'.\n        path (str): The path to the dataset file or the in-memory data, depending on the type.\n        source (str): The source of the dataset. Currently, only 'local' is supported. Defaults to 'local'.\n        parsing (Optional[List[Dict[str, str]]]): A list of parsing tools to apply to the data. Each parsing tool\n                                                  is represented by a dictionary with 'input_key', 'function', and\n                                                  'output_key' keys. Defaults to None.\n\n    Example:\n        ```yaml\n        datasets:\n          my_dataset:\n            type: file\n            path: input.json\n            parsing:\n              - input_key: file_path\n                function: txt_to_string\n                output_key: content\n        ```\n\n    Note:\n        The parsing tools are applied in the order they are listed. Each parsing tool takes the output\n        of the previous tool as its input, allowing for chained processing of the data.\n    \"\"\"\n\n    type: str\n    path: str\n    source: str = \"local\"\n    parsing: Optional[List[Dict[str, str]]] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.ParsingTool","title":"<code>docetl.schemas.ParsingTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a parsing tool used for custom data parsing in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the parsing tool. This should be unique within the pipeline configuration.</p> <code>function_code</code> <code>str</code> <p>The Python code defining the parsing function. This code will be executed                  to parse the input data according to the specified logic. It should return a list of strings, where each string is its own document.</p> Example <pre><code>parsing_tools:\n  - name: ocr_parser\n    function_code: |\n      import pytesseract\n      from pdf2image import convert_from_path\n      def ocr_parser(filename: str) -&gt; List[str]:\n          images = convert_from_path(filename)\n          text = \"\"\n          for image in images:\n              text += pytesseract.image_to_string(image)\n          return [text]\n</code></pre> Source code in <code>docetl/schemas.py</code> <pre><code>class ParsingTool(BaseModel):\n    \"\"\"\n    Represents a parsing tool used for custom data parsing in the pipeline.\n\n    Attributes:\n        name (str): The name of the parsing tool. This should be unique within the pipeline configuration.\n        function_code (str): The Python code defining the parsing function. This code will be executed\n                             to parse the input data according to the specified logic. It should return a list of strings, where each string is its own document.\n\n    Example:\n        ```yaml\n        parsing_tools:\n          - name: ocr_parser\n            function_code: |\n              import pytesseract\n              from pdf2image import convert_from_path\n              def ocr_parser(filename: str) -&gt; List[str]:\n                  images = convert_from_path(filename)\n                  text = \"\"\n                  for image in images:\n                      text += pytesseract.image_to_string(image)\n                  return [text]\n        ```\n    \"\"\"\n\n    name: str\n    function_code: str\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.PipelineStep","title":"<code>docetl.schemas.PipelineStep</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a step in the pipeline.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the step.</p> <code>operations</code> <code>List[Union[Dict[str, Any], str]]</code> <p>A list of operations to be applied in this step. Each operation can be either a string (the name of the operation) or a dictionary (for more complex configurations).</p> <code>input</code> <code>Optional[str]</code> <p>The input for this step. It can be either the name of a dataset or the name of a previous step. If not provided, the step will use the output of the previous step as its input.</p> Example <pre><code># Simple step with a single operation\nprocess_step = PipelineStep(\n    name=\"process_step\",\n    input=\"my_dataset\",\n    operations=[\"process\"]\n)\n\n# Step with multiple operations\nsummarize_step = PipelineStep(\n    name=\"summarize_step\",\n    input=\"process_step\",\n    operations=[\"summarize\"]\n)\n\n# Step with a more complex operation configuration\ncustom_step = PipelineStep(\n    name=\"custom_step\",\n    input=\"previous_step\",\n    operations=[\n        {\n            \"custom_operation\": {\n                \"model\": \"gpt-4\",\n                \"prompt\": \"Perform a custom analysis on the following text:\"\n            }\n        }\n    ]\n)\n</code></pre> <p>These examples show different ways to configure pipeline steps, from simple single-operation steps to more complex configurations with custom parameters.</p> Source code in <code>docetl/schemas.py</code> <pre><code>class PipelineStep(BaseModel):\n    \"\"\"\n    Represents a step in the pipeline.\n\n    Attributes:\n        name (str): The name of the step.\n        operations (List[Union[Dict[str, Any], str]]): A list of operations to be applied in this step.\n            Each operation can be either a string (the name of the operation) or a dictionary\n            (for more complex configurations).\n        input (Optional[str]): The input for this step. It can be either the name of a dataset\n            or the name of a previous step. If not provided, the step will use the output\n            of the previous step as its input.\n\n    Example:\n        ```python\n        # Simple step with a single operation\n        process_step = PipelineStep(\n            name=\"process_step\",\n            input=\"my_dataset\",\n            operations=[\"process\"]\n        )\n\n        # Step with multiple operations\n        summarize_step = PipelineStep(\n            name=\"summarize_step\",\n            input=\"process_step\",\n            operations=[\"summarize\"]\n        )\n\n        # Step with a more complex operation configuration\n        custom_step = PipelineStep(\n            name=\"custom_step\",\n            input=\"previous_step\",\n            operations=[\n                {\n                    \"custom_operation\": {\n                        \"model\": \"gpt-4\",\n                        \"prompt\": \"Perform a custom analysis on the following text:\"\n                    }\n                }\n            ]\n        )\n        ```\n\n    These examples show different ways to configure pipeline steps, from simple\n    single-operation steps to more complex configurations with custom parameters.\n    \"\"\"\n\n    name: str\n    operations: List[Union[Dict[str, Any], str]]\n    input: Optional[str] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.schemas.PipelineOutput","title":"<code>docetl.schemas.PipelineOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the output configuration for a pipeline.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of output. This could be 'file', 'database', etc.</p> <code>path</code> <code>str</code> <p>The path where the output will be stored. This could be a file path,         database connection string, etc., depending on the type.</p> <code>intermediate_dir</code> <code>Optional[str]</code> <p>The directory to store intermediate results,                               if applicable. Defaults to None.</p> Example <pre><code>output = PipelineOutput(\n    type=\"file\",\n    path=\"/path/to/output.json\",\n    intermediate_dir=\"/path/to/intermediate/results\"\n)\n</code></pre> Source code in <code>docetl/schemas.py</code> <pre><code>class PipelineOutput(BaseModel):\n    \"\"\"\n    Represents the output configuration for a pipeline.\n\n    Attributes:\n        type (str): The type of output. This could be 'file', 'database', etc.\n        path (str): The path where the output will be stored. This could be a file path,\n                    database connection string, etc., depending on the type.\n        intermediate_dir (Optional[str]): The directory to store intermediate results,\n                                          if applicable. Defaults to None.\n\n    Example:\n        ```python\n        output = PipelineOutput(\n            type=\"file\",\n            path=\"/path/to/output.json\",\n            intermediate_dir=\"/path/to/intermediate/results\"\n        )\n        ```\n    \"\"\"\n\n    type: str\n    path: str\n    intermediate_dir: Optional[str] = None\n</code></pre>"},{"location":"api-reference/python/#docetl.api.Pipeline","title":"<code>docetl.api.Pipeline</code>","text":"<p>Represents a complete document processing pipeline.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the pipeline.</p> <code>datasets</code> <code>Dict[str, Dataset]</code> <p>A dictionary of datasets used in the pipeline,                            where keys are dataset names and values are Dataset objects.</p> <code>operations</code> <code>List[OpType]</code> <p>A list of operations to be performed in the pipeline.</p> <code>steps</code> <code>List[PipelineStep]</code> <p>A list of steps that make up the pipeline.</p> <code>output</code> <code>PipelineOutput</code> <p>The output configuration for the pipeline.</p> <code>parsing_tools</code> <code>List[ParsingTool]</code> <p>A list of parsing tools used in the pipeline.                                Defaults to an empty list.</p> <code>default_model</code> <code>Optional[str]</code> <p>The default language model to use for operations                            that require one. Defaults to None.</p> Example <pre><code>def custom_parser(text: str) -&gt; List[str]:\n    # this will convert the text in the column to uppercase\n    # You should return a list of strings, where each string is a separate document\n    return [text.upper()]\n\npipeline = Pipeline(\n    name=\"document_processing_pipeline\",\n    datasets={\n        \"input_data\": Dataset(type=\"file\", path=\"/path/to/input.json\", parsing=[{\"name\": \"custom_parser\", \"input_key\": \"content\", \"output_key\": \"uppercase_content\"}]),\n    },\n    parsing_tools=[custom_parser],\n    operations=[\n        MapOp(\n            name=\"process\",\n            type=\"map\",\n            prompt=\"Determine what type of document this is: {{ input.uppercase_content }}\",\n            output={\"schema\": {\"document_type\": \"string\"}}\n        ),\n        ReduceOp(\n            name=\"summarize\",\n            type=\"reduce\",\n            reduce_key=\"document_type\",\n            prompt=\"Summarize the processed contents: {% for item in inputs %}{{ item.uppercase_content }} {% endfor %}\",\n            output={\"schema\": {\"summary\": \"string\"}}\n        )\n    ],\n    steps=[\n        PipelineStep(name=\"process_step\", input=\"input_data\", operations=[\"process\"]),\n        PipelineStep(name=\"summarize_step\", input=\"process_step\", operations=[\"summarize\"])\n    ],\n    output=PipelineOutput(type=\"file\", path=\"/path/to/output.json\"),\n    default_model=\"gpt-4o-mini\"\n)\n</code></pre> <p>This example shows a complete pipeline configuration with datasets, operations, steps, and output settings.</p> Source code in <code>docetl/api.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    Represents a complete document processing pipeline.\n\n    Attributes:\n        name (str): The name of the pipeline.\n        datasets (Dict[str, Dataset]): A dictionary of datasets used in the pipeline,\n                                       where keys are dataset names and values are Dataset objects.\n        operations (List[OpType]): A list of operations to be performed in the pipeline.\n        steps (List[PipelineStep]): A list of steps that make up the pipeline.\n        output (PipelineOutput): The output configuration for the pipeline.\n        parsing_tools (List[ParsingTool]): A list of parsing tools used in the pipeline.\n                                           Defaults to an empty list.\n        default_model (Optional[str]): The default language model to use for operations\n                                       that require one. Defaults to None.\n\n    Example:\n        ```python\n        def custom_parser(text: str) -&gt; List[str]:\n            # this will convert the text in the column to uppercase\n            # You should return a list of strings, where each string is a separate document\n            return [text.upper()]\n\n        pipeline = Pipeline(\n            name=\"document_processing_pipeline\",\n            datasets={\n                \"input_data\": Dataset(type=\"file\", path=\"/path/to/input.json\", parsing=[{\"name\": \"custom_parser\", \"input_key\": \"content\", \"output_key\": \"uppercase_content\"}]),\n            },\n            parsing_tools=[custom_parser],\n            operations=[\n                MapOp(\n                    name=\"process\",\n                    type=\"map\",\n                    prompt=\"Determine what type of document this is: {{ input.uppercase_content }}\",\n                    output={\"schema\": {\"document_type\": \"string\"}}\n                ),\n                ReduceOp(\n                    name=\"summarize\",\n                    type=\"reduce\",\n                    reduce_key=\"document_type\",\n                    prompt=\"Summarize the processed contents: {% for item in inputs %}{{ item.uppercase_content }} {% endfor %}\",\n                    output={\"schema\": {\"summary\": \"string\"}}\n                )\n            ],\n            steps=[\n                PipelineStep(name=\"process_step\", input=\"input_data\", operations=[\"process\"]),\n                PipelineStep(name=\"summarize_step\", input=\"process_step\", operations=[\"summarize\"])\n            ],\n            output=PipelineOutput(type=\"file\", path=\"/path/to/output.json\"),\n            default_model=\"gpt-4o-mini\"\n        )\n        ```\n\n    This example shows a complete pipeline configuration with datasets, operations,\n    steps, and output settings.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        datasets: Dict[str, Dataset],\n        operations: List[OpType],\n        steps: List[PipelineStep],\n        output: PipelineOutput,\n        parsing_tools: List[Union[ParsingTool, Callable]] = [],\n        default_model: Optional[str] = None,\n        rate_limits: Optional[Dict[str, int]] = None,\n    ):\n        self.name = name\n        self.datasets = datasets\n        self.operations = operations\n        self.steps = steps\n        self.output = output\n        self.parsing_tools = [\n            (\n                tool\n                if isinstance(tool, ParsingTool)\n                else ParsingTool(\n                    name=tool.__name__, function_code=inspect.getsource(tool)\n                )\n            )\n            for tool in parsing_tools\n        ]\n        self.default_model = default_model\n        self.rate_limits = rate_limits\n        self._load_env()\n\n    def _load_env(self):\n        from dotenv import load_dotenv\n        import os\n\n        # Get the current working directory\n        cwd = os.getcwd()\n\n        # Load .env file from the current working directory if it exists\n        env_file = os.path.join(cwd, \".env\")\n        if os.path.exists(env_file):\n            load_dotenv(env_file)\n\n    def optimize(\n        self,\n        max_threads: Optional[int] = None,\n        model: str = \"gpt-4o\",\n        resume: bool = False,\n        timeout: int = 60,\n    ) -&gt; \"Pipeline\":\n        \"\"\"\n        Optimize the pipeline using the Optimizer.\n\n        Args:\n            max_threads (Optional[int]): Maximum number of threads to use for optimization.\n            model (str): The model to use for optimization. Defaults to \"gpt-4o\".\n            resume (bool): Whether to resume optimization from a previous state. Defaults to False.\n            timeout (int): Timeout for optimization in seconds. Defaults to 60.\n\n        Returns:\n            Pipeline: An optimized version of the pipeline.\n        \"\"\"\n        config = self._to_dict()\n        optimizer = Optimizer(\n            config,\n            base_name=os.path.join(os.getcwd(), self.name),\n            yaml_file_suffix=self.name,\n            max_threads=max_threads,\n            model=model,\n            timeout=timeout,\n            resume=resume,\n        )\n        optimizer.optimize()\n        optimized_config = optimizer.clean_optimized_config()\n\n        updated_pipeline = Pipeline(\n            name=self.name,\n            datasets=self.datasets,\n            operations=self.operations,\n            steps=self.steps,\n            output=self.output,\n            default_model=self.default_model,\n            parsing_tools=self.parsing_tools,\n        )\n        updated_pipeline._update_from_dict(optimized_config)\n        return updated_pipeline\n\n    def run(self, max_threads: Optional[int] = None) -&gt; float:\n        \"\"\"\n        Run the pipeline using the DSLRunner.\n\n        Args:\n            max_threads (Optional[int]): Maximum number of threads to use for execution.\n\n        Returns:\n            float: The total cost of running the pipeline.\n        \"\"\"\n        config = self._to_dict()\n        runner = DSLRunner(config, max_threads=max_threads)\n        result = runner.run()\n        return result\n\n    def to_yaml(self, path: str) -&gt; None:\n        \"\"\"\n        Convert the Pipeline object to a YAML string and save it to a file.\n\n        Args:\n            path (str): Path to save the YAML file.\n\n        Returns:\n            None\n        \"\"\"\n        config = self._to_dict()\n        with open(path, \"w\") as f:\n            yaml.safe_dump(config, f)\n\n        print(f\"[green]Pipeline saved to {path}[/green]\")\n\n    def _to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert the Pipeline object to a dictionary representation.\n\n        Returns:\n            Dict[str, Any]: Dictionary representation of the Pipeline.\n        \"\"\"\n        d = {\n            \"datasets\": {\n                name: dataset.dict() for name, dataset in self.datasets.items()\n            },\n            \"operations\": [\n                {k: v for k, v in op.dict().items() if v is not None}\n                for op in self.operations\n            ],\n            \"pipeline\": {\n                \"steps\": [\n                    {k: v for k, v in step.dict().items() if v is not None}\n                    for step in self.steps\n                ],\n                \"output\": self.output.dict(),\n            },\n            \"default_model\": self.default_model,\n            \"parsing_tools\": (\n                [tool.dict() for tool in self.parsing_tools]\n                if self.parsing_tools\n                else None\n            ),\n        }\n        if self.rate_limits:\n            d[\"rate_limits\"] = self.rate_limits\n        return d\n\n    def _update_from_dict(self, config: Dict[str, Any]):\n        \"\"\"\n        Update the Pipeline object from a dictionary representation.\n\n        Args:\n            config (Dict[str, Any]): Dictionary representation of the Pipeline.\n        \"\"\"\n        self.datasets = {\n            name: Dataset(\n                type=dataset[\"type\"],\n                source=dataset[\"source\"],\n                path=dataset[\"path\"],\n                parsing=dataset.get(\"parsing\"),\n            )\n            for name, dataset in config[\"datasets\"].items()\n        }\n        self.operations = []\n        for op in config[\"operations\"]:\n            op_type = op.pop(\"type\")\n            if op_type == \"map\":\n                self.operations.append(MapOp(**op, type=op_type))\n            elif op_type == \"resolve\":\n                self.operations.append(ResolveOp(**op, type=op_type))\n            elif op_type == \"reduce\":\n                self.operations.append(ReduceOp(**op, type=op_type))\n            elif op_type == \"parallel_map\":\n                self.operations.append(ParallelMapOp(**op, type=op_type))\n            elif op_type == \"filter\":\n                self.operations.append(FilterOp(**op, type=op_type))\n            elif op_type == \"equijoin\":\n                self.operations.append(EquijoinOp(**op, type=op_type))\n            elif op_type == \"split\":\n                self.operations.append(SplitOp(**op, type=op_type))\n            elif op_type == \"gather\":\n                self.operations.append(GatherOp(**op, type=op_type))\n            elif op_type == \"unnest\":\n                self.operations.append(UnnestOp(**op, type=op_type))\n        self.steps = [PipelineStep(**step) for step in config[\"pipeline\"][\"steps\"]]\n        self.output = PipelineOutput(**config[\"pipeline\"][\"output\"])\n        self.default_model = config.get(\"default_model\")\n        self.parsing_tools = (\n            [ParsingTool(**tool) for tool in config.get(\"parsing_tools\", [])]\n            if config.get(\"parsing_tools\")\n            else []\n        )\n</code></pre>"},{"location":"api-reference/python/#docetl.api.Pipeline.optimize","title":"<code>optimize(max_threads=None, model='gpt-4o', resume=False, timeout=60)</code>","text":"<p>Optimize the pipeline using the Optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>max_threads</code> <code>Optional[int]</code> <p>Maximum number of threads to use for optimization.</p> <code>None</code> <code>model</code> <code>str</code> <p>The model to use for optimization. Defaults to \"gpt-4o\".</p> <code>'gpt-4o'</code> <code>resume</code> <code>bool</code> <p>Whether to resume optimization from a previous state. Defaults to False.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>Timeout for optimization in seconds. Defaults to 60.</p> <code>60</code> <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>An optimized version of the pipeline.</p> Source code in <code>docetl/api.py</code> <pre><code>def optimize(\n    self,\n    max_threads: Optional[int] = None,\n    model: str = \"gpt-4o\",\n    resume: bool = False,\n    timeout: int = 60,\n) -&gt; \"Pipeline\":\n    \"\"\"\n    Optimize the pipeline using the Optimizer.\n\n    Args:\n        max_threads (Optional[int]): Maximum number of threads to use for optimization.\n        model (str): The model to use for optimization. Defaults to \"gpt-4o\".\n        resume (bool): Whether to resume optimization from a previous state. Defaults to False.\n        timeout (int): Timeout for optimization in seconds. Defaults to 60.\n\n    Returns:\n        Pipeline: An optimized version of the pipeline.\n    \"\"\"\n    config = self._to_dict()\n    optimizer = Optimizer(\n        config,\n        base_name=os.path.join(os.getcwd(), self.name),\n        yaml_file_suffix=self.name,\n        max_threads=max_threads,\n        model=model,\n        timeout=timeout,\n        resume=resume,\n    )\n    optimizer.optimize()\n    optimized_config = optimizer.clean_optimized_config()\n\n    updated_pipeline = Pipeline(\n        name=self.name,\n        datasets=self.datasets,\n        operations=self.operations,\n        steps=self.steps,\n        output=self.output,\n        default_model=self.default_model,\n        parsing_tools=self.parsing_tools,\n    )\n    updated_pipeline._update_from_dict(optimized_config)\n    return updated_pipeline\n</code></pre>"},{"location":"api-reference/python/#docetl.api.Pipeline.run","title":"<code>run(max_threads=None)</code>","text":"<p>Run the pipeline using the DSLRunner.</p> <p>Parameters:</p> Name Type Description Default <code>max_threads</code> <code>Optional[int]</code> <p>Maximum number of threads to use for execution.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The total cost of running the pipeline.</p> Source code in <code>docetl/api.py</code> <pre><code>def run(self, max_threads: Optional[int] = None) -&gt; float:\n    \"\"\"\n    Run the pipeline using the DSLRunner.\n\n    Args:\n        max_threads (Optional[int]): Maximum number of threads to use for execution.\n\n    Returns:\n        float: The total cost of running the pipeline.\n    \"\"\"\n    config = self._to_dict()\n    runner = DSLRunner(config, max_threads=max_threads)\n    result = runner.run()\n    return result\n</code></pre>"},{"location":"api-reference/python/#docetl.api.Pipeline.to_yaml","title":"<code>to_yaml(path)</code>","text":"<p>Convert the Pipeline object to a YAML string and save it to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the YAML file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>docetl/api.py</code> <pre><code>def to_yaml(self, path: str) -&gt; None:\n    \"\"\"\n    Convert the Pipeline object to a YAML string and save it to a file.\n\n    Args:\n        path (str): Path to save the YAML file.\n\n    Returns:\n        None\n    \"\"\"\n    config = self._to_dict()\n    with open(path, \"w\") as f:\n        yaml.safe_dump(config, f)\n\n    print(f\"[green]Pipeline saved to {path}[/green]\")\n</code></pre>"},{"location":"community/","title":"Community","text":"<p>Welcome to the DocETL community! We're excited to have you join us in exploring and improving document extraction and transformation workflows. We are committed to fostering an inclusive community for all people, regardless of technical background.</p>"},{"location":"community/#code-of-conduct","title":"Code of Conduct","text":"<p>While we don't have a formal code of conduct page, we expect all community members to treat each other with respect and kindness. We do not tolerate harassment or discrimination of any kind. If you experience any issues, please reach out to the project maintainers immediately.</p>"},{"location":"community/#contributions","title":"Contributions","text":"<p>We welcome contributions from everyone who is interested in improving DocETL. Here's how you can get involved:</p> <ol> <li> <p>Report Issues: If you encounter a bug or have a feature request, open an issue on our GitHub repository.</p> </li> <li> <p>Join Discussions: Have a question or want to discuss ideas? Post on our Discord server.</p> </li> <li> <p>Contribute Code: Look for issues tagged with \"help wanted\" or \"good first issue\" on GitHub. These are great starting points for new contributors.</p> </li> <li> <p>Join Working Groups: We will create working groups in Discord focused on different project areas as discussed in our roadmap. Join the group(s) that interests you most!</p> </li> </ol> <p>To contribute code:</p> <ol> <li>Fork the repository on GitHub.</li> <li>Create a new branch for your changes.</li> <li>Make your changes in your branch.</li> <li>Submit a pull request with your changes.</li> </ol>"},{"location":"community/#connect-with-us","title":"Connect with Us","text":"<ul> <li>GitHub Repository: Contribute to the project or report issues on our GitHub repo.</li> <li>Discord Community: Join our Discord server; we're looking to build a vibrant community of people interested in intelligent document processing.</li> <li>Lab Webpages: We are affiliated with the EPIC Lab at UC Berkeley. Visit our Lab Page for a description of our research. We are also affiliated with the Data Systems and Foundations group at UC Berkeley--visit our DSF Page for more information.</li> </ul> <p>Request a Tutorial or Research Talk</p> <p>Interested in having us give a tutorial or research talk on DocETL? We'd love to connect! Please email shreyashankar@berkeley.edu to set up a time. Let us know what your team is interested in learning about (e.g., tutorial or research) so we can tailor the presentation to your interests.</p>"},{"location":"community/#frequently-encountered-issues","title":"Frequently Encountered Issues","text":""},{"location":"community/#keyerror-in-operations","title":"KeyError in Operations","text":"<p>If you're encountering a KeyError, it's often due to missing an unnest operation in your workflow. The unnest operation is crucial for flattening nested data structures.</p> <p>Solution: Add an unnest operation to your pipeline before accessing nested keys. If you're still having trouble, don't hesitate to open an issue on GitHub or ask for help on our Discord server.</p>"},{"location":"community/roadmap/","title":"Roadmap","text":"<p>Join Our Working Groups</p> <p>Are you interested in contributing to any of these projects or have ideas for new areas of exploration? Join our Discord server to participate in our working groups and collaborate with the community!</p> <p>We're constantly working to improve DocETL and explore new possibilities in document processing. Our current ideas span both research and engineering problems, and are organized into the following categories:</p> <pre><code>mindmap\n  root((DocETL Roadmap))\n    User Interface and Interaction\n    Debugging and Optimization\n    Data Handling and Storage\n    Model and Tool Integrations\n    Agents and Planning</code></pre>"},{"location":"community/roadmap/#user-interface-and-interaction","title":"User Interface and Interaction","text":"<ul> <li>Natural Language to DocETL Pipeline: Building tools to generate DocETL pipelines from natural language descriptions.</li> <li>Interactive Pipeline Creation: Developing intuitive interfaces for creating and optimizing DocETL pipelines interactively.</li> </ul>"},{"location":"community/roadmap/#debugging-and-optimization","title":"Debugging and Optimization","text":"<ul> <li>DocETL Debugger: Creating a debugger with provenance tracking, allowing users to visualize all intermediates that contributed to a specific output.</li> <li>Plan Efficiency Optimization: Implementing strategies (and devising new strategies) to reduce latency and cost for the most accurate plans. This includes batching LLM calls, using model cascades, and fusing operators.</li> </ul>"},{"location":"community/roadmap/#data-handling-and-storage","title":"Data Handling and Storage","text":"<ul> <li>Comprehensive Data Loading: Expanding support beyond JSON to include formats like CSV and Apache Arrow, as well as loading from the cloud.</li> <li>New Storage Formats: Exploring a specialized storage format for unstructured data and documents, particularly suited for pipeline intermediates. For example, tokens that do not contribute much to the final output can be compressed further.</li> </ul>"},{"location":"community/roadmap/#model-and-tool-integrations","title":"Model and Tool Integrations","text":"<ul> <li>Model Diversity: Extending support beyond OpenAI to include a wider range of models, with a focus on local models.</li> <li>OCR and PDF Extraction: Improving integration with OCR technologies and PDF extraction tools for more robust document processing.</li> <li>Multimodal Data Processing: Enhancing DocETL to handle multimodal data, including text, images, audio, and video (as many of the LLMs support multimodal inputs, anyways).</li> </ul>"},{"location":"community/roadmap/#agents-and-planning","title":"Agents and Planning","text":"<ul> <li> <p>Smarter Agent and Planning Architectures: Optimizing plan exploration based on data characteristics. For instance, refining the optimizer to avoid unnecessary exploration of plans with the gather operator for tasks that don't require peripheral context when decomposing map operations for large documents.</p> </li> <li> <p>Context-Aware Sampling for Validation: Creating algorithms that can identify and extract the most representative samples from different parts of a document, including the beginning, middle, and end, to use in validaton prompts. This approach will help validation agents to verify that all sections of documents are adequately represented in the outputs, avoiding blind spots in the analysis due to truncation--as we currently naive truncate the middle of documents in validation prompts.</p> </li> <li> <p>Benchmarks: Developing a suite of benchmarks to evaluate the performance of different optimization strategies and agent architectures. These benchmarks will help us understand the trade-offs between accuracy, efficiency, and cost in different scenarios, guiding the development of more effective optimization techniques.</p> </li> </ul>"},{"location":"concepts/operators/","title":"Operators","text":"<p>Operators in DocETL are designed for semantically processing unstructured data. They form the building blocks of data processing pipelines, allowing you to transform, analyze, and manipulate datasets efficiently.</p>"},{"location":"concepts/operators/#overview","title":"Overview","text":"<ul> <li>Datasets contain documents, where a document is an object in the JSON list, with fields and values.</li> <li>DocETL provides several operators, each tailored for specific unstructured data processing tasks.</li> <li>By default, operations are parallelized on your data using multithreading for improved performance.</li> </ul> <p>Caching in DocETL</p> <p>DocETL employs caching for all LLM calls and partially-optimized plans. The cache is stored in the <code>.docetl/cache</code> and <code>.docetl/llm_cache</code> directories within your home directory. This caching mechanism helps to improve performance and reduce redundant API calls when running similar operations or reprocessing data.</p>"},{"location":"concepts/operators/#common-attributes","title":"Common Attributes","text":"<p>All operators share some common attributes:</p> <ul> <li><code>name</code>: A unique identifier for the operator.</li> <li><code>type</code>: Specifies the type of operation (e.g., \"map\", \"reduce\", \"filter\").</li> </ul> <p>LLM-based operators have additional attributes:</p> <ul> <li><code>prompt</code>: A Jinja2 template that defines the instruction for the language model.</li> <li><code>output</code>: Specifies the schema for the output from the LLM call.</li> <li><code>model</code> (optional): Allows specifying a different model from the pipeline default.</li> </ul> <p>Example:</p> <pre><code>- name: extract_insights\n  type: map\n  model: gpt-4o\n  prompt: |\n    Analyze the following user interaction log:\n    {{ input.log }}\n\n    Extract 2-3 main insights from this log, each being 1-2 words, to help inform future product development. Consider any difficulties or pain points the user may have had. Also provide 1-2 supporting actions for each insight.\n    Return the results as a list of dictionaries, each containing 'insight' and 'supporting_actions' keys.\n  output:\n    schema:\n      insights: \"list[{insight: string, supporting_actions: list[string]}]\"\n</code></pre>"},{"location":"concepts/operators/#input-and-output","title":"Input and Output","text":"<p>Prompts can reference any fields in the data, including:</p> <ul> <li>Original fields from the input data.</li> <li>Fields synthesized by previous operations in the pipeline.</li> </ul> <p>For map operations, you can only reference <code>input</code>, but in reduce operations, you can reference <code>inputs</code> (since it's a list of inputs).</p> <p>Example:</p> <pre><code>prompt: |\n  Summarize the user behavior insights for the country: {{ inputs[0].country }}\n\n  Insights and supporting actions:\n  {% for item in inputs %}\n  - Insight: {{ item.insight }}\n  Supporting actions:\n  {% for action in item.supporting_actions %}\n  - {{ action }}\n  {% endfor %}\n  {% endfor %}\n</code></pre> <p>What happens if the input is too long?</p> <p>When the input data exceeds the token limit of the LLM, DocETL automatically truncates tokens from the middle of the data to make it fit in the prompt. This approach preserves the beginning and end of the input, which often contain crucial context.</p> <p>A warning is displayed whenever truncation occurs, alerting you to potential loss of information:</p> <pre><code>WARNING: Input exceeded token limit. Truncated 500 tokens from the middle of the input.\n</code></pre> <p>If you frequently encounter this warning, consider using DocETL's optimizer or breaking down your input yourself into smaller chunks to handle large inputs more effectively.</p>"},{"location":"concepts/operators/#output-schema","title":"Output Schema","text":"<p>The <code>output</code> attribute defines the structure of the LLM's response. It supports various data types:</p> <ul> <li><code>string</code> (or <code>str</code>, <code>text</code>, <code>varchar</code>): For text data</li> <li><code>integer</code> (or <code>int</code>): For whole numbers</li> <li><code>number</code> (or <code>float</code>, <code>decimal</code>): For decimal numbers</li> <li><code>boolean</code> (or <code>bool</code>): For true/false values</li> <li><code>list</code>: For arrays or sequences of items</li> <li>objects: Using notation <code>{field: type}</code></li> </ul> <p>Example:</p> <pre><code>output:\n  schema:\n    insights: \"list[{insight: string, supporting_actions: string}]\"\n    detailed_summary: string\n</code></pre> <p>Keep Output Types Simple</p> <p>It's recommended to keep output types as simple as possible. Complex nested structures may be difficult for the LLM to consistently produce, potentially leading to parsing errors. The structured output feature works best with straightforward schemas. If you need complex data structures, consider breaking them down into multiple simpler operations.</p> <p>For example, instead of: <pre><code>output:\n  schema:\n    insights: \"list[{insight: string, supporting_actions: list[{action: string, priority: integer}]}]\"\n</code></pre></p> <p>Consider: <pre><code>output:\n  schema:\n    insights: \"list[{insight: string, supporting_actions: string}]\"\n</code></pre></p> <p>And then use a separate operation to further process the supporting actions if needed.</p> <p>Read more about schemas in the schemas section.</p>"},{"location":"concepts/operators/#validation","title":"Validation","text":"<p>Validation is a first-class citizen in DocETL, ensuring the quality and correctness of processed data.</p>"},{"location":"concepts/operators/#basic-validation","title":"Basic Validation","text":"<p>LLM-based operators can include a <code>validate</code> field, which accepts a list of Python statements:</p> <pre><code>validate:\n  - len(output[\"insights\"]) &gt;= 2\n  - all(len(insight[\"supporting_actions\"]) &gt;= 1 for insight in output[\"insights\"])\n</code></pre> <p>Access variables using dictionary syntax: <code>output[\"field\"]</code>. Note that you can't access <code>input</code> docs in validation, but the output docs should have all the fields from the input docs (for non-reduce operations), since fields pass through unchanged.</p> <p>The <code>num_retries_on_validate_failure</code> attribute specifies how many times to retry the LLM if any validation statements fail.</p>"},{"location":"concepts/operators/#advanced-validation-gleaning","title":"Advanced Validation: Gleaning","text":"<p>Gleaning is an advanced validation technique that uses LLM-based validators to refine outputs iteratively.</p> <p>To enable gleaning, specify:</p> <ul> <li><code>validation_prompt</code>: Instructions for the LLM to evaluate and improve the output.</li> <li><code>num_rounds</code>: The maximum number of refinement iterations.</li> </ul> <p>Example:</p> <pre><code>gleaning:\n  num_rounds: 1\n  validation_prompt: |\n    Evaluate the extraction for completeness and relevance:\n    1. Are all key user behaviors and pain points from the log addressed in the insights?\n    2. Are the supporting actions practical and relevant to the insights?\n    3. Is there any important information missing or any irrelevant information included?\n</code></pre> <p>This approach allows for context-aware validation and refinement of LLM outputs. Note that it is expensive, since it at least doubles the number of LLM calls required for each operator.</p>"},{"location":"concepts/operators/#how-gleaning-works","title":"How Gleaning Works","text":"<p>Gleaning is an iterative process that refines LLM outputs using context-aware validation. Here's how it works:</p> <ol> <li> <p>Initial Operation: The LLM generates an initial output based on the original operation prompt.</p> </li> <li> <p>Validation: The validation prompt is appended to the chat thread, along with the original operation prompt and output. This is submitted to the LLM. Note that the validation prompt doesn't need any variables, since it's appended to the chat thread.</p> </li> <li> <p>Assessment: The LLM responds with an assessment of the output according to the validation prompt.</p> </li> <li> <p>Decision: The system interprets the assessment:</p> </li> <li> <p>If there's no error or room for improvement, the current output is returned.</p> </li> <li> <p>If improvements are suggested, the process continues.</p> </li> <li> <p>Refinement: If improvements are needed:</p> </li> <li> <p>A new prompt is created, including the original operation prompt, the original output, and the validator feedback.</p> </li> <li> <p>This is submitted to the LLM to generate an improved output.</p> </li> <li> <p>Iteration: Steps 2-5 are repeated until either:</p> </li> <li> <p>The validator has no more feedback (i.e., the evaluation passes), or</p> </li> <li> <p>The number of iterations exceeds <code>num_rounds</code>.</p> </li> <li> <p>Final Output: The last refined output is returned.</p> </li> </ol> <p>This process allows for nuanced, context-aware validation and refinement of LLM outputs. It's particularly useful for complex tasks where simple rule-based validation might miss subtleties or context-dependent aspects of the output.</p> <p>Note that gleaning can significantly increase the number of LLM calls for each operator, potentially doubling it at minimum. While this increases cost and latency, it can lead to higher quality outputs for complex tasks.</p>"},{"location":"concepts/optimization/","title":"Optimization","text":"<p>Sometimes, finding the optimal pipeline for your task can be challenging. You might wonder:</p> <p>Questions</p> <ul> <li>Will a single LLM call suffice for your task?</li> <li>Do you need to decompose your task or data further for better results?</li> </ul> <p>To address these questions and improve your pipeline's performance, you can use DocETL to build an optimized version of your pipeline.</p>"},{"location":"concepts/optimization/#the-docetl-optimizer","title":"The DocETL Optimizer","text":"<p>The DocETL optimizer is designed to decompose operators (and sequences of operators) into their own subpipelines, potentially leading to higher accuracy.</p> <p>Example</p> <p>A map operation attempting to perform multiple tasks can be decomposed into separate map operations, ultimately improving overall accuracy. For example, consider a map operation on student survey responses that tries to:</p> <ol> <li>Extract actionable suggestions for course improvement</li> <li>Identify potential interdisciplinary connections</li> </ol> <p>This could be optimized into two separate map operations:</p> <ul> <li>Suggestion Extraction:   Focus solely on identifying concrete, actionable suggestions for improving the course.</li> </ul> <pre><code>prompt: |\n  From this student survey response, extract any specific, actionable suggestions\n  for improving the course. If no suggestions are present, output 'No suggestions found.':\n  '{{ input.response }}'\n</code></pre> <ul> <li>Interdisciplinary Connection Analysis:   Analyze the response for mentions of concepts or ideas that could connect to other disciplines or courses.</li> </ul> <pre><code>prompt: |\n  Identify any concepts or ideas in this student survey response that could have\n  interdisciplinary connections. For each connection, specify the related discipline or course:\n  '{{ input.response }}'\n</code></pre> <p>By breaking these tasks into separate operations, each LLM call can focus on a specific aspect of the analysis. This specialization might lead to more accurate results, depending on the LLM, data, and nature of the task!</p>"},{"location":"concepts/optimization/#how-it-works","title":"How It Works","text":"<p>The DocETL optimizer operates using the following mechanism:</p> <ol> <li> <p>Generation and Evaluation Agents: These agents generate different plans for the pipeline according to predefined rewrite rules. Evaluation agents then compare plans and outputs to determine the best approach.</p> </li> <li> <p>Operator Rewriting: The optimizer looks through operators in your pipeline where you've set optimize: true, and attempts to rewrite them using predefined rules.</p> </li> <li> <p>Output: After optimization, DocETL outputs a new YAML file representing the optimized pipeline.</p> </li> </ol>"},{"location":"concepts/optimization/#using-the-optimizer","title":"Using the Optimizer","text":"<p>You can invoke the optimizer using the following command:</p> <pre><code>docetl build your_pipeline.yaml\n</code></pre> <p>This command will save the optimized pipeline to <code>your_pipeline_opt.yaml</code>. Note that the optimizer will only rewrite operators where you've set <code>optimize: true</code>. Leaving this field unset will skip optimization for that operator.</p>"},{"location":"concepts/pipelines/","title":"Pipelines","text":"<p>Pipelines in DocETL are the core structures that define the flow of data processing. They orchestrate the application of operators to datasets, creating a seamless workflow for complex document processing tasks.</p>"},{"location":"concepts/pipelines/#components-of-a-pipeline","title":"Components of a Pipeline","text":"<p>A pipeline in DocETL consists of four main components:</p> <ol> <li>Default Model: The language model to use for the pipeline.</li> <li>Datasets: The input data sources for your pipeline.</li> <li>Operators: The processing steps that transform your data.</li> <li>Pipeline Specification: The sequence of steps and the output configuration.</li> </ol>"},{"location":"concepts/pipelines/#default-model","title":"Default Model","text":"<p>You can set the default model for a pipeline in the YAML configuration file. If no model is specified at the operation level, the default model will be used.</p> <pre><code>default_model: gpt-4o-mini\n</code></pre>"},{"location":"concepts/pipelines/#datasets","title":"Datasets","text":"<p>Datasets define the input data for your pipeline. They are collections of documents, where each document is an object in a JSON list (or row in a CSV file). Datasets are typically specified in the YAML configuration file, indicating the type and path of the data source. For example:</p> <pre><code>datasets:\n  user_logs:\n    type: file\n    path: \"user_logs.json\"\n</code></pre>"},{"location":"concepts/pipelines/#dynamic-data-loading","title":"Dynamic Data Loading","text":"<p>DocETL supports dynamic data loading, allowing you to process various file types by specifying a key that points to a path or using a custom parsing function. This feature is particularly useful for handling diverse data sources, such as audio files, PDFs, or any other non-standard format.</p> <p>To implement dynamic data loading, you can use parsing tools in your dataset configuration. Here's an example:</p> <pre><code>datasets:\n  audio_transcripts:\n    type: file\n    source: local\n    path: \"audio_files/audio_paths.json\"\n    parsing_tools:\n      - input_key: audio_path\n        function: whisper_speech_to_text\n        output_key: transcript\n</code></pre> <p>In this example, the dataset configuration specifies a JSON file (audio_paths.json) that contains paths to audio files. The parsing_tools section defines how to process these files:</p> <ul> <li><code>input_key</code>: Specifies which key in the JSON contains the path to the audio file. In this example, each object in the dataset should have a \"audio_path\" key, that represents a path to an audio file or mp3.</li> <li><code>function</code>: Names the parsing function to use (in this case, the built-in whisper_speech_to_text function for audio transcription).</li> <li><code>output_key</code>: Defines the key where the processed data (transcript) will be stored. You can access this in the pipeline in any prompts with the <code>{{ input.transcipt }}</code> syntax.</li> </ul> <p>This approach allows DocETL to dynamically load and process various file types, extending its capabilities beyond standard JSON or CSV inputs. You can use built-in parsing tools or define custom ones to handle specific file formats or data processing needs. See the Custom Parsing documentation for more details.</p> <p>Note</p> <p>Currently, DocETL only supports JSON files or CSV files as input datasets. If you're interested in support for other data types or cloud-based datasets, please reach out to us or join our open-source community and contribute! We welcome new ideas and contributions to expand the capabilities of DocETL.</p>"},{"location":"concepts/pipelines/#operators","title":"Operators","text":"<p>Operators are the building blocks of your pipeline, defining the transformations and analyses to be performed on your data. They are detailed in the Operators documentation. Operators can include map, reduce, filter, and other types of operations.</p>"},{"location":"concepts/pipelines/#pipeline-specification","title":"Pipeline Specification","text":"<p>The pipeline specification outlines the sequence of steps to be executed and the final output configuration. It typically includes:</p> <ul> <li>Steps: The sequence of operations to be applied to the data.</li> <li>Output: The configuration for the final output of the pipeline.</li> </ul> <p>For example:</p> <pre><code>pipeline:\n  steps:\n    - name: analyze_user_logs\n      input: user_logs\n      operations:\n        - extract_insights\n        - unnest_insights\n        - summarize_by_country\n  output:\n    type: file\n    path: \"country_summaries.json\"\n</code></pre> <p>For a practical example of how these components come together, refer to the Tutorial, which demonstrates a complete pipeline for analyzing user behavior data.</p>"},{"location":"concepts/schemas/","title":"Schemas","text":"<p>In DocETL, schemas play an important role in defining the structure of output from LLM operations. Every LLM call in DocETL is associated with an output schema, which specifies the expected format and types of the output data.</p>"},{"location":"concepts/schemas/#overview","title":"Overview","text":"<ul> <li>Schemas define the structure and types of output data from LLM operations.</li> <li>They help ensure consistency and facilitate downstream processing.</li> <li>DocETL uses structured outputs or tool API to enforce these schemas.</li> </ul> <p>Schema Simplicity</p> <p>We've observed that the more complex the output schema is, the worse the quality of the output tends to be. Keep your schemas as simple as possible for better results.</p>"},{"location":"concepts/schemas/#defining-schemas","title":"Defining Schemas","text":"<p>Schemas are defined in the <code>output</code> section of an operator. They support various data types:</p> Type Aliases Description <code>string</code> <code>str</code>, <code>text</code>, <code>varchar</code> For text data <code>integer</code> <code>int</code> For whole numbers <code>number</code> <code>float</code>, <code>decimal</code> For decimal numbers <code>boolean</code> <code>bool</code> For true/false values <code>list</code> - For arrays or sequences of items (must specify element type) Objects - Using notation <code>{field: type}</code> <p>Filter Operation Schemas</p> <p>Filter operation schemas must have a boolean type output field. This is used to determine whether each item should be included or excluded based on the filter criteria.</p>"},{"location":"concepts/schemas/#examples","title":"Examples","text":""},{"location":"concepts/schemas/#simple-schema","title":"Simple Schema","text":"<pre><code>output:\n  schema:\n    summary: string\n    sentiment: string\n    include_item: boolean # For filter operations\n</code></pre>"},{"location":"concepts/schemas/#complex-schema","title":"Complex Schema","text":"<pre><code>output:\n  schema:\n    insights: \"list[{insight: string, confidence: number}]\"\n    metadata: \"{timestamp: string, source: string}\"\n</code></pre>"},{"location":"concepts/schemas/#lists-and-objects","title":"Lists and Objects","text":"<p>Lists in schemas must specify their element type:</p> <ul> <li><code>list[string]</code>: A list of strings</li> <li><code>list[int]</code>: A list of integers</li> <li><code>list[{name: string, age: integer}]</code>: A list of objects</li> </ul> <p>Objects are defined using curly braces and must have typed fields:</p> <ul> <li><code>{name: string, age: integer, is_active: boolean}</code></li> </ul> <p>Complex List Example</p> <pre><code>output:\n  schema:\n    users: \"list[{name: string, age: integer, hobbies: list[string]}]\"\n</code></pre> <p>Make sure that you put the type in quotation marks, if it references an object type (i.e., has curly braces)! Otherwise the yaml won't compile!</p>"},{"location":"concepts/schemas/#structured-outputs-and-tool-api","title":"Structured Outputs and Tool API","text":"<p>DocETL uses structured outputs or tool API to enforce schema typing. This ensures that the LLM outputs adhere to the specified schema, making the results more consistent and easier to process in subsequent operations.</p>"},{"location":"concepts/schemas/#best-practices","title":"Best Practices","text":"<ol> <li>Keep output fields simple and use string types whenever possible.</li> <li>Only use structured fields (like lists and objects) when necessary for downstream analysis or reduce operations.</li> <li>If you need to reference structured fields in downstream operations, consider breaking complex structures into multiple simpler operations.</li> </ol> <p>Schema Optimization</p> <p>If you find your schema becoming too complex, consider breaking it down into multiple operations. This can improve both the quality of LLM outputs and the manageability of your pipeline.</p> <p>Breaking Down Complex Schemas</p> <p>Instead of: <pre><code>output:\n  schema:\n    summary: string\n    key_points: \"list[{point: string, sentiment: string}]\"\n</code></pre></p> <p>Consider: <pre><code>output:\n  schema:\n    summary: string\n    key_points: \"string\"\n</code></pre></p> <p>Where in the prompt you can say something like: <code>In your key points, please include the sentiment of each point.</code></p> <p>The only reason to use the complex schema is if you need to do an operation at the point level, like resolve them and reduce on them.</p> <p>By following these guidelines and best practices, you can create effective schemas that enhance the performance and reliability of your DocETL operations.</p>"},{"location":"examples/annotating-legal-documents/","title":"Annotating legal documents","text":"<p>TODO</p>"},{"location":"examples/characterizing-troll-behavior/","title":"Characterizing troll behavior","text":"<p>TODO</p>"},{"location":"examples/custom-parsing/","title":"Pointing to External Data and Custom Parsing","text":"<p>In DocETL, you have full control over your dataset JSONs. These JSONs typically contain objects with key-value pairs, where you can reference external files that you want to process in your pipeline. This referencing mechanism, which we call \"pointing\", allows DocETL to locate and process external files that require special handling before they can be used in your main pipeline.</p> <p>Why Use Custom Parsing?</p> <p>Consider these scenarios where custom parsing of referenced files is beneficial:</p> <ul> <li>Your dataset JSON references Excel spreadsheets containing sales data.</li> <li>You have entries pointing to scanned receipts in PDF format that need OCR processing.</li> <li>You want to extract text from Word documents or PowerPoint presentations by referencing their file locations.</li> </ul> <p>In these cases, custom parsing enables you to transform your raw external data into a format that DocETL can process effectively within your pipeline. The pointing mechanism allows DocETL to locate these external files and apply custom parsing seamlessly. (Pointing in DocETL refers to the practice of including references or paths to external files within your dataset JSON. Instead of embedding the entire content of these files, you simply \"point\" to their locations, allowing DocETL to access and process them as needed during the pipeline execution.)</p>"},{"location":"examples/custom-parsing/#dataset-json-example","title":"Dataset JSON Example","text":"<p>Let's look at a typical dataset JSON file that you might create:</p> <pre><code>[\n  { \"id\": 1, \"excel_path\": \"sales_data/january_sales.xlsx\" },\n  { \"id\": 2, \"excel_path\": \"sales_data/february_sales.xlsx\" }\n]\n</code></pre> <p>In this example, you've specified paths to Excel files. DocETL will use these paths to locate and process the external files. However, without custom parsing, DocETL wouldn't know how to handle the contents of these files. This is where parsing tools come in handy.</p>"},{"location":"examples/custom-parsing/#custom-parsing-in-action","title":"Custom Parsing in Action","text":""},{"location":"examples/custom-parsing/#1-configuration","title":"1. Configuration","text":"<p>To use custom parsing, you need to define parsing tools in your DocETL configuration file. Here's an example:</p> <pre><code>parsing_tools:\n  - name: top_products_report\n    function_code: |\n      def top_products_report(document: Dict) -&gt; List[Dict]:\n          import pandas as pd\n\n          # Read the Excel file\n          filename = document[\"excel_path\"]\n          df = pd.read_excel(filename)\n\n          # Calculate total sales\n          total_sales = df['Sales'].sum()\n\n          # Find top 500 products by sales\n          top_products = df.groupby('Product')['Sales'].sum().nlargest(500)\n\n          # Calculate month-over-month growth\n          df['Date'] = pd.to_datetime(df['Date'])\n          monthly_sales = df.groupby(df['Date'].dt.to_period('M'))['Sales'].sum()\n          mom_growth = monthly_sales.pct_change().fillna(0)\n\n          # Prepare the analysis report\n          report = [\n              f\"Total Sales: ${total_sales:,.2f}\",\n              \"\\nTop 500 Products by Sales:\",\n              top_products.to_string(),\n              \"\\nMonth-over-Month Growth:\",\n              mom_growth.to_string()\n          ]\n\n          # Return a list of dicts representing the output\n          # The input document will be merged into each output doc,\n          # so we can access all original fields from the input doc.\n          return [{\"sales_analysis\": \"\\n\".join(report)}]\n\ndatasets:\n  sales_reports:\n    type: file\n    source: local\n    path: \"sales_data/sales_paths.json\"\n    parsing:\n      - function: top_products_report\n\n  receipts:\n    type: file\n    source: local\n    path: \"receipts/receipt_paths.json\"\n    parsing:\n      - input_key: pdf_path\n        function: paddleocr_pdf_to_string\n        output_key: receipt_text\n        ocr_enabled: true\n        lang: \"en\"\n</code></pre> <p>In this configuration:</p> <ul> <li>We define a custom <code>top_products_report</code> function for Excel files.</li> <li>We use the built-in <code>paddleocr_pdf_to_string</code> parser for PDF files.</li> <li>We apply these parsing tools to the external files referenced in the respective datasets.</li> </ul>"},{"location":"examples/custom-parsing/#2-pipeline-integration","title":"2. Pipeline Integration","text":"<p>Once you've defined your parsing tools and datasets, you can use the processed data in your pipeline:</p> <pre><code>pipeline:\n  steps:\n    - name: process_sales\n      input: sales_reports\n      operations:\n        - summarize_sales\n    - name: process_receipts\n      input: receipts\n      operations:\n        - extract_receipt_info\n</code></pre> <p>This pipeline will use the parsed data from both Excel files and PDFs for further processing.</p>"},{"location":"examples/custom-parsing/#how-data-gets-parsed-and-formatted","title":"How Data Gets Parsed and Formatted","text":"<p>When you run your DocETL pipeline, the parsing tools you've specified in your configuration file are applied to the external files referenced in your dataset JSONs. Here's what happens:</p> <ol> <li>DocETL reads your dataset JSON file.</li> <li>For each entry in the dataset, it looks at the parsing configuration you've specified.</li> <li>It applies the appropriate parsing function to the file path provided in the dataset JSON.</li> <li>The parsing function processes the file and returns the data in a format DocETL can work with (typically a list of strings).</li> </ol> <p>Let's look at how this works for our earlier examples:</p>"},{"location":"examples/custom-parsing/#excel-files-using-top_products_report","title":"Excel Files (using top_products_report)","text":"<p>For an Excel file like \"sales_data/january_sales.xlsx\":</p> <ul> <li>The top_products_report function reads the Excel file.</li> <li>It processes the sales data and generates a report of top-selling products.</li> <li>The output might look like this:</li> </ul> <pre><code>Top Products Report - January 2023\n\n1. Widget A - 1500 units sold\n2. Gadget B - 1200 units sold\n3. Gizmo C - 950 units sold\n4. Doohickey D - 800 units sold\n5. Thingamajig E - 650 units sold\n   ...\n\nTotal Revenue: $245,000\nBest Selling Category: Electronics\n</code></pre>"},{"location":"examples/custom-parsing/#pdf-files-using-paddleocr_pdf_to_string","title":"PDF Files (using paddleocr_pdf_to_string)","text":"<p>For a PDF file like \"receipts/receipt001.pdf\":</p> <ul> <li>The paddleocr_pdf_to_string function reads the PDF file.</li> <li>It uses PaddleOCR to perform optical character recognition on each page.</li> <li>The function combines the extracted text from all pages into a single string.   The output might look like this:</li> </ul> <pre><code>RECEIPT\nStore: Example Store\nDate: 2023-05-15\nItems:\n\n1. Product A - $10.99\n2. Product B - $15.50\n3. Product C - $7.25\n4. Product D - $22.00\n   Subtotal: $55.74\n   Tax (8%): $4.46\n   Total: $60.20\n\nPayment Method: Credit Card\nCard Number: \\***\\* \\*\\*** \\*\\*\\*\\* 1234\n\nThank you for your purchase!\n</code></pre> <p>This parsed and formatted data is then passed to the respective operations in your pipeline for further processing.</p>"},{"location":"examples/custom-parsing/#running-the-pipeline","title":"Running the Pipeline","text":"<p>Once you've set up your pipeline configuration file with the appropriate parsing tools and dataset definitions, you can run your DocETL pipeline. Here's how:</p> <ol> <li>Ensure you have DocETL installed in your environment.</li> <li>Open a terminal or command prompt.</li> <li>Navigate to the directory containing your pipeline configuration file.</li> <li>Run the following command:</li> </ol> <pre><code>docetl run pipeline.yaml\n</code></pre> <p>Replace <code>pipeline.yaml</code> with the name of your pipeline file if it's different.</p> <p>When you run this command:</p> <ol> <li>DocETL reads your pipeline file.</li> <li>It processes each dataset using the specified parsing tools.</li> <li>The pipeline steps are executed in the order you defined.</li> <li>Any operations you've specified (like <code>summarize_sales</code> or <code>extract_receipt_info</code>) are applied to the parsed data.</li> <li>The results are saved according to your output configuration.</li> </ol>"},{"location":"examples/custom-parsing/#built-in-parsing-tools","title":"Built-in Parsing Tools","text":"<p>DocETL provides several built-in parsing tools to handle common file formats and data processing tasks. These tools can be used directly in your configuration by specifying their names in the <code>function</code> field of your parsing tools configuration. Here's an overview of the available built-in parsing tools:</p> <p>Convert an Excel file to a string representation or a list of string representations.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the xlsx file.</p> required <code>orientation</code> <code>str</code> <p>Either \"row\" or \"col\" for cell arrangement.</p> <code>'col'</code> <code>col_order</code> <code>Optional[List[str]]</code> <p>List of column names to specify the order.</p> <code>None</code> <code>doc_per_sheet</code> <code>bool</code> <p>If True, return a list of strings, one per sheet.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: String representation(s) of the Excel file content.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef xlsx_to_string(\n    filename: str,\n    orientation: str = \"col\",\n    col_order: Optional[List[str]] = None,\n    doc_per_sheet: bool = False,\n) -&gt; List[str]:\n    \"\"\"\n    Convert an Excel file to a string representation or a list of string representations.\n\n    Args:\n        filename (str): Path to the xlsx file.\n        orientation (str): Either \"row\" or \"col\" for cell arrangement.\n        col_order (Optional[List[str]]): List of column names to specify the order.\n        doc_per_sheet (bool): If True, return a list of strings, one per sheet.\n\n    Returns:\n        List[str]: String representation(s) of the Excel file content.\n    \"\"\"\n    import openpyxl\n\n    wb = openpyxl.load_workbook(filename)\n\n    def process_sheet(sheet):\n        if col_order:\n            headers = [\n                col for col in col_order if col in sheet.iter_cols(1, sheet.max_column)\n            ]\n        else:\n            headers = [cell.value for cell in sheet[1]]\n\n        result = []\n        if orientation == \"col\":\n            for col_idx, header in enumerate(headers, start=1):\n                column = sheet.cell(1, col_idx).column_letter\n                column_values = [cell.value for cell in sheet[column][1:]]\n                result.append(f\"{header}: \" + \"\\n\".join(map(str, column_values)))\n                result.append(\"\")  # Empty line between columns\n        else:  # row\n            for row in sheet.iter_rows(min_row=2, values_only=True):\n                row_dict = {\n                    header: value for header, value in zip(headers, row) if header\n                }\n                result.append(\n                    \" | \".join(\n                        [f\"{header}: {value}\" for header, value in row_dict.items()]\n                    )\n                )\n\n        return \"\\n\".join(result)\n\n    if doc_per_sheet:\n        return [process_sheet(sheet) for sheet in wb.worksheets]\n    else:\n        return [process_sheet(wb.active)]\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Read the content of a text file and return it as a list of strings (only one element).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the txt or md file.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Content of the file as a list of strings.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef txt_to_string(filename: str) -&gt; List[str]:\n    \"\"\"\n    Read the content of a text file and return it as a list of strings (only one element).\n\n    Args:\n        filename (str): Path to the txt or md file.\n\n    Returns:\n        List[str]: Content of the file as a list of strings.\n    \"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as file:\n        return [file.read()]\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Extract text from a Word document.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the docx file.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Extracted text from the document.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef docx_to_string(filename: str) -&gt; List[str]:\n    \"\"\"\n    Extract text from a Word document.\n\n    Args:\n        filename (str): Path to the docx file.\n\n    Returns:\n        List[str]: Extracted text from the document.\n    \"\"\"\n    from docx import Document\n\n    doc = Document(filename)\n    return [\"\\n\".join([paragraph.text for paragraph in doc.paragraphs])]\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Transcribe speech from an audio file to text using Whisper model via litellm. If the file is larger than 25 MB, it's split into 10-minute chunks with 30-second overlap.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the mp3 or mp4 file.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Transcribed text.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef whisper_speech_to_text(filename: str) -&gt; List[str]:\n    \"\"\"\n    Transcribe speech from an audio file to text using Whisper model via litellm.\n    If the file is larger than 25 MB, it's split into 10-minute chunks with 30-second overlap.\n\n    Args:\n        filename (str): Path to the mp3 or mp4 file.\n\n    Returns:\n        List[str]: Transcribed text.\n    \"\"\"\n    import os\n    from litellm import transcription\n\n    file_size = os.path.getsize(filename)\n    if file_size &gt; 25 * 1024 * 1024:  # 25 MB in bytes\n        from pydub import AudioSegment\n\n        audio = AudioSegment.from_file(filename)\n        chunk_length = 10 * 60 * 1000  # 10 minutes in milliseconds\n        overlap = 30 * 1000  # 30 seconds in milliseconds\n\n        chunks = []\n        for i in range(0, len(audio), chunk_length - overlap):\n            chunk = audio[i : i + chunk_length]\n            chunks.append(chunk)\n\n        transcriptions = []\n\n        for i, chunk in enumerate(chunks):\n            buffer = io.BytesIO()\n            buffer.name = f\"temp_chunk_{i}_{os.path.basename(filename)}\"\n            chunk.export(buffer, format=\"mp3\")\n            buffer.seek(0)  # Reset buffer position to the beginning\n\n            response = transcription(model=\"whisper-1\", file=buffer)\n            transcriptions.append(response.text)\n\n        return transcriptions\n    else:\n        with open(filename, \"rb\") as audio_file:\n            response = transcription(model=\"whisper-1\", file=audio_file)\n\n        return [response.text]\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Extract text from a PowerPoint presentation.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the pptx file.</p> required <code>doc_per_slide</code> <code>bool</code> <p>If True, return each slide as a separate document. If False, return the entire presentation as one document.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Extracted text from the presentation. If doc_per_slide is True, each string in the list represents a single slide. Otherwise, the list contains a single string with all slides' content.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef pptx_to_string(filename: str, doc_per_slide: bool = False) -&gt; List[str]:\n    \"\"\"\n    Extract text from a PowerPoint presentation.\n\n    Args:\n        filename (str): Path to the pptx file.\n        doc_per_slide (bool): If True, return each slide as a separate\n            document. If False, return the entire presentation as one document.\n\n    Returns:\n        List[str]: Extracted text from the presentation. If doc_per_slide\n            is True, each string in the list represents a single slide.\n            Otherwise, the list contains a single string with all slides'\n            content.\n    \"\"\"\n    from pptx import Presentation\n\n    prs = Presentation(filename)\n    result = []\n\n    for slide in prs.slides:\n        slide_content = []\n        for shape in slide.shapes:\n            if hasattr(shape, \"text\"):\n                slide_content.append(shape.text)\n\n        if doc_per_slide:\n            result.append(\"\\n\".join(slide_content))\n        else:\n            result.extend(slide_content)\n\n    if not doc_per_slide:\n        result = [\"\\n\".join(result)]\n\n    return result\n</code></pre> <p>options:     show_root_heading: true     heading_level: 3</p> <p>Note to developers: We used this documentation as a reference.</p> <p>This function uses Azure Document Intelligence to extract text from documents. To use this function, you need to set up an Azure Document Intelligence resource:</p> <ol> <li>Create an Azure account if you don't have one</li> <li>Set up a Document Intelligence resource in the Azure portal</li> <li>Once created, find the resource's endpoint and key in the Azure portal</li> <li>Set these as environment variables:</li> <li>DOCUMENTINTELLIGENCE_API_KEY: Your Azure Document Intelligence API key</li> <li>DOCUMENTINTELLIGENCE_ENDPOINT: Your Azure Document Intelligence endpoint URL</li> </ol> <p>The function will use these credentials to authenticate with the Azure service. If the environment variables are not set, the function will raise a ValueError.</p> <p>The Azure Document Intelligence client is then initialized with these credentials. It sends the document (either as a file or URL) to Azure for analysis. The service processes the document and returns structured information about its content.</p> <p>This function then extracts the text content from the returned data, applying any specified formatting options (like including line numbers or font styles). The extracted text is returned as a list of strings, with each string representing either a page (if doc_per_page is True) or the entire document.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file to be analyzed or URL of the document if use_url is True.</p> required <code>use_url</code> <code>bool</code> <p>If True, treat filename as a URL. Defaults to False.</p> <code>False</code> <code>include_line_numbers</code> <code>bool</code> <p>If True, include line numbers in the output. Defaults to False.</p> <code>False</code> <code>include_handwritten</code> <code>bool</code> <p>If True, include handwritten text in the output. Defaults to False.</p> <code>False</code> <code>include_font_styles</code> <code>bool</code> <p>If True, include font style information in the output. Defaults to False.</p> <code>False</code> <code>include_selection_marks</code> <code>bool</code> <p>If True, include selection marks in the output. Defaults to False.</p> <code>False</code> <code>doc_per_page</code> <code>bool</code> <p>If True, return each page as a separate document. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Extracted text from the document. If doc_per_page is True, each string in the list represents        a single page. Otherwise, the list contains a single string with all pages' content.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DOCUMENTINTELLIGENCE_API_KEY or DOCUMENTINTELLIGENCE_ENDPOINT environment variables are not set.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef azure_di_read(\n    filename: str,\n    use_url: bool = False,\n    include_line_numbers: bool = False,\n    include_handwritten: bool = False,\n    include_font_styles: bool = False,\n    include_selection_marks: bool = False,\n    doc_per_page: bool = False,\n) -&gt; List[str]:\n    \"\"\"\n    &gt; Note to developers: We used [this documentation](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/use-sdk-rest-api?view=doc-intel-4.0.0&amp;tabs=windows&amp;pivots=programming-language-python) as a reference.\n\n    This function uses Azure Document Intelligence to extract text from documents.\n    To use this function, you need to set up an Azure Document Intelligence resource:\n\n    1. [Create an Azure account](https://azure.microsoft.com/) if you don't have one\n    2. Set up a Document Intelligence resource in the [Azure portal](https://portal.azure.com/#create/Microsoft.CognitiveServicesFormRecognizer)\n    3. Once created, find the resource's endpoint and key in the Azure portal\n    4. Set these as environment variables:\n       - DOCUMENTINTELLIGENCE_API_KEY: Your Azure Document Intelligence API key\n       - DOCUMENTINTELLIGENCE_ENDPOINT: Your Azure Document Intelligence endpoint URL\n\n    The function will use these credentials to authenticate with the Azure service.\n    If the environment variables are not set, the function will raise a ValueError.\n\n    The Azure Document Intelligence client is then initialized with these credentials.\n    It sends the document (either as a file or URL) to Azure for analysis.\n    The service processes the document and returns structured information about its content.\n\n    This function then extracts the text content from the returned data,\n    applying any specified formatting options (like including line numbers or font styles).\n    The extracted text is returned as a list of strings, with each string\n    representing either a page (if doc_per_page is True) or the entire document.\n\n    Args:\n        filename (str): Path to the file to be analyzed or URL of the document if use_url is True.\n        use_url (bool, optional): If True, treat filename as a URL. Defaults to False.\n        include_line_numbers (bool, optional): If True, include line numbers in the output. Defaults to False.\n        include_handwritten (bool, optional): If True, include handwritten text in the output. Defaults to False.\n        include_font_styles (bool, optional): If True, include font style information in the output. Defaults to False.\n        include_selection_marks (bool, optional): If True, include selection marks in the output. Defaults to False.\n        doc_per_page (bool, optional): If True, return each page as a separate document. Defaults to False.\n\n    Returns:\n        List[str]: Extracted text from the document. If doc_per_page is True, each string in the list represents\n                   a single page. Otherwise, the list contains a single string with all pages' content.\n\n    Raises:\n        ValueError: If DOCUMENTINTELLIGENCE_API_KEY or DOCUMENTINTELLIGENCE_ENDPOINT environment variables are not set.\n    \"\"\"\n    import os\n\n    from azure.ai.documentintelligence import DocumentIntelligenceClient\n    from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n    from azure.core.credentials import AzureKeyCredential\n\n    key = os.getenv(\"DOCUMENTINTELLIGENCE_API_KEY\")\n    endpoint = os.getenv(\"DOCUMENTINTELLIGENCE_ENDPOINT\")\n\n    if key is None:\n        raise ValueError(\"DOCUMENTINTELLIGENCE_API_KEY environment variable is not set\")\n    if endpoint is None:\n        raise ValueError(\n            \"DOCUMENTINTELLIGENCE_ENDPOINT environment variable is not set\"\n        )\n\n    document_analysis_client = DocumentIntelligenceClient(\n        endpoint=endpoint, credential=AzureKeyCredential(key)\n    )\n\n    if use_url:\n        poller = document_analysis_client.begin_analyze_document(\n            \"prebuilt-read\", AnalyzeDocumentRequest(url_source=filename)\n        )\n    else:\n        with open(filename, \"rb\") as f:\n            poller = document_analysis_client.begin_analyze_document(\"prebuilt-read\", f)\n\n    result = poller.result()\n\n    style_content = []\n    content = []\n\n    if result.styles:\n        for style in result.styles:\n            if style.is_handwritten and include_handwritten:\n                handwritten_text = \",\".join(\n                    [\n                        result.content[span.offset : span.offset + span.length]\n                        for span in style.spans\n                    ]\n                )\n                style_content.append(f\"Handwritten content: {handwritten_text}\")\n\n            if style.font_style and include_font_styles:\n                styled_text = \",\".join(\n                    [\n                        result.content[span.offset : span.offset + span.length]\n                        for span in style.spans\n                    ]\n                )\n                style_content.append(f\"'{style.font_style}' font style: {styled_text}\")\n\n    for page in result.pages:\n        page_content = []\n\n        if page.lines:\n            for line_idx, line in enumerate(page.lines):\n                if include_line_numbers:\n                    page_content.append(f\" Line #{line_idx}: {line.content}\")\n                else:\n                    page_content.append(f\"{line.content}\")\n\n        if page.selection_marks and include_selection_marks:\n            # TODO: figure this out\n            for selection_mark_idx, selection_mark in enumerate(page.selection_marks):\n                page_content.append(\n                    f\"Selection mark #{selection_mark_idx}: State is '{selection_mark.state}' within bounding polygon \"\n                    f\"'{selection_mark.polygon}' and has a confidence of {selection_mark.confidence}\"\n                )\n\n        content.append(\"\\n\".join(page_content))\n\n    if doc_per_page:\n        return style_content + content\n    else:\n        return [\n            \"\\n\\n\".join(\n                [\n                    \"\\n\".join(style_content),\n                    \"\\n\\n\".join(\n                        f\"Page {i+1}:\\n{page_content}\"\n                        for i, page_content in enumerate(content)\n                    ),\n                ]\n            )\n        ]\n</code></pre> <p>options:     heading_level: 3     show_root_heading: true</p> <p>Extract text and image information from a PDF file using PaddleOCR for image-based PDFs.</p> <p>Note: this is very slow!!</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input PDF file.</p> required <code>doc_per_page</code> <code>bool</code> <p>If True, return a list of strings, one per page. If False, return a single string.</p> <code>False</code> <code>ocr_enabled</code> <code>bool</code> <p>Whether to enable OCR for image-based PDFs.</p> <code>True</code> <code>lang</code> <code>str</code> <p>Language of the PDF file.</p> <code>'en'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Extracted content as a list of formatted strings.</p> Source code in <code>docetl/parsing_tools.py</code> <pre><code>@with_input_output_key\ndef paddleocr_pdf_to_string(\n    input_path: str,\n    doc_per_page: bool = False,\n    ocr_enabled: bool = True,\n    lang: str = \"en\",\n) -&gt; List[str]:\n    \"\"\"\n    Extract text and image information from a PDF file using PaddleOCR for image-based PDFs.\n\n    **Note: this is very slow!!**\n\n    Args:\n        input_path (str): Path to the input PDF file.\n        doc_per_page (bool): If True, return a list of strings, one per page.\n            If False, return a single string.\n        ocr_enabled (bool): Whether to enable OCR for image-based PDFs.\n        lang (str): Language of the PDF file.\n\n    Returns:\n        List[str]: Extracted content as a list of formatted strings.\n    \"\"\"\n    from paddleocr import PaddleOCR\n    import fitz\n    import numpy as np\n\n    ocr = PaddleOCR(use_angle_cls=True, lang=lang)\n\n    pdf_content = []\n\n    with fitz.open(input_path) as pdf:\n        for page_num in range(len(pdf)):\n            page = pdf[page_num]\n            text = page.get_text()\n            images = []\n\n            # Extract image information\n            for img_index, img in enumerate(page.get_images(full=True)):\n                rect = page.get_image_bbox(img)\n                images.append(f\"Image {img_index + 1}: bbox {rect}\")\n\n            page_content = f\"Page {page_num + 1}:\\n\"\n            page_content += f\"Text:\\n{text}\\n\"\n            page_content += \"Images:\\n\" + \"\\n\".join(images) + \"\\n\"\n\n            if not text and ocr_enabled:\n                mat = fitz.Matrix(2, 2)\n                pix = page.get_pixmap(matrix=mat)\n                img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(\n                    pix.height, pix.width, 3\n                )\n\n                ocr_result = ocr.ocr(img, cls=True)\n                page_content += \"OCR Results:\\n\"\n                for line in ocr_result[0]:\n                    bbox, (text, _) = line\n                    page_content += f\"{bbox}, {text}\\n\"\n\n            pdf_content.append(page_content)\n\n    if not doc_per_page:\n        return [\"\\n\\n\".join(pdf_content)]\n\n    return pdf_content\n</code></pre> <p>options:     heading_level: 3     show_root_heading: true</p>"},{"location":"examples/custom-parsing/#using-function-arguments-with-parsing-tools","title":"Using Function Arguments with Parsing Tools","text":"<p>When using parsing tools in your DocETL configuration, you can pass additional arguments to the parsing functions.</p> <p>For example, when using the xlsx_to_string parsing tool, you can specify options like the orientation of the data, the order of columns, or whether to process each sheet separately. Here's an example of how to use such kwargs in your configuration:</p> <pre><code>datasets:\n  my_sales:\n    type: file\n    source: local\n    path: \"sales_data/sales_paths.json\"\n    parsing_tools:\n      - name: excel_parser\n        function: xlsx_to_string\n        orientation: row\n        col_order: [\"Date\", \"Product\", \"Quantity\", \"Price\"]\n        doc_per_sheet: true\n</code></pre>"},{"location":"examples/custom-parsing/#contributing-built-in-parsing-tools","title":"Contributing Built-in Parsing Tools","text":"<p>While DocETL provides several built-in parsing tools, the community can always benefit from additional utilities. If you've developed a parsing tool that you think could be useful for others, consider contributing it to the DocETL repository. Here's how you can add new built-in parsing utilities:</p> <ol> <li>Fork the DocETL repository on GitHub.</li> <li>Clone your forked repository to your local machine.</li> <li>Navigate to the <code>docetl/parsing_tools.py</code> file.</li> <li>Add your new parsing function to this file. The function should also be added to the <code>PARSING_TOOLS</code> dictionary.</li> <li>Update the documentation in the function's docstring.</li> <li>Create a pull request to merge your changes into the main DocETL repository.</li> </ol> <p>Guidelines for Contributing Parsing Tools</p> <p>When contributing a new parsing tool, make sure it follows these guidelines:</p> <ul> <li>The function should have a clear, descriptive name.</li> <li>Include comprehensive docstrings explaining the function's purpose, parameters, and return value. The return value should be a list of strings.</li> <li>Handle potential errors gracefully and provide informative error messages.</li> <li>If your parser requires additional dependencies, make sure to mention them in the pull request.</li> </ul>"},{"location":"examples/custom-parsing/#creating-custom-parsing-tools","title":"Creating Custom Parsing Tools","text":"<p>If the built-in tools don't meet your needs, you can create your own custom parsing tools. Here's how:</p> <ol> <li>Define your parsing function in the <code>parsing_tools</code> section of your configuration.</li> <li>Ensure your function takes a document (dict) as input and returns a list of documents (dicts).</li> <li>Use your custom parser in the <code>parsing</code> section of your dataset configuration.</li> </ol> <p>For example:</p> <pre><code>parsing_tools:\n  - name: my_custom_parser\n    function_code: |\n      def my_custom_parser(document: Dict) -&gt; List[Dict]:\n          # Your custom parsing logic here\n          return [processed_data]\n\ndatasets:\n  my_dataset:\n    type: file\n    source: local\n    path: \"data/paths.json\"\n    parsing:\n      - function: my_custom_parser\n</code></pre>"},{"location":"examples/mining-product-reviews/","title":"Mining Product Reviews: Identifying Polarizing Themes in Video Games","text":"<p>This tutorial demonstrates how to use DocETL to analyze product reviews, specifically focusing on identifying polarizing themes across multiple video games. We'll walk through the process of building a pipeline that extracts insights from Steam game reviews, resolves common themes, and generates comprehensive reports.</p> <p>Optimization Cost</p> <p>Optimizing this pipeline can be computationally expensive and time-consuming, especially for large datasets. The process involves running multiple LLM calls and comparisons between different plans, which can result in significant resource usage and potential costs.</p> <p>For reference, optimizing a pipeline of this complexity could cost up to $70 in OpenAI credits, depending on the size of your dataset and the specific models used. Always monitor your usage and set appropriate limits to avoid unexpected charges.</p>"},{"location":"examples/mining-product-reviews/#task-overview","title":"Task Overview","text":"<p>Our goal is to create a pipeline that will:</p> <ol> <li>Identify polarizing themes within individual game reviews</li> <li>Resolve similar themes across different games</li> <li>Generate reports of polarizing themes common across games, supported by quotes from different game reviews</li> </ol> <p>We'll be using a subset of the STEAM review dataset. We've created a subset that contains reviews for 500 of the most popular games, with approximately 400 reviews per game, balanced between positive and negative ratings. For each game, we concatenate all reviews into a single text for analysis---so we'll have 500 input documents, each representing a game. You can get the dataset sample here.</p>"},{"location":"examples/mining-product-reviews/#pipeline-structure","title":"Pipeline Structure","text":"<p>Let's examine the pipeline structure and its operations:</p> <pre><code>pipeline:\n  steps:\n    - name: game_analysis\n      input: steam_reviews\n      operations:\n        - identify_polarizing_themes\n        - unnest_polarizing_themes\n        - resolve_themes\n        - aggregate_common_themes\n\n  output:\n    type: file\n    path: \"output_polarizing_themes.json\"\n    intermediate_dir: \"intermediates\"\n</code></pre> Full Pipeline Configuration <pre><code>default_model: gpt-4o-mini\n\ndatasets:\n  steam_reviews:\n    type: file\n    path: \"path/to/top_apps_steam_sample.json\"\n\noperations:\n  - name: identify_polarizing_themes\n    optimize: true\n    type: map\n    prompt: |\n      Analyze the following concatenated reviews for a video game and identify polarizing themes that divide player opinions. A polarizing theme is one that some players love while others strongly dislike.\n\n      Game: {{ input.app_name }}\n      Reviews: {{ input.concatenated_reviews }}\n\n      For each polarizing theme you identify:\n      1. Provide a summary of the theme\n      2. Explain why it's polarizing\n      3. Include supporting quotes from both positive and negative perspectives\n\n      Aim to identify ~10 polarizing themes, if present.\n\n    output:\n      schema:\n        polarizing_themes: \"list[{theme: str, summary: str, polarization_reason: str, positive_quotes: str, negative_quotes: str}]\"\n\n  - name: unnest_polarizing_themes\n    type: unnest\n    unnest_key: polarizing_themes\n    recursive: true\n    depth: 2\n\n  - name: resolve_themes\n    type: resolve\n    optimize: true\n    comparison_prompt: |\n      Are the themes \"{{ input1.theme }}\" and \"{{ input2.theme }}\" the same? Here is some context to help you decide:\n\n      Theme 1: {{ input1.theme }}\n      Summary 1: {{ input1.summary }}\n\n      Theme 2: {{ input2.theme }}\n      Summary 2: {{ input2.summary }}\n    resolution_prompt: |\n      Given the following themes, please come up with a theme that best captures the essence of all the themes:\n\n      {% for input in inputs %}\n      Theme {{ loop.index }}: {{ input.theme }}\n      {% if not loop.last %}\n      ---\n      {% endif %}\n      {% endfor %}\n\n      Based on these themes, provide a consolidated theme that captures the essence of all the above themes. Ensure that the consolidated theme is concise yet comprehensive.\n    output:\n      schema:\n        theme: str\n\n  - name: aggregate_common_themes\n    type: reduce\n    optimize: true\n    reduce_key: theme\n    prompt: |\n      You are given a theme and summary that appears across multiple video games, along with various apps and review quotes related to this theme. Your task is to consolidate this information into a comprehensive report.\n\n      For each input, you will receive:\n      - theme: A specific polarizing theme\n      - summary: A brief summary of the theme\n      - app_name: The name of the game\n      - positive_quotes: List of supporting quotes from positive perspectives\n      - negative_quotes: List of supporting quotes from negative perspectives\n\n      Create a report that includes:\n      1. The name of the common theme\n      2. A summary of the theme and why it's common across games\n      3. Representative quotes from different games, both positive and negative\n\n      Here's the information for the theme:\n      Theme: {{ inputs[0].theme }}\n      Summary: {{ inputs[0].summary }}\n\n      {% for app in inputs %}\n      Game: {{ app.app_name }}\n      Positive Quotes: {{ app.positive_quotes }}\n      Negative Quotes: {{ app.negative_quotes }}\n      {% if not loop.last %}\n      ----------------------------------------\n      {% endif %}\n      {% endfor %}\n\n    output:\n      schema:\n        theme_summary: str\n        representative_quotes: \"list[{game: str, quote: str, sentiment: str}]\"\n\npipeline:\n  steps:\n    - name: game_analysis\n      input: steam_reviews\n      operations:\n        - identify_polarizing_themes\n        - unnest_polarizing_themes\n        - resolve_themes\n        - aggregate_common_themes\n\n  output:\n    type: file\n    path: \"path/to/output_polarizing_themes.json\"\n    intermediate_dir: \"path/to/intermediates\"\n</code></pre>"},{"location":"examples/mining-product-reviews/#pipeline-operations","title":"Pipeline Operations","text":""},{"location":"examples/mining-product-reviews/#1-identify-polarizing-themes","title":"1. Identify Polarizing Themes","text":"<p>This map operation processes each game's reviews to identify polarizing themes:</p> <pre><code>- name: identify_polarizing_themes\n  optimize: true\n  type: map\n  prompt: |\n    Analyze the following concatenated reviews for a video game and identify polarizing themes that divide player opinions. A polarizing theme is one that some players love while others strongly dislike.\n\n    Game: {{ input.app_name }}\n    Reviews: {{ input.concatenated_reviews }}\n\n    For each polarizing theme you identify:\n    1. Provide a summary of the theme\n    2. Explain why it's polarizing\n    3. Include supporting quotes from both positive and negative perspectives\n\n    Aim to identify ~10 polarizing themes, if present.\n\n  output:\n    schema:\n      polarizing_themes: \"list[{theme: str, summary: str, polarization_reason: str, positive_quotes: str, negative_quotes: str}]\"\n</code></pre>"},{"location":"examples/mining-product-reviews/#2-unnest-polarizing-themes","title":"2. Unnest Polarizing Themes","text":"<p>This operation flattens the list of themes extracted from each game:</p> <pre><code>- name: unnest_polarizing_themes\n  type: unnest\n  unnest_key: polarizing_themes\n  recursive: true\n  depth: 2\n</code></pre>"},{"location":"examples/mining-product-reviews/#3-resolve-themes","title":"3. Resolve Themes","text":"<p>This operation identifies and consolidates similar themes across different games:</p> <pre><code>- name: resolve_themes\n  type: resolve\n  optimize: true\n  comparison_prompt: |\n    Are the themes \"{{ input1.theme }}\" and \"{{ input2.theme }}\" the same? Here is some context to help you decide:\n\n    Theme 1: {{ input1.theme }}\n    Summary 1: {{ input1.summary }}\n\n    Theme 2: {{ input2.theme }}\n    Summary 2: {{ input2.summary }}\n  resolution_prompt: |\n    Given the following themes, please come up with a theme that best captures the essence of all the themes:\n\n    {% for input in inputs %}\n    Theme {{ loop.index }}: {{ input.theme }}\n    {% if not loop.last %}\n    ---\n    {% endif %}\n    {% endfor %}\n\n    Based on these themes, provide a consolidated theme that captures the essence of all the above themes. Ensure that the consolidated theme is concise yet comprehensive.\n  output:\n    schema:\n      theme: str\n</code></pre>"},{"location":"examples/mining-product-reviews/#4-aggregate-common-themes","title":"4. Aggregate Common Themes","text":"<p>This reduce operation generates a comprehensive report for each common theme:</p> <pre><code>- name: aggregate_common_themes\n  type: reduce\n  optimize: true\n  reduce_key: theme\n  prompt: |\n    You are given a theme and summary that appears across multiple video games, along with various apps and review quotes related to this theme. Your task is to consolidate this information into a comprehensive report.\n\n    For each input, you will receive:\n    - theme: A specific polarizing theme\n    - summary: A brief summary of the theme\n    - app_name: The name of the game\n    - positive_quotes: List of supporting quotes from positive perspectives\n    - negative_quotes: List of supporting quotes from negative perspectives\n\n    Create a report that includes:\n    1. The name of the common theme\n    2. A summary of the theme and why it's common across games\n    3. Representative quotes from different games, both positive and negative\n\n    Here's the information for the theme:\n    Theme: {{ inputs[0].theme }}\n    Summary: {{ inputs[0].summary }}\n\n    {% for app in inputs %}\n    Game: {{ app.app_name }}\n    Positive Quotes: {{ app.positive_quotes }}\n    Negative Quotes: {{ app.negative_quotes }}\n    {% if not loop.last %}\n    ----------------------------------------\n    {% endif %}\n    {% endfor %}\n\n  output:\n    schema:\n      theme_summary: str\n      representative_quotes: \"list[{game: str, quote: str, sentiment: str}]\"\n</code></pre>"},{"location":"examples/mining-product-reviews/#optimizing-the-pipeline","title":"Optimizing the Pipeline","text":"<p>After writing the pipeline, we can use the DocETL <code>build</code> command to optimize it:</p> <pre><code>docetl build pipeline.yaml\n</code></pre> <p>This command, with <code>optimize: true</code> set for the map and resolve operations, provides us with:</p> <ol> <li> <p>A chunking-based plan for the map operation: This helps handle the large input sizes (up to 380,000 tokens) by breaking them into manageable chunks. The optimizer gives us a chunking plan of 87,776 tokens per chunk, with 10% of the previous chunk as peripheral context.</p> </li> <li> <p>Blocking statements and thresholds for the resolve operation: This optimizes the theme resolution process, making it more efficient when dealing with a large number of themes across multiple games. The optimizer provided us with blocking keys of <code>summary</code> and <code>theme</code>, and a threshold of 0.596 for similarity (to get 95% recall of duplicates).</p> </li> </ol> <p>These optimizations are crucial for handling the scale of our dataset, which includes 500 games with an average of 66,000 tokens per game, and 12% of the documents exceeding the context length limits of the OpenAI LLMs (128k tokens).</p> Optimized Pipeline <pre><code>default_model: gpt-4o-mini\n\ndatasets:\n  steam_reviews:\n    type: file\n    path: \"/path/to/steam_reviews_dataset.json\"\n\noperations:\n  - name: split_identify_polarizing_themes\n    type: split\n    split_key: concatenated_reviews\n    method: token_count\n    method_kwargs:\n      token_count: 87776\n    optimize: false\n\n  - name: gather_concatenated_reviews_identify_polarizing_themes\n    type: gather\n    content_key: concatenated_reviews_chunk\n    doc_id_key: split_identify_polarizing_themes_id\n    order_key: split_identify_polarizing_themes_chunk_num\n    peripheral_chunks:\n      previous:\n        tail:\n          count: 0.1\n    optimize: false\n\n  - name: submap_identify_polarizing_themes\n    type: map\n    prompt: |\n      Analyze the following review snippet from a video game {{ input.app_name }} and identify any polarizing themes within it. A polarizing theme is one that diverges opinions among players, where some express strong approval while others express strong disapproval.\n\n      Review Snippet: {{ input.concatenated_reviews_chunk_rendered }}\n\n      For each polarizing theme you identify:\n      1. Provide a brief summary of the theme\n      2. Explain why it's polarizing\n      3. Include supporting quotes from both positive and negative perspectives.\n\n      Aim to identify and analyze 3-5 polarizing themes within this snippet. Only process the main chunk.\n    model: gpt-4o-mini\n    output:\n      schema:\n        polarizing_themes: \"list[{theme: str, summary: str, polarization_reason: str, positive_quotes: str, negative_quotes: str}]\"\n    optimize: false\n\n  - name: subreduce_identify_polarizing_themes\n    type: reduce\n    reduce_key: [\"split_identify_polarizing_themes_id\"]\n    prompt: |\n      Combine the following results and create a cohesive summary of ~10 polarizing themes for the video game {{ inputs[0].app_name }}:\n\n      {% for chunk in inputs %}\n          {% for theme in chunk.polarizing_themes %}\n              {{ theme }}\n              ----------------------------------------\n          {% endfor %}\n      {% endfor %}\n\n      Make sure each theme is unique and not a duplicate of another theme. You should include summaries and supporting quotes (both positive and negative) for each theme.\n    model: gpt-4o-mini\n    output:\n      schema:\n        polarizing_themes: \"list[{theme: str, summary: str, polarization_reason: str, positive_quotes: str, negative_quotes: str}]\"\n    pass_through: true\n    associative: true\n    optimize: false\n    synthesize_resolve: false\n\n  - name: unnest_polarizing_themes\n    type: unnest\n    unnest_key: polarizing_themes\n    recursive: true\n    depth: 2\n\n  - name: resolve_themes\n    type: resolve\n    blocking_keys:\n      - summary\n      - theme\n    blocking_threshold: 0.596\n    optimize: true\n    comparison_prompt: |\n      Are the themes \"{{ input1.theme }}\" and \"{{ input2.theme }}\" the same? Here is some context to help you decide:\n\n      Theme 1: {{ input1.theme }}\n      Summary 1: {{ input1.summary }}\n\n      Theme 2: {{ input2.theme }}\n      Summary 2: {{ input2.summary }}\n    resolution_prompt: |\n      Given the following themes, please come up with a theme that best captures the essence of all the themes:\n\n      {% for input in inputs %}\n      Theme {{ loop.index }}: {{ input.theme }}\n      {% if not loop.last %}\n      ---\n      {% endif %}\n      {% endfor %}\n\n      Based on these themes, provide a consolidated theme that captures the essence of all the above themes. Ensure that the consolidated theme is concise yet comprehensive.\n    output:\n      schema:\n        theme: str\n\n  - name: aggregate_common_themes\n    type: reduce\n    reduce_key: theme\n    prompt: |\n      You are given a theme and summary that appears across multiple video games, along with various apps and review quotes related to this theme. Your task is to consolidate this information into a comprehensive report.\n\n      For each input, you will receive:\n      - theme: A specific polarizing theme\n      - summary: A brief summary of the theme\n      - app_name: The name of the game\n      - positive_quotes: List of supporting quotes from positive perspectives\n      - negative_quotes: List of supporting quotes from negative perspectives\n\n      Create a report that includes:\n      1. The name of the common theme\n      2. A summary of the theme and why it's common across games\n      3. Representative quotes from different games, both positive and negative\n\n      Here's the information for the theme:\n      Theme: {{ inputs[0].theme }}\n      Summary: {{ inputs[0].summary }}\n\n      {% for app in inputs %}\n      Game: {{ app.app_name }}\n      Positive Quotes: {{ app.positive_quotes }}\n      Negative Quotes: {{ app.negative_quotes }}\n      {% if not loop.last %}\n      ----------------------------------------\n      {% endif %}\n      {% endfor %}\n\n    output:\n      schema:\n        theme_summary: str\n        representative_quotes: \"list[{game: str, quote: str, sentiment: str}]\"\n\npipeline:\n  steps:\n    - name: game_analysis\n      input: steam_reviews\n      operations:\n        - split_identify_polarizing_themes\n        - gather_concatenated_reviews_identify_polarizing_themes\n        - submap_identify_polarizing_themes\n        - subreduce_identify_polarizing_themes\n        - unnest_polarizing_themes\n        - resolve_themes\n        - aggregate_common_themes\n\n  output:\n    type: file\n    path: \"/path/to/output/output_polarizing_themes.json\"\n    intermediate_dir: \"/path/to/intermediates\"\n</code></pre>"},{"location":"examples/mining-product-reviews/#running-the-pipeline","title":"Running the Pipeline","text":"<p>With our optimized pipeline in place, we can now run it:</p> <pre><code>docetl run pipeline.yaml\n</code></pre> <p>This command will process the game reviews, extract polarizing themes, resolve similar themes across games, and generate comprehensive reports for each common theme. The results will be saved in output_polarizing_themes.json, providing insights into the polarizing aspects of various video games based on user reviews.</p> <p>The output costs for running this pipeline will depend on the size of the dataset and the specific models used. We used gpt-4o-mini and had ~200,000 reviews we were aggregating. Here's the logs from my terminal:</p> <pre><code>docetl run workloads/steamgames/pipeline_opt.yaml\n[11:05:46] Performing syntax check on all operations...\n           Syntax check passed for all operations.\n           Running Operation:\n             Type: split\n             Name: split_identify_polarizing_themes\n[11:06:08] Intermediate saved for operation 'split_identify_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: gather\n             Name: gather_concatenated_reviews_identify_polarizing_themes\n[11:06:10] Intermediate saved for operation 'gather_concatenated_reviews_identify_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: map\n             Name: submap_identify_polarizing_themes\n\u2839 Running step game_analysis...\n[11:06:14] Intermediate saved for operation 'submap_identify_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: reduce\n             Name: subreduce_identify_polarizing_themes\n\u2834 Running step game_analysis...\n[11:06:16] Intermediate saved for operation 'subreduce_identify_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: unnest\n             Name: unnest_polarizing_themes\n[11:06:25] Intermediate saved for operation 'unnest_polarizing_themes' in step 'game_analysis'\n           Running Operation:\n             Type: resolve\n             Name: resolve_themes\n[11:06:37] Comparisons saved by blocking: 6802895 (97.50%)\n\u2826 Running step game_analysis...\n[13:05:58] Number of keys before resolution: 3736\n           Number of distinct keys after resolution: 1421\n\u2839 Running step game_analysis...\n[13:06:23] Self-join selectivity: 0.1222\n[13:06:36] Intermediate saved for operation 'resolve_themes' in step 'game_analysis'\n           Running Operation:\n             Type: reduce\n             Name: aggregate_common_themes\n\u2834 Running step game_analysis...\n[13:08:05] Intermediate saved for operation 'aggregate_common_themes' in step 'game_analysis'\n           Flushing cache to disk...\n           Cache flushed to disk.\n  Step game_analysis completed. Cost: $13.21\n  Operation split_identify_polarizing_themes completed. Cost: $0.00\n  Operation gather_concatenated_reviews_identify_polarizing_themes completed. Cost: $0.00\n  Operation submap_identify_polarizing_themes completed. Cost: $5.02\n  Operation subreduce_identify_polarizing_themes completed. Cost: $0.38\n  Operation unnest_polarizing_themes completed. Cost: $0.00\n  Operation resolve_themes completed. Cost: $7.56\n  Operation aggregate_common_themes completed. Cost: $0.26\n           \ud83d\udcbe Output saved to output_polarizing_themes.json\n           Total cost: $13.21\n           Total time: 7339.11 seconds\n</code></pre> <p>Upon further analysis, 1421 themes is still a lot! I realized that my resolve operation was not exactly what I wanted---it did not merge together themes that I believed were similar, since the comparison prompt only asked if theme X or Y were the same. I should have given context in the comparison prompt, such as \"Could one of these themes be a subset of the other?\" This underscores the iterative nature of pipeline development and the importance of refining prompts to achieve the desired results; we don't really know what the desired results are until we see the output.</p> <p>Something else we could have done is included a list of themes we care about in the original map operation, e.g., graphics. Since our map prompt was very open-ended, the LLM could have generated themes that we didn't care about, leading to a large number of themes in the output.</p> <p>Anyways, we've filtered the 1421 reports down to 65 themes/reports that contain quotes from 3 or more different games. You can check out the output here.</p>"},{"location":"examples/ollama/","title":"Medical Document Classification with Ollama","text":"<p>This tutorial demonstrates how to use DocETL with Ollama models to classify medical documents into predefined categories. We'll use a simple map operation to process a set of medical records, ensuring that sensitive information remains private by using a locally-run model.</p>"},{"location":"examples/ollama/#setup","title":"Setup","text":"<p>Prerequisites</p> <p>Before we begin, make sure you have Ollama installed and running on your local machine.</p> <p>You'll need to set the OLLAMA_API_BASE environment variable:</p> <pre><code>export OLLAMA_API_BASE=http://localhost:11434/\n</code></pre> <p>API Details</p> <p>For more information on the Ollama REST API, refer to the Ollama documentation.</p>"},{"location":"examples/ollama/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Let's create a pipeline that classifies medical documents into categories such as \"Cardiology\", \"Neurology\", \"Oncology\", etc.</p> <p>Initial Pipeline Configuration</p> <pre><code>datasets:\n  medical_records:\n    type: file\n    path: \"medical_records.json\"\n\ndefault_model: ollama/llama3\n\noperations:\n  - name: classify_medical_record\n    type: map\n    output:\n      schema:\n        categories: \"list[str]\"\n    prompt: |\n      Classify the following medical record into one or more of these categories: Cardiology, Neurology, Oncology, Pediatrics, Orthopedics.\n\n      Medical Record:\n      {{ input.text }}\n\n      Return your answer as a JSON list of strings, e.g., [\"Cardiology\", \"Neurology\"].\n\npipeline:\n  steps:\n    - name: medical_classification\n      input: medical_records\n      operations:\n        - classify_medical_record\n\noutput:\n  type: file\n  path: \"classified_records.json\"\n</code></pre>"},{"location":"examples/ollama/#running-the-pipeline-with-a-sample","title":"Running the Pipeline with a Sample","text":"<p>To test our pipeline and estimate the required timeout, we'll first run it on a sample of documents.</p> <p>Modify the <code>classify_medical_record</code> operation in your configuration to include a <code>sample</code> parameter:</p> <pre><code>operations:\n  - name: classify_medical_record\n    type: map\n    sample: 5\n    output:\n      schema:\n        categories: \"list[str]\"\n    prompt: |\n      Classify the following medical record into one or more of these categories: Cardiology, Neurology, Oncology, Pediatrics, Orthopedics.\n\n      Medical Record:\n      {{ input.text }}\n\n      Return your answer as a JSON list of strings, e.g., [\"Cardiology\", \"Neurology\"].\n</code></pre> <p>Now, run the pipeline with this sample configuration:</p> <pre><code>docetl run pipeline.yaml\n</code></pre>"},{"location":"examples/ollama/#adjusting-the-timeout","title":"Adjusting the Timeout","text":"<p>After running the sample, note the time it took to process 5 documents.</p> <p>Timeout Calculation</p> <p>Let's say it took 100 seconds to process 5 documents. You can use this to estimate the time needed for your full dataset. For example, if you have 1000 documents in total, you might want to set the timeout to:</p> <p>(100 seconds / 5 documents) * 1000 documents = 20,000 seconds</p> <p>Now, adjust your pipeline configuration to include this timeout and remove the sample parameter:</p> <pre><code>operations:\n  - name: classify_medical_record\n    type: map\n    timeout: 20000\n    output:\n      schema:\n        categories: \"list[str]\"\n    prompt: |\n      Classify the following medical record into one or more of these categories: Cardiology, Neurology, Oncology, Pediatrics, Orthopedics.\n      Medical Record:\n      {{ input.text }}\n      Return your answer as a JSON list of strings, e.g., [\"Cardiology\", \"Neurology\"].\n</code></pre> <p>Caching</p> <p>DocETL caches results (even between runs), so if the same document is processed again, the answer will be returned from the cache rather than processed again (significantly speeding up processing).</p>"},{"location":"examples/ollama/#running-the-full-pipeline","title":"Running the Full Pipeline","text":"<p>Now you can run the full pipeline with the adjusted timeout:</p> <pre><code>docetl run pipeline.yaml\n</code></pre> <p>This will process all your medical records, classifying them into the predefined categories.</p>"},{"location":"examples/ollama/#conclusion","title":"Conclusion","text":"<p>Key Takeaways</p> <ul> <li>This pipeline demonstrates how to use Ollama with DocETL for local processing of sensitive data.</li> <li>Ollama integrates into multi-operation pipelines, maintaining data privacy.</li> <li>Ollama is a local model, so it is much slower than leveraging an LLM API like OpenAI. Adjust the timeout accordingly.</li> <li>DocETL's sample and timeout parameters help optimize the pipeline for efficient use of Ollama's capabilities.</li> </ul> <p>For more information, e.g., for specific models, visit https://ollama.com/.</p>"},{"location":"examples/presidential-debate-themes/","title":"Presidential Debate Themes Analysis","text":"<p>This tutorial explains how to analyze themes in presidential debates using the DocETL pipeline. We'll cover the pipeline structure, explain each operation, and discuss the importance of theme resolution.</p>"},{"location":"examples/presidential-debate-themes/#pipeline-overview","title":"Pipeline Overview","text":"<p>Our goal is to build a pipeline that will:</p> <ol> <li>Extract key themes and viewpoints from presidential debate transcripts</li> <li>Analyze how these themes have evolved over time, with references to specific debates and quotes</li> </ol> <p>You can take a look at the raw data here.</p> <p>Let's examine the pipeline structure and its operations:</p> <pre><code>pipeline:\n  steps:\n    - name: debate_analysis\n      input: debates\n      operations:\n        - extract_themes_and_viewpoints\n        - unnest_themes\n        - summarize_theme_evolution\n</code></pre> Full Pipeline Configuration <pre><code>datasets:\n  debates:\n    type: file\n    path: \"data.json\"\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_themes_and_viewpoints\n    type: map\n    output:\n      schema:\n        themes: \"list[{theme: str, viewpoints: str}]\"\n    prompt: |\n      Analyze the following debate transcript for {{ input.title }} on {{ input.date }}:\n\n      {{ input.content }}\n\n      Extract the main themes discussed in this debate and the viewpoints of the candidates on these themes.\n      Return a list of themes and corresponding viewpoints in the following format:\n      [\n        {\n          \"theme\": \"Theme 1\",\n          \"viewpoints\": \"Candidate A's viewpoint... Candidate B's viewpoint...\"\n        },\n        {\n          \"theme\": \"Theme 2\",\n          \"viewpoints\": \"Candidate A's viewpoint... Candidate B's viewpoint...\"\n        },\n        ...\n      ]\n\n  - name: unnest_themes\n    type: unnest\n    unnest_key: themes\n    recursive: true\n\n  - name: summarize_theme_evolution\n    type: reduce\n    reduce_key: theme\n    output:\n      schema:\n        theme: str\n        report: str\n    prompt: |\n      Analyze the following viewpoints on the theme \"{{ inputs[0].theme }}\" from various debates over the years:\n\n      {% for item in inputs %}\n      Year: {{ item.year }}\n      Date: {{ item.date }}\n      Title: {{ item.title }}\n      Viewpoints: {{ item.viewpoints }}\n\n      {% endfor %}\n\n      Generate a comprehensive summary of how Democratic and Republican viewpoints on this theme have evolved through the years. Include supporting quotes from the debates to illustrate key points or shifts in perspective.\n\n      Your summary should:\n      1. Identify *all* major trends or shifts in each party's stance over time\n      2. Highlight any significant agreements or disagreements between the parties\n      3. Note any external events or factors that may have influenced changes in viewpoints\n      4. Use specific quotes to support your analysis\n      5. The title should contain the start and end years of the analysis\n\n      Format your response as a well-structured report.\n\npipeline:\n  steps:\n    - name: debate_analysis\n      input: debates\n      operations:\n        - extract_themes_and_viewpoints\n        - unnest_themes\n        - summarize_theme_evolution\n\n  output:\n    type: file\n    path: \"theme_evolution_analysis.json\"\n    intermediate_dir: \"checkpoints\"\n</code></pre>"},{"location":"examples/presidential-debate-themes/#pipeline-operations","title":"Pipeline Operations","text":""},{"location":"examples/presidential-debate-themes/#1-extract-themes-and-viewpoints","title":"1. Extract Themes and Viewpoints","text":"<pre><code>- name: extract_themes_and_viewpoints\n  type: map\n  output:\n    schema:\n      themes: \"list[{theme: str, viewpoints: str}]\"\n  prompt: |\n    Analyze the following debate transcript for {{ input.title }} on {{ input.date }}:\n\n    {{ input.content }}\n\n    Extract the main themes discussed in this debate and the viewpoints of the candidates on these themes.\n    Return a list of themes and corresponding viewpoints in the following format:\n    [\n      {\n        \"theme\": \"Theme 1\",\n        \"viewpoints\": \"Candidate A's viewpoint... Candidate B's viewpoint...\"\n      },\n      {\n        \"theme\": \"Theme 2\",\n        \"viewpoints\": \"Candidate A's viewpoint... Candidate B's viewpoint...\"\n      },\n      ...\n    ]\n</code></pre> <p>This operation processes each debate transcript to identify main themes and candidates' viewpoints. It uses AI to analyze the content and structure the output in a consistent format.</p>"},{"location":"examples/presidential-debate-themes/#2-unnest-themes","title":"2. Unnest Themes","text":"<pre><code>- name: unnest_themes\n  type: unnest\n  unnest_key: themes\n  recursive: true\n</code></pre> <p>The unnest operation flattens the list of themes extracted from each debate. This step prepares the data for further analysis by creating individual entries for each theme.</p>"},{"location":"examples/presidential-debate-themes/#3-summarize-theme-evolution","title":"3. Summarize Theme Evolution","text":"<pre><code>- name: summarize_theme_evolution\n  type: reduce\n  reduce_key: theme\n  output:\n    schema:\n      theme: str\n      report: str\n  prompt: |\n    Analyze the following viewpoints on the theme \"{{ inputs[0].theme }}\" from various debates over the years:\n\n    {% for item in inputs %}\n    Year: {{ item.year }}\n    Date: {{ item.date }}\n    Title: {{ item.title }}\n    Viewpoints: {{ item.viewpoints }}\n\n    {% endfor %}\n\n    Generate a comprehensive summary of how Democratic and Republican viewpoints on this theme have evolved through the years. Include supporting quotes from the debates to illustrate key points or shifts in perspective.\n\n    Your summary should:\n    1. Identify all major trends or shifts in each party's stance over time\n    2. Highlight any significant agreements or disagreements between the parties\n    3. Note any external events or factors that may have influenced changes in viewpoints\n    4. Use specific quotes to support your analysis\n    5. The title should contain the start and end years of the analysis\n\n    Format your response as a well-structured report.\n</code></pre> <p>This operation analyzes how each theme has evolved over time. It considers viewpoints from multiple debates, identifies trends, and generates a comprehensive summary of the theme's evolution.</p>"},{"location":"examples/presidential-debate-themes/#the-need-for-theme-resolution","title":"The Need for Theme Resolution","text":"<p>An important consideration in this pipeline is the potential for similar themes to be generated with slightly different wording (e.g., \"Climate Change Policy\" vs. \"Environmental Regulations\"). To address this, we need to add a resolve operation before the summarization step.</p> <p>To synthesize a resolve operation, we can use the <code>docetl build</code> command:</p> <pre><code>docetl build pipeline.yaml\n</code></pre> <p>This command adds a resolve operation to our pipeline, resulting in an optimized version:</p> <pre><code>operations:\n    ...\n    - name: synthesized_resolve_0\n      type: resolve\n      blocking_keys:\n        - theme\n      blocking_threshold: 0.6465\n      comparison_model: gpt-4o-mini\n      comparison_prompt: |\n        Compare the following two debate themes:\n\n        [Entity 1]:\n        {{ input1.theme }}\n\n        [Entity 2]:\n        {{ input2.theme }}\n\n        Are these themes likely referring to the same concept? Consider the following attributes:\n        - The core subject matter being discussed\n        - The context in which the theme is presented\n        - The viewpoints of the candidates associated with each theme\n\n        Respond with \"True\" if they are likely the same theme, or \"False\" if they are likely different themes.\n      embedding_model: text-embedding-3-small\n      compare_batch_size: 1000\n      output:\n        schema:\n          theme: string\n      resolution_model: gpt-4o-mini\n      resolution_prompt: |\n        Analyze the following duplicate themes:\n\n        {% for key in inputs %}\n        Entry {{ loop.index }}:\n        {{ key.theme }}\n\n        {% endfor %}\n\n        Create a single, consolidated key that combines the information from all duplicate entries. When merging, follow these guidelines:\n        1. Prioritize the most comprehensive and detailed viewpoint available among the duplicates. If multiple entries discuss the same theme with varying details, select the entry that includes the most information.\n        2. Ensure clarity and coherence in the merged key; if key terms or phrases are duplicated, synthesize them into a single statement or a cohesive description that accurately represents the theme.\n\n        Ensure that the merged key conforms to the following schema:\n        {\n          \"theme\": \"string\"\n        }\n\n        Return the consolidated key as a single JSON object.\n\n\npipeline:\n  steps:\n    - name: debate_analysis\n      input: debates\n      operations:\n        - extract_themes_and_viewpoints\n        - unnest_themes\n        - synthesized_resolve_0\n        - summarize_theme_evolution\n</code></pre> <p>The new <code>synthesized_resolve_0</code> operation groups similar themes together, ensuring a more accurate and comprehensive analysis of each theme's evolution.</p>"},{"location":"examples/presidential-debate-themes/#running-the-optimized-pipeline","title":"Running the Optimized Pipeline","text":"<p>With the resolve operation in place, we can now run our optimized pipeline:</p> <pre><code>docetl run pipeline_opt.yaml\n</code></pre> <p>This command processes the debate transcripts, extracts themes, resolves similar themes, and generates summaries of theme evolution over time. The results will be saved in <code>theme_evolution_analysis.json</code>, providing insights into the changing landscape of topics discussed in presidential debates. Since we've also set an <code>intermediate_dir</code> in our pipeline configuration, intermediate results will be saved in the <code>intermediate_dir</code> directory.</p> <p>Here's the output from running our optimized pipeline:</p> <pre><code>$ docetl run pipeline_opt.yaml\n[09:28:17] Performing syntax check on all operations...\n           Syntax check passed for all operations.\n           Running Operation:\n             Type: map\n             Name: extract_themes_and_viewpoints\n\u2827 Running step debate_analysis...\n[09:28:36] Intermediate saved for operation 'extract_themes_and_viewpoints'\n           Running Operation:\n             Type: unnest\n             Name: unnest_themes\n           Intermediate saved for operation 'unnest_themes'\n           Running Operation:\n             Type: resolve\n             Name: synthesized_resolve_0\n[09:28:38] Comparisons saved by blocking: 56002 (97.75%)\n\u280b Running step debate_analysis...\n[09:29:02] Number of keys before resolution: 339\n           Number of distinct keys after resolution: 152\n\u280b Running step debate_analysis...\n[09:29:04] Self-join selectivity: 0.0390\n           Intermediate saved for operation 'synthesized_resolve_0'\n           Running Operation:\n             Type: reduce\n             Name: summarize_theme_evolution\n\u283c Running step debate_analysis...\n[09:29:54] Intermediate saved for operation 'summarize_theme_evolution'\n           Flushing cache to disk...\n           Cache flushed to disk.\n  Step debate_analysis completed. Cost: $0.29\n  Operation extract_themes_and_viewpoints completed. Cost: $0.16\n  Operation unnest_themes completed. Cost: $0.00\n  Operation synthesized_resolve_0 completed. Cost: $0.04\n  Operation summarize_theme_evolution completed. Cost: $0.09\n           \ud83d\udcbe Output saved to theme_evolution_analysis_baseline.json\n           Total cost: $0.29\n           Total time: 97.25 seconds\n</code></pre> <p>This output shows the progress of our pipeline execution, including the different operations performed, intermediate saves, and the final results. Note the total cost was only $0.29!</p>"},{"location":"examples/presidential-debate-themes/#initial-results","title":"Initial Results","text":"<p>Our pipeline generated reports on various themes discussed in the presidential debates. We've put the results up here. However, upon inspection, we found that these reports were lacking in depth and recency. Let's look at a few examples:</p> <p>Example Reports Lacking in Recent Quotes</p> Infrastructure DevelopmentCrime and Gun ControlDrug Policy <pre><code># Infrastructure Development: A Comparative Analysis of Democratic and Republican Viewpoints from 1992 to 2023\n\n## Introduction\nInfrastructure development has long been a pivotal theme in American political discourse, with varying perspectives presented by major party candidates. This report analyzes shifts and trends in Democratic and Republican viewpoints from 1992, during the second presidential debate between George Bush, Bill Clinton, and Ross Perot, to 2023.\n\n## Republican Viewpoints\n### Early 1990s\nIn 1992, President George Bush emphasized a forward-looking approach to infrastructure, stating, \"We passed this year the most furthest looking transportation bill in the history of this country...$150 billion for improving the infrastructure.\" This statement indicated a commitment to substantial federal investment in infrastructure aimed at enhancing transportation networks.\n\n### 2000s\nMoving into the early 2000s, the Republican party maintained a focus on infrastructure but began to frame it within the context of economic growth and public-private partnerships. However, after the 2008 financial crisis, there was a noticeable shift. The party emphasized tax cuts and reducing regulation over large public investments in infrastructure.\n\n### Recent Years\nBy 2020 and 2021, under the Trump administration, the emphasis returned to infrastructure. However, the tone shifted towards emphasizing private sector involvement and deregulation rather than large public spending. The Republican approach became more fragmented, with some factions calling for aggressive infrastructure investment, while others remained cautious about expenditures.\n\n## Democratic Viewpoints\n### Early 1990s\nIn contrast, Governor Bill Clinton in 1992 proposed a more systematic investment strategy, noting, \"My plan would dedicate $20 billion a year in each of the next 4 years for investments in new transportation.\" This highlighted a stronger emphasis on direct federal involvement in infrastructure as a means of fostering economic opportunity and job creation.\n\n### 2000s\nThrough the late 1990s and early 2000s, the Democratic party continued to push for comprehensive federal infrastructure plans, often attached to broader economic initiatives aimed at reducing inequality and spurring job growth. The party emphasized sustainable infrastructure and investments that address climate change.\n\n### Recent Years\nBy 2020, under the Biden administration, the Democrat viewpoint strongly advocated for significant infrastructure investments, combining traditional infrastructure with climate resilience. The American Jobs Plan symbolized this shift, proposing vast funds for transit systems, renewable energy projects, and rural broadband internet. The framing increasingly included social equity as a core component of infrastructure, influenced by movements advocating for racial and economic justice.\n\n## Agreements and Disagreements\n### Agreements\nDespite inherent differences, both parties have historically acknowledged the necessity of infrastructure investments for economic growth. Both Bush and Clinton in 1992 recognized infrastructure as vital for job creation, but diverged on the scope and funding mechanisms.\n\n### Disagreements\nOver the years, major disagreements have surfaced, particularly in funding approaches. The Republican party has increasingly favored private sector involvement and tax incentives, while Democrats have consistently pushed for robust federal spending and the incorporation of progressive values into infrastructure projects.\n\n## Influencing Factors\nThe evolution of viewpoints has often mirrored external events such as economic recessions, technological advancement, and social movements. The post-9/11 era and the 2008 financial crisis notably shifted priorities, with bipartisan discussions centered around recovery through infrastructure spending. Additionally, increasing awareness of climate change and social justice has over the years significantly influenced Democratic priorities, leading to a more inclusive and sustainable approach to infrastructure development.\n\n## Conclusion\nThe comparative analysis of Democratic and Republican viewpoints on infrastructure development from 1992 to 2023 reveals significant shifts in priorities and strategies. While both parties agree on the need for infrastructure improvements, their approaches and underlying philosophies continue to diverge, influenced by economic, social, and environmental factors.\n</code></pre> <pre><code>## The Evolution of Democratic and Republican Viewpoints on Crime and Gun Control: 1992-2023\n\n### Introduction\nThis report analyzes the shifting perspectives of the Democratic and Republican parties on the theme of \"Crime and Gun Control\" from 1992 to 2023. The exploration encompasses key debates, significant shifts in stance, party alignments, and influences from external events that shaped these viewpoints.\n\n### Democratic Party Viewpoints\n1. **Initial Stance (1992)**: In the early 1990s, the Democratic viewpoint, as exemplified by Governor Bill Clinton during the Second Presidential Debate in 1992, supported individual gun ownership but emphasized the necessity of regulation: \"I support the right to keep and bear arms...but I believe we have to have some way of checking handguns before they're sold.\"\n- **Trend**: This reflects a moderate position seeking to balance gun rights with public safety\u2014a common theme in Democratic rhetoric during this era that resonated with many constituents.\n\n2. **Shift Towards Stricter Gun Control (Late 1990s - 2000s)**: Following events such as the Columbine High School shooting in 1999, the Democratic Party increasingly advocated for more stringent gun control measures. The passing of the Brady Bill and the Assault Weapons Ban in 1994 marked a peak in regulatory measures supported by Democrats, emphasizing public safety over gun ownership rights.\n- **Quote Impact**: During this time, Democratic leaders often highlighted the need for legislative action to combat rising gun violence.\n\n3. **Response to Mass Shootings (2010s)**: The tragic events of the Sandy Hook Elementary School shooting in 2012 ignited a renewed push for gun control from leading Democrats, including then-President Barack Obama. His call for \"common-sense gun laws\" marked a decisive moment in Democrat advocacy, focusing on background checks and bans on assault weapons.\n- **Quote**: Obama stated, \"We can't tolerate this anymore. These tragedies must end. And to end them, we must change.\"\n\n4. **Current Stance (2020s)**: The Democratic viewpoint has continued to become increasingly aligned with comprehensive gun control measures, including calls for universal background checks and red flag laws. In the wake of ongoing gun violence, this approach highlights a commitment to addressing systemic issues related to crime and public safety.\n\n### Republican Party Viewpoints\n1. **Consistent Support for Gun Rights (1992)**: In the same 1992 debate, President George Bush emphasized the rights of gun owners, stating, \"I'm a sportsman and I don't think you ought to eliminate all kinds of weapons.\" This illustrates a steadfast commitment to Second Amendment rights that has characterized Republican positions over the years.\n- **Trend**: The Republican Party has traditionally promoted a pro-gun agenda, often resisting calls for stricter regulations or bans.\n\n2. **Response to Gun Control Advocacy (2000s)**: As Democrats pushed for stricter gun laws, Republicans increasingly framed these measures as infringements on individual rights. The response to high-profile shootings tended to focus on mental health and crime prevention rather than gun regulation.\n- **Disagreement**: Republicans consistently argued against the effectiveness of gun control, indicating belief in personal responsibility and the right to self-defense.\n\n3. **Shift to Increased Resistance (2010s)**: In the wake of prominent mass shootings, the Republican party maintained its focus on supporting gun rights, opposing federal gun control initiatives. Notable figures, such as former NRA spokesperson Dana Loesch, articulated this resistance by stating, \"We are not going to let tragedies be used to violate our rights.\"\n- **Impact of External Events**: The rise of organizations like the NRA and increased gun ownership among constituents have fortified this pro-gun stance.\n\n4. **Contemporary Stance (2020s)**: Currently, the Republican viewpoint remains largely unchanged with an emphasis on individual rights to bear arms and skepticism regarding the effectiveness of gun control laws. Recent discussions around gun violence often focus on addressing crime through law enforcement and community safety programs instead of legislative gun restrictions.\n\n### Key Agreements and Disagreements\n- **Common Ground**: Both parties, at different times, have recognized the necessity for addressing gun-related violence but diverge on methods\u2014Democrats typically advocate for regulations while Republicans focus on rights preservation.\n- **Disagreements**: A significant divide exists on whether stricter gun laws equate to reduced crime rates, with Republicans consistently refuting this correlation, arguing instead that law-abiding citizens need access to firearms for self-defense.\n\n### Conclusion\nThe evolution of viewpoints on crime and gun control from 1992 to 2023 highlights a pronounced divergence between the Democratic and Republican parties. While Democrats have increasingly pursued stricter regulatory measures focused on public safety, Republicans have maintained a consistent advocacy for gun rights, underscoring a broader ideological conflict over individual freedoms and collective responsibility for public safety. The trajectories of both parties reflect their core values and responses to notable events impacting society.\n</code></pre> <pre><code>## Evolution of Drug Policy Viewpoints: 1996 to 2023\n\n### Summary of Democratic and Republican Perspectives on Drug Policy\nOver the years, drug policy has been a contentious issue in American politics, reflecting profound changes within both the Democratic and Republican parties. This report examines how views have evolved, highlighting significant trends, areas of agreement and disagreement, and influential external factors utilizing debates as primary reference points.\n\n### Democratic Party Trends:\n1. **Increased Emphasis on Comprehensive Approaches**:\n   - In the 1996 Clinton-Dole debate, President Bill Clinton stated, \"We have dramatically increased control and enforcement at the border.\" This reflects a focus on enforcement as part of drug policy. However, Clinton's later years signaled a shift towards recognizing the need for treatment and prevention.\n\n2. **Emergence of Harm Reduction and Decriminalization**:\n   - Moving into the 2000s and beyond, Democrats began to embrace harm reduction strategies. For instance, during the 2020 Democratic primary debates, candidates such as Bernie Sanders and Elizabeth Warren emphasized decriminalization and treatment over incarceration, signifying a notable shift from punitive measures.\n\n3. **Growing Advocacy for Social Justice**:\n   - Recent years have seen an alignment with social justice movements, arguing that drug policy disproportionately affects marginalized communities. Kamala Harris in 2020 stated, \"We need to truly decriminalize marijuana and address the impact of the War on Drugs on communities of color.\"\n\n### Republican Party Trends:\n1. **Strong Focus on Law and Order**:\n   - During the 1996 debate, Senator Bob Dole reflected a traditional Republican stance, highlighting concerns over drugs without markedly addressing the social implications. \"The President doesn't want to politicize drugs, but it's already politicized Medicare...\" displays a defensive posture toward the political ramifications of drug issues.\n\n2. **Shift Towards Treatment and Prevention**:\n   - By the mid-2010s, there was a growing recognition of the opioid crisis, leading to a bipartisan approach promoting treatment. For example, former President Donald Trump, in addressing the opioid epidemic, stated, \"We have to take care of our people. We can't just lock them up.\"\n\n3. **Conflict Between Hardline Stance and Pragmatism**:\n   - Despite some shifts, many Republicans still emphasize law enforcement solutions. This tension was evident in the polarizing responses to marijuana legalization across states, with figures like former Attorney General Jeff Sessions taking a hardline stance against marijuana legalization, contrasting with more progressive approaches adopted by some Republican governors.\n\n### Areas of Agreement:\n1. **Opioid Crisis**:\n   - Both parties acknowledged the severity of the opioid epidemic, leading to legislation aimed at addressing addiction and treatment, indicating a rare consensus on the need for health-focused solutions.\n\n### Areas of Disagreement:\n1. **Approach to Drug Policy**:\n   - The Democratic party's shift towards decriminalization and harm reduction contrasts sharply with segments of the Republican party that still advocate for strict enforcement and criminalization of certain drugs.\n\n### Influential External Events and Factors:\n1. **The Opioid Crisis**:\n   - The rise of the opioid epidemic has forced both parties to reevaluate their positions on drug policy, pushing them towards more compassionate approaches focusing on addiction treatment.\n\n2. **Social Justice Movements**:\n   - The Black Lives Matter movement and other social justice efforts have altered the discourse surrounding drug policies, with increased focus on the need to rectify injustices in enforcement practices, particularly among minorities.\n\n### Conclusion:\nThrough the years, drug policy viewpoints within the Democratic and Republican parties have experienced significant evolution, characterized by complex layers of agreement and disagreement. As social dynamics shift, both parties continue to grapple with finding a balanced approach towards a more effective drug policy that prioritizes health and social justice.\n</code></pre> <p>Upon inspecting the intermediates, it appears that the map operation is doing a good job at extracting relevant information. The issue seems to lie in the reduce operation, which is ignoring some of the analysis.</p> <p>It's possible that trying to summarize all the insights across all debates for a topic in a single LLM call is too ambitious. To address this, we can set <code>optimize: true</code> for the reduce operation.</p> <p>Let's update our <code>summarize_theme_evolution</code> operation in the pipeline:</p> <pre><code>- name: summarize_theme_evolution\n  type: reduce\n  reduce_key: theme\n  optimize: true\n  output:\n    schema:\n      theme: str\n      report: str\n  prompt: |\n    [... existing prompt ...]\n</code></pre> <p>Rerunning the build command <code>docetl build pipeline.yaml</code> will synthesize the optimized reduce operation (make sure <code>pipeline.yaml</code> is the pipeline you want to optimize).</p>"},{"location":"examples/presidential-debate-themes/#running-the-pipeline-with-optimized-reduce","title":"Running the Pipeline (with Optimized Reduce)","text":"<p>In our optimized pipeline, we see that DocETL added a <code>gleaning</code> configuration to the reduce operation:</p> <pre><code>- name: summarize_theme_evolution\n  type: reduce\n  reduce_key: theme\n  ...\n  gleaning:\n    num_rounds: 1\n    validation_prompt: |\n        1. Does the output adequately summarize the evolution of viewpoints on the theme based on the\n        provided debate texts? Are all critical shifts and trends mentioned?\n        2. Are there any crucial quotes or data points missing from the output that\n        were present in the debate transcripts that could reinforce the analysis?\n        3. Is the output well-structured and easy to follow, following any\n        formatting guidelines specified in the prompt, such as using headings for\n        sections or maintaining a coherent narrative flow?\n</code></pre> <p>Tip</p> <p>Note that you should always feel free to edit the <code>validation_prompt</code> to better suit your specific needs! The optimizer uses LLMs to write all prompts, but you have the best context on your task and what you're looking for in the output, so you should adjust anything accordingly.</p> <p>And when running the pipeline, we can observe the impact of this optimization; for example, one of the outputs gets amended to include more recent quotes:</p> <pre><code>Validator improvements (gleaning round 1):\n1. **Coverage of Critical Shifts and Trends**: While the output captures several significant trends and shifts in Democratic and Republican viewpoints, it could benefit from a more thorough overview, especially about the changing perceptions and proposals related to economic downturns and recoveries across the decades. For instance, it could include how the response to the 2008 financial crisis tied back to historical precedents, linking back to earlier debates about the importance of jobs and middle-class stability (like in the 1976 or 1992 debates).\n\n2. **Missing Quotes and Data Points**: The response could further bolster the analysis by incorporating additional quotes that illustrate the evolving narrative, particularly surrounding tax fairness and job creation. For example, quotes like George Bush in 1984 calling out \"working man\" perspectives against seemingly progressive taxation could enhance the depth. Additionally, quotes from debates emphasizing the impact of tax cuts on economic recovery and job growth\u2014such as Obama's focus on the automotive industry's recovery in 2012 or McCain's 'putting homeowners first'\u2014could provide essential context to the arguments for both parties.\n\n3. **Structure and Flow**: Overall, the output is fairly well-structured and maintains a logical flow, using headings appropriately to signal key sections. However, it may benefit from clearer subsections under each party's overview to delineate specific key points, such as 'Tax Policy', 'Job Creation', and 'Response to Economic Crises'. This would enhance readability and assist the reader in navigating the shifts in viewpoints. For example, adding bullet points or more vivid transitions between historical periods could clarify the evolution timeline. Moreover, resolving any redundancy (such as multiple mentions of similar themes across years) would streamline the narrative.\n</code></pre> <p>Check out the new output here to see the improvements made by the optimized pipeline! Of course, we can probably optimize the initial map operation too, do prompt engineering, and more to further enhance the pipeline.</p> <p>Interactive Pipeline Creation</p> <p>We're currently building interfaces to interactively create and iterate on these pipelines after seeing outputs. This will allow for more intuitive pipeline development and refinement based on real-time results. If you're interested in this feature or would like to provide input, please reach out to us! Your feedback can help shape the future of DocETL's user experience.</p>"},{"location":"examples/rate-limiting/","title":"Rate Limiting","text":"<p>When using DocETL, you might have rate limits based on your usage tier with various API providers. To help manage these limits and prevent exceeding them, DocETL allows you to configure rate limits in your YAML configuration file.</p>"},{"location":"examples/rate-limiting/#configuring-rate-limits","title":"Configuring Rate Limits","text":"<p>You can add rate limits to your YAML config by including a <code>rate_limits</code> key with specific configurations for different types of API calls. Here's an example of how to set up rate limits:</p> <pre><code>rate_limits:\n  embedding_call:\n    - count: 1000\n      per: 1\n      unit: second\n  llm_call:\n    - count: 1\n      per: 1\n      unit: second\n    - count: 10\n      per: 5\n      unit: hour\n</code></pre> <p>Your YAML configuration should have a <code>rate_limits</code> key with the config as shown above. This example sets limits for embedding calls and language model (LLM) calls, with multiple rules for LLM calls to accommodate different time scales.</p> <p>You can also use rate limits in the Python API, passing in a <code>rate_limits</code> dictionary when you initialize the <code>Pipeline</code> object.</p>"},{"location":"examples/split-gather/","title":"Split and Gather Example: Analyzing Trump Immunity Case","text":"<p>This example demonstrates how to use the Split and Gather operations in DocETL to process and analyze a large legal document, specifically the government's motion for immunity determinations in the case against former President Donald Trump. You can download the dataset we'll be using here. This dataset contains a single document.</p>"},{"location":"examples/split-gather/#problem-statement","title":"Problem Statement","text":"<p>We want to analyze a lengthy legal document to identify all people involved in the Trump vs. United States case regarding presidential immunity. The document is too long to process in a single operation, so we need to split it into manageable chunks and then gather context to ensure each chunk can be analyzed effectively.</p>"},{"location":"examples/split-gather/#chunking-strategy","title":"Chunking Strategy","text":"<p>When dealing with long documents, it's often necessary to break them down into smaller, manageable pieces. This is where the Split and Gather operations come in handy:</p> <ol> <li> <p>Split Operation: This divides the document into smaller chunks based on token count or delimiters. For legal documents, using a token count method is often preferable to ensure consistent chunk sizes.</p> </li> <li> <p>Gather Operation: After splitting, we use the Gather operation to add context to each chunk. This operation can include content from surrounding chunks, as well as document-level metadata and headers, ensuring that each piece maintains necessary context for accurate analysis.</p> </li> </ol> <p>Pipeline Overview</p> <p>Our pipeline will follow these steps:</p> <ol> <li>Extract metadata from the full document</li> <li>Split the document into chunks</li> <li>Extract headers from each chunk</li> <li>Gather context for each chunk</li> <li>Analyze each chunk to identify people and their involvements in the case</li> <li>Reduce the results to compile a comprehensive list of people and their roles</li> </ol>"},{"location":"examples/split-gather/#example-pipeline-and-output","title":"Example Pipeline and Output","text":"<p>Here's a breakdown of the pipeline defined in trump-immunity_opt.yaml:</p> <ol> <li> <p>Dataset Definition:     We define a dataset (json file) with a single document.</p> </li> <li> <p>Metadata Extraction:     We define a map operation to extract any document-level metadata that we want to pass to each chunk being processed.</p> </li> <li> <p>Split Operation:     The document is split into chunks using the following configuration:</p> <pre><code>- name: split_find_people_and_involvements\n  type: split\n  method: token_count\n  method_kwargs:\n    num_tokens: 3993\n  split_key: extracted_text\n</code></pre> <p>This operation splits the document into chunks of approximately 3993 tokens each. This size is chosen to balance between maintaining context and staying within model token limits. <code>split_key</code> should be the field in the document that contains the text to split.</p> </li> <li> <p>Header Extraction:     We define a map operation to extract headers from each chunk. Then, when rendering each chunk, we can also render the headers in levels above the headers in the chunk--ensuring that we can maintain hierarchical context, even when the headers are in other chunks.</p> </li> <li> <p>Gather Operation:     Context is gathered for each chunk using the following configuration:</p> <pre><code>- name: gather_extracted_text_find_people_and_involvements\n  type: gather\n  content_key: extracted_text_chunk # (1)!\n  doc_header_key: headers # (2)!\n  doc_id_key: split_find_people_and_involvements_id # (3)!\n  order_key: split_find_people_and_involvements_chunk_num # (4)!\n  peripheral_chunks:\n    next:\n      head:\n        count: 1\n    previous:\n      tail:\n        count: 1\n</code></pre> <ol> <li>The field containing the chunk content; the split_key with \"_chunk\" appended. Automatically exists as a result of the split operation. This is required.</li> <li>The field containing the extracted headers for each chunk. Only exists if you have a header extraction map operation. This can be omitted if you don't have headers extracted for each chunk.</li> <li>The unique identifier for each document; the split operation name with \"_id\" appended. Automatically exists as a result of the split operation. This is required.</li> <li>The field indicating the order of chunks; the split operation name with \"_chunk_num\" appended. Automatically exists as a result of the split operation. This is required.</li> </ol> <p>This operation gathers context for each chunk, including the previous chunk, the current chunk, and the next chunk. We also render the headers populated by the previous operation.</p> </li> <li> <p>Chunk Analysis:     We define a map operation to analyze each chunk.</p> </li> <li> <p>Result Reduction:     We define a reduce operation to reduce the results of the map operation (applied to each chunk) to a single list of people and their involvements in the case.</p> </li> </ol> <p>Here is the full pipeline configuration, with the split and gather operations highlighted. Assuming the sample dataset looks like this:</p> <pre><code>[\n  {\n    \"pdf_url\": \"https://storage.courtlistener.com/recap/gov.uscourts.dcd.258148/gov.uscourts.dcd.258148.252.0.pdf\"\n  }\n]\n</code></pre> Full Pipeline Configuration <pre><code>datasets:\n  legal_doc:\n    type: file\n    path: /path/to/your/dataset.json\n    parsing: # (1)!\n      - function: azure_di_read\n        input_key: pdf_url\n        output_key: extracted_text\n        function_kwargs:\n          use_url: true\n          include_line_numbers: true\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_metadata_find_people_and_involvements\n    type: map\n    model: gpt-4o-mini\n    prompt: |\n      Given the document excerpt: {{ input.extracted_text }}\n      Extract all the people mentioned and summarize their involvements in the case described.\n    output:\n      schema:\n        metadata: str\n\n  - name: split_find_people_and_involvements\n    type: split\n    method: token_count\n    method_kwargs:\n      num_tokens: 3993\n    split_key: extracted_text\n\n  - name: header_extraction_extracted_text_find_people_and_involvements\n    type: map\n    model: gpt-4o-mini\n    output:\n      schema:\n        headers: \"list[{header: string, level: integer}]\"\n    prompt: |\n      Analyze the following chunk of a document and extract any headers you see.\n\n      { input.extracted_text_chunk }\n\n      Examples of headers and their levels based on the document structure:\n      - \"GOVERNMENT'S MOTION FOR IMMUNITY DETERMINATIONS\" (level 1)\n      - \"Legal Framework\" (level 1)\n      - \"Section I\" (level 2)\n      - \"Section II\" (level 2)\n      - \"Section III\" (level 2)\n      - \"A. Formation of the Conspiracies\" (level 3)\n      - \"B. The Defendant Knew that His Claims of Outcome-Determinative Fraud Were False\" (level 3)\n      - \"1. Arizona\" (level 4)\n      - \"2. Georgia\" (level 4)\n\n  - name: gather_extracted_text_find_people_and_involvements\n    type: gather\n    content_key: extracted_text_chunk\n    doc_header_key: headers\n    doc_id_key: split_find_people_and_involvements_id\n    order_key: split_find_people_and_involvements_chunk_num\n    peripheral_chunks:\n      next:\n        head:\n          count: 1\n      previous:\n        tail:\n          count: 1\n\n  - name: submap_find_people_and_involvements\n    type: map\n    model: gpt-4o-mini\n    output:\n      schema:\n        people_and_involvements: list[str]\n    prompt: |\n      Given the document excerpt: {{ input.extracted_text_chunk_rendered }}\n      Extract all the people mentioned and summarize their involvements in the case described. Only process the main chunk.\n\n  - name: subreduce_find_people_and_involvements\n    type: reduce\n    model: gpt-4o-mini\n    associative: true\n    pass_through: true\n    synthesize_resolve: false\n    output:\n      schema:\n        people_and_involvements: list[str]\n    reduce_key:\n      - split_find_people_and_involvements_id\n    prompt: |\n      Given the following extracted information about individuals involved in the case, compile a comprehensive list of people and their specific involvements in the case:\n\n      {% for chunk in inputs %}\n      {% for involvement in chunk.people_and_involvements %}\n      - {{ involvement }}\n      {% endfor %}\n      {% endfor %}\n\n      Make sure to include all the people and their involvements. If a person has multiple involvements, group them together.\n\npipeline:\n  steps:\n    - name: analyze_document\n      input: legal_doc\n      operations:\n        - extract_metadata_find_people_and_involvements\n        - split_find_people_and_involvements\n        - header_extraction_extracted_text_find_people_and_involvements\n        - gather_extracted_text_find_people_and_involvements\n        - submap_find_people_and_involvements\n        - subreduce_find_people_and_involvements\n\n  output:\n    type: file\n    path: /path/to/your/output/people_and_involvements.json\n    intermediate_dir: /path/to/your/intermediates\n</code></pre> <ol> <li>This is an example parsing function, as explained in the Parsing docs. You can define your own parsing function to extract the text you want to split, or just have the text be directly in the json file.</li> </ol> <p>Running the pipeline with <code>docetl run pipeline.yaml</code> will execute the pipeline and save the output to the path specified in the output section. It cost $0.05 and took 23.8 seconds with gpt-4o-mini.</p> <p>Here's a table with one column listing all the people mentioned in the case and their involvements:</p> Final Output People Involved in the Case and Their Involvements DONALD J. TRUMP: Defendant accused of orchestrating a criminal scheme to overturn the 2020 presidential election results through deceit and collaboration with private co-conspirators; charged with leading conspiracies to overturn the 2020 presidential election; made numerous claims of election fraud and pressured officials to find votes to overturn the election results; incited a crowd to march to the Capitol; communicated with various officials regarding election outcomes; exerted political pressure on Vice President Pence; publicly attacked fellow party members for not supporting his claims; involved in spreading false claims about the election, including through Twitter; pressured state legislatures to take unlawful actions regarding electors; influenced campaign decisions and narrative regarding the election results; called for action to overturn the certified results and demanded compliance from officials; worked with co-conspirators on efforts to promote fraudulent elector plans and led actions that culminated in the Capitol riot. MICHAEL R. PENCE: Vice President at the time, pressured by Trump to obstruct Congress's certification of the election; informed Trump there was no evidence of significant fraud; encouraged Trump to accept election results; involved in discussions with Trump regarding election challenges and strategies; publicly asserted his constitutional limitations in the face of Trump's pressure; became the target of attacks from Trump and the Capitol rioters; sought to distance himself from Trump's efforts to overturn the election. CC1: Private attorney who Trump enlisted to falsely claim victory and perpetuate fraud allegations; participated in efforts to influence political actions in targeted states; suggested the defendant declare victory despite ongoing counting; actively involved in making false fraud claims regarding the election; pressured state officials; spread false claims about election irregularities and raised threats against election workers; coordinated fraudulent elector meetings and misrepresented legal bases. CC2: Mentioned as a private co-conspirator involved in the efforts to invalidate election results; proposed illegal strategies to influence the election certification; urged others to decertify legitimate electors; involved in discussions influencing state officials; pressured Mike Pence to act against certification; experienced disappointment with Pence's rejection of proposed strategies; presented unlawful plans to key figures. CC3: Another private co-conspirator involved in scheming to undermine legitimate vote counts; promoted false claims during public hearings and made remarks inciting fraud allegations; encouraged fraudulent election lawsuits and made claims about voting machines; pressured other officials regarding claims of election fraud. CC5: Private political operative who collaborated in the conspiracy; worked on coordinating actions related to the fraudulent elector plan; engaged in text discussions regarding the electors and strategized about the fraud claims. CC6: Private political advisor providing strategic guidance to Trump's re-election efforts; involved in communications with campaign staff regarding the electoral vote processes. P1: Private political advisor who assisted with Trump's re-election campaign; advocated declaring victory before final counts; maintained a podcast spreading false claims about the election. P2: Trump's Campaign Manager, providing campaign direction during the election aftermath; informed the defendant regarding false claims related to state actions. P3: Deputy Campaign Manager, involved in assessing election outcomes; coordinated with team members discussing legal strategies post-election; marked by frequent contact with Trump regarding campaign operations. P4: Senior Campaign Advisor, part of the team advising Trump on election outcome communication; expressed skepticism about allegations of fraud; contradicted Trump's claims about deceased voters in Georgia. P5: Campaign operative and co-conspirator, instructed to create chaos during vote counting and incited unrest at polling places; engaged in discussions about the elector plan. P6: Private citizen campaign advisor who provided early warnings regarding the election outcome; engaged in discussions about the validity of allegations. P7: White House staffer and campaign volunteer who advised Trump on potential election challenges and outcomes; acted as a conduit between Trump and various officials; communicated political advice relevant to the election. P8: Staff member of Pence, who communicated about the electoral process and advised against Trump's unlawful plans; was involved in discussions of political strategy surrounding election results. P9: White House staffer who became a link between Trump and campaign efforts regarding fraud claims; provided truthful assessments of the situation; facilitated communications during post-election fraud discussions. P12: Attended non-official legislative hearings; involved in spreading disinformation about election irregularities. P15: Assistant to the President who overheard Trump's private comments about fighting to remain in power after the 2020 election; involved in discussions about various election-related strategies. P16: Governor of Arizona; received calls from Trump regarding election fraud claims and the count in Arizona. P18: Speaker of the Arizona State House contacted as part of efforts to challenge election outcomes; also expressed reservations about Trump's strategies. P21: Chief of Staff who exchanged communications about the fraudulent allegations; facilitated discussions and logistics during meetings. P22: Campaign attorney who verified that claims about deceased voters were false; participated in discussions around the integrity of the election results. P26: Georgia Attorney General contacted regarding fraud claims; openly stated there was no substantive evidence to support fraud allegations; discussed Texas v. Pennsylvania lawsuit with Trump. P33: Georgia Secretary of State; defended election integrity publicly; stated rumors of election fraud were false; involved in discussions about the impact of fraudulent elector claims in Georgia. P39: RNC Chairwoman; advised against lobbying with state legislators; coordinated with Trump on fraudulent elector efforts; refused to promote inaccurate reports regarding election fraud. P47: Philadelphia City Commissioner; stated there was no evidence of widespread fraud; targeted by Trump for criticism after his public statements. P52: Attorney General who publicly stated that there was no evidence of fraud that would affect election results; faced pressure from Trump's narrative. P50: CISA Director; publicly declared the election secure; faced backlash after contradicting Trump's claims about election fraud. P53: Various Republican U.S. Senators participated in rallies organized by Trump; linked to his campaign efforts regarding the election process. P54: Campaign staff member involved in strategizing about elector votes; discussed procedures and expectations surrounding election tasks and claims. P57: Former U.S. Representative who opted out of the fraudulent elector plan in Pennsylvania; cited legal concerns about the actions being proposed. P58: A staff member of Pence involved in communications directing Pence regarding official duties, managing conversations surrounding election processes. P59: Community organizers who were engaged in discussions relating to Trump's electoral undertakings. P60: Individual responses to Trump's directives aimed at influencing ongoing election outcomes and legislative actions."},{"location":"examples/split-gather/#optional-compiling-a-pipeline-into-a-split-gather-pipeline","title":"Optional: Compiling a Pipeline into a Split-Gather Pipeline","text":"<p>You can also compile a pipeline into a split-gather pipeline using the <code>docetl build</code> command. Say we had a much simpler pipeline for the same document analysis task as above, with just one map operation to extract people and their involvements.</p> <pre><code>default_model: gpt-4o-mini\n\ndatasets:\n  legal_doc: # (1)!\n    path: /path/to/dataset.json\n    type: file\n    parsing: # (2)!\n      - input_key: pdf_url\n        function: azure_di_read\n        output_key: extracted_text\n        function_kwargs:\n          use_url: true\n          include_line_numbers: true\n\noperations:\n  - name: find_people_and_involvements\n    type: map\n    optimize: true\n    prompt: |\n      Given this document, extract all the people and their involvements in the case described by the document.\n\n      {{ input.extracted_text }}\n\n      Return a list of people and their involvements in the case.\n    output:\n      schema:\n        people_and_involvements: list[str]\n\npipeline:\n  steps:\n    - name: analyze_document\n      input: legal_doc\n      operations:\n        - find_people_and_involvements\n\n  output:\n    type: file\n    path: \"/path/to/output/people_and_involvements.json\"\n</code></pre> <ol> <li>This is an example parsing function, as explained in the Parsing docs. You can define your own parsing function to extract the text you want to split, or just have the text be directly in the json file. If you want the text directly in the json file, you can have your json be a list of objects with a single field \"extracted_text\".</li> <li>You can remove this parsing section if you don't need to parse the document (i.e., if the text is already in the json file in the \"extracted_text\" field in the object).</li> </ol> <p>In the pipeline above, we don't have any split or gather operations. Running <code>docetl build pipeline.yaml [--model=gpt-4o-mini]</code> will output a new pipeline_opt.yaml file with the split and gather operations highlighted--like we had defined in the previous example. Note that this cost us $20 to compile, since we tried a bunch of different plans...</p>"},{"location":"execution/running-pipelines/","title":"Additional Notes","text":"<p>Here are some additional notes to help you get the most out of your pipeline:</p> <ul> <li>Sampling Operations: If you want to run an operation on a random sample of your data, you can set the sample parameter for that operation. For example:</li> </ul> <pre><code>operations:\n  extract_medications:\n    sample: 100 # This will run the operation on a random sample of 100 items\n    # ... rest of the operation configuration\n</code></pre> <ul> <li> <p>Caching: DocETL caches the results of operations by default. This means that if you run the same operation on the same data multiple times, the results will be retrieved from the cache rather than being recomputed. You can clear the cache by running docetl clear-cache.</p> </li> <li> <p>The run Function: The main entry point for running a pipeline is the run function in docetl/cli.py. Here's a description of its parameters and functionality:</p> </li> </ul> <p>Run the configuration specified in the YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Path</code> <p>Path to the YAML file containing the pipeline configuration.</p> <code>Argument(..., help='Path to the YAML file containing the pipeline configuration')</code> <code>max_threads</code> <code>Optional[int]</code> <p>Maximum number of threads to use for running operations.</p> <code>Option(None, help='Maximum number of threads to use for running operations')</code> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef run(\n    yaml_file: Path = typer.Argument(\n        ..., help=\"Path to the YAML file containing the pipeline configuration\"\n    ),\n    max_threads: Optional[int] = typer.Option(\n        None, help=\"Maximum number of threads to use for running operations\"\n    ),\n):\n    \"\"\"\n    Run the configuration specified in the YAML file.\n\n    Args:\n        yaml_file (Path): Path to the YAML file containing the pipeline configuration.\n        max_threads (Optional[int]): Maximum number of threads to use for running operations.\n    \"\"\"\n    # Get the current working directory (where the user called the command)\n    cwd = os.getcwd()\n\n    # Load .env file from the current working directory\n    env_file = os.path.join(cwd, \".env\")\n    if os.path.exists(env_file):\n        load_dotenv(env_file)\n\n    runner = DSLRunner.from_yaml(str(yaml_file), max_threads=max_threads)\n    runner.run()\n</code></pre> <p>handler: python   options:     members: - run   show_root_full_path: true   show_root_toc_entry: true   show_root_heading: true   show_source: false   show_name: true</p> <ul> <li>Intermediate Output: If you provide an intermediate directory in your configuration, the outputs of each operation will be saved to this directory. This allows you to inspect the results of individual steps in the pipeline and can be useful for debugging or analyzing the pipeline's progress. Set the intermediate_dir parameter in your pipeline's output configuration to specify the directory where intermediate results should be saved; e.g.,</li> </ul> <pre><code>pipeline:\n  output:\n    type: file\n    path: ...\n    intermediate_dir: intermediate_results\n</code></pre>"},{"location":"operators/cluster/","title":"Cluster operation","text":"<p>The Cluster operation in DocETL groups all items into a binary tree using agglomerative clustering of the embedding of some keys, and annotates each item with the path through this tree down to the item (Note that the path is reversed, starting with the most specific grouping, and ending in the root of the tree, the cluster that encompasses all your input).</p> <p>Each cluster is summarized using an llm prompt, taking the summaries of its children as inputs (or for the leaf nodes, the actual items).</p>"},{"location":"operators/cluster/#example-grouping-concepts-from-a-knowledge-graph","title":"\ud83d\ude80 Example: Grouping concepts from a knowledge-graph","text":"<pre><code>- name: cluster_concepts\n  type: cluster\n  max_batch_size: 5\n  embedding_keys:\n    - concept\n    - description\n  output_key: categories # This is optional, and defaults to \"clusters\"\n  summary_schema:\n    concept: str\n    description: str\n  summary_prompt: |\n    The following describes two related concepts. What concept\n    encompasses both? Try not to be too broad; it might be that one of\n    these two concepts already encompasses the other; in that case,\n    you should just use that concept.\n\n    {{left.concept}}:\n    {{left.description}}\n\n    {{right.concept}}:\n    {{right.description}}\n\n    Provide the title of the super-concept, and a description.\n</code></pre> <p>This cluster operation processes a set of concepts, each with a title and a description, and groups them into a tree of categories.</p> Sample Input and Output <p>Input: <pre><code>[\n  {\n    \"concept\": \"Shed\",\n    \"description\": \"A shed is typically a simple, single-story roofed structure, often used for storage, for hobbies, or as a workshop, and typically serving as outbuilding, such as in a back garden or on an allotment. Sheds vary considerably in their size and complexity of construction, from simple open-sided ones designed to cover bicycles or garden items to large wood-framed structures with shingled roofs, windows, and electrical outlets. Sheds used on farms or in the industry can be large structures. The main types of shed construction are metal sheathing over a metal frame, plastic sheathing and frame, all-wood construction (the roof may be asphalt shingled or sheathed in tin), and vinyl-sided sheds built over a wooden frame. Small sheds may include a wooden or plastic floor, while more permanent ones may be built on a concrete pad or foundation. Sheds may be lockable to deter theft or entry by children, domestic animals, wildlife, etc.\"\n  },\n  {\n    \"concept\": \"Barn\",\n    \"description\": \"A barn is an agricultural building usually on farms and used for various purposes. In North America, a barn refers to structures that house livestock, including cattle and horses, as well as equipment and fodder, and often grain.[2] As a result, the term barn is often qualified e.g. tobacco barn, dairy barn, cow house, sheep barn, potato barn. In the British Isles, the term barn is restricted mainly to storage structures for unthreshed cereals and fodder, the terms byre or shippon being applied to cow shelters, whereas horses are kept in buildings known as stables.[2][3] In mainland Europe, however, barns were often part of integrated structures known as byre-dwellings (or housebarns in US literature). In addition, barns may be used for equipment storage, as a covered workplace, and for activities such as threshing.\"\n  },\n  {\n    \"concept\": \"Tree house\",\n    \"description\": \"A tree house, tree fort or treeshed, is a platform or building constructed around, next to or among the trunk or branches of one or more mature trees while above ground level. Tree houses can be used for recreation, work space, habitation, a hangout space and observation. People occasionally connect ladders or staircases to get up to the platforms.\"\n  },\n  {\n    \"concept\": \"Castle\",\n    \"description\": \"A castle is a type of fortified structure built during the Middle Ages predominantly by the nobility or royalty and by military orders. Scholars usually consider a castle to be the private fortified residence of a lord or noble. This is distinct from a mansion, palace, and villa, whose main purpose was exclusively for pleasance and are not primarily fortresses but may be fortified.[a] Use of the term has varied over time and, sometimes, has also been applied to structures such as hill forts and 19th- and 20th-century homes built to resemble castles. Over the Middle Ages, when genuine castles were built, they took on a great many forms with many different features, although some, such as curtain walls, arrowslits, and portcullises, were commonplace.\"\n  },\n  {\n    \"concept\": \"Fortress\",\n    \"description\": \"A fortification (also called a fort, fortress, fastness, or stronghold) is a military construction designed for the defense of territories in warfare, and is used to establish rule in a region during peacetime. The term is derived from Latin fortis ('strong') and facere ('to make'). From very early history to modern times, defensive walls have often been necessary for cities to survive in an ever-changing world of invasion and conquest. Some settlements in the Indus Valley Civilization were the first small cities to be fortified. In ancient Greece, large stone walls had been built in Mycenaean Greece, such as the ancient site of Mycenae (known for the huge stone blocks of its 'cyclopean' walls). A Greek phrourion was a fortified collection of buildings used as a military garrison, and is the equivalent of the Roman castellum or fortress. These constructions mainly served the purpose of a watch tower, to guard certain roads, passes, and borders. Though smaller than a real fortress, they acted as a border guard rather than a real strongpoint to watch and maintain the border.\"\n  }\n]\n</code></pre></p> <p>Output: <pre><code>[\n  {\n    \"concept\": \"Shed\",\n    \"description\": \"A shed is typically a simple, single-story roofed structure, often used for storage, for hobbies, or as a workshop, and typically serving as outbuilding, such as in a back garden or on an allotment. Sheds vary considerably in their size and complexity of construction, from simple open-sided ones designed to cover bicycles or garden items to large wood-framed structures with shingled roofs, windows, and electrical outlets. Sheds used on farms or in the industry can be large structures. The main types of shed construction are metal sheathing over a metal frame, plastic sheathing and frame, all-wood construction (the roof may be asphalt shingled or sheathed in tin), and vinyl-sided sheds built over a wooden frame. Small sheds may include a wooden or plastic floor, while more permanent ones may be built on a concrete pad or foundation. Sheds may be lockable to deter theft or entry by children, domestic animals, wildlife, etc.\",\n    \"categories\": [\n      {\n        \"distance\": 0.9907871670904073,\n        \"concept\": \"Outbuildings\",\n        \"description\": \"Outbuildings are structures that are separate from a main building, typically located on a property for purposes such as storage, workshops, or housing animals and equipment. This category includes structures like sheds and barns, which serve specific functions like storing tools, equipment, or livestock.\"\n      },\n      {\n        \"distance\": 1.148880974178631,\n        \"concept\": \"Auxiliary Structures\",\n        \"description\": \"Auxiliary structures are secondary or additional buildings that serve various practical purposes related to a main dwelling or property. This category encompasses structures like tree houses and outbuildings, which provide functional, recreational, or storage spaces, often designed to enhance the usability of the property.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  },\n  {\n    \"concept\": \"Barn\",\n    \"description\": \"A barn is an agricultural building usually on farms and used for various purposes. In North America, a barn refers to structures that house livestock, including cattle and horses, as well as equipment and fodder, and often grain.[2] As a result, the term barn is often qualified e.g. tobacco barn, dairy barn, cow house, sheep barn, potato barn. In the British Isles, the term barn is restricted mainly to storage structures for unthreshed cereals and fodder, the terms byre or shippon being applied to cow shelters, whereas horses are kept in buildings known as stables.[2][3] In mainland Europe, however, barns were often part of integrated structures known as byre-dwellings (or housebarns in US literature). In addition, barns may be used for equipment storage, as a covered workplace, and for activities such as threshing.\",\n    \"categories\": [\n      {\n        \"distance\": 0.9907871670904073,\n        \"concept\": \"Outbuildings\",\n        \"description\": \"Outbuildings are structures that are separate from a main building, typically located on a property for purposes such as storage, workshops, or housing animals and equipment. This category includes structures like sheds and barns, which serve specific functions like storing tools, equipment, or livestock.\"\n      },\n      {\n        \"distance\": 1.148880974178631,\n        \"concept\": \"Auxiliary Structures\",\n        \"description\": \"Auxiliary structures are secondary or additional buildings that serve various practical purposes related to a main dwelling or property. This category encompasses structures like tree houses and outbuildings, which provide functional, recreational, or storage spaces, often designed to enhance the usability of the property.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  },\n  {\n    \"concept\": \"Tree house\",\n    \"description\": \"A tree house, tree fort or treeshed, is a platform or building constructed around, next to or among the trunk or branches of one or more mature trees while above ground level. Tree houses can be used for recreation, work space, habitation, a hangout space and observation. People occasionally connect ladders or staircases to get up to the platforms.\",\n    \"categories\": [\n      {\n        \"distance\": 1.148880974178631,\n        \"concept\": \"Auxiliary Structures\",\n        \"description\": \"Auxiliary structures are secondary or additional buildings that serve various practical purposes related to a main dwelling or property. This category encompasses structures like tree houses and outbuildings, which provide functional, recreational, or storage spaces, often designed to enhance the usability of the property.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  },\n  {\n    \"concept\": \"Castle\",\n    \"description\": \"A castle is a type of fortified structure built during the Middle Ages predominantly by the nobility or royalty and by military orders. Scholars usually consider a castle to be the private fortified residence of a lord or noble. This is distinct from a mansion, palace, and villa, whose main purpose was exclusively for pleasance and are not primarily fortresses but may be fortified.[a] Use of the term has varied over time and, sometimes, has also been applied to structures such as hill forts and 19th- and 20th-century homes built to resemble castles. Over the Middle Ages, when genuine castles were built, they took on a great many forms with many different features, although some, such as curtain walls, arrowslits, and portcullises, were commonplace.\",\n    \"categories\": [\n      {\n        \"distance\": 0.9152435235428339,\n        \"concept\": \"Fortified structures\",\n        \"description\": \"Fortified structures refer to buildings designed to protect from attacks and enhance defense. This category encompasses various forms of military architecture, including castles and fortresses. Castles serve as private residences for nobility or military orders with substantial fortification features, while fortresses are broader military constructions aimed at defending territories and establishing control. Both types share the common purpose of defense against invasion, though they serve different social and functional roles.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  },\n  {\n    \"concept\": \"Fortress\",\n    \"description\": \"A fortification (also called a fort, fortress, fastness, or stronghold) is a military construction designed for the defense of territories in warfare, and is used to establish rule in a region during peacetime. The term is derived from Latin fortis ('strong') and facere ('to make'). From very early history to modern times, defensive walls have often been necessary for cities to survive in an ever-changing world of invasion and conquest. Some settlements in the Indus Valley Civilization were the first small cities to be fortified. In ancient Greece, large stone walls had been built in Mycenaean Greece, such as the ancient site of Mycenae (known for the huge stone blocks of its 'cyclopean' walls). A Greek phrourion was a fortified collection of buildings used as a military garrison, and is the equivalent of the Roman castellum or fortress. These constructions mainly served the purpose of a watch tower, to guard certain roads, passes, and borders. Though smaller than a real fortress, they acted as a border guard rather than a real strongpoint to watch and maintain the border.\",\n    \"categories\": [\n      {\n        \"distance\": 0.9152435235428339,\n        \"concept\": \"Fortified structures\",\n        \"description\": \"Fortified structures refer to buildings designed to protect from attacks and enhance defense. This category encompasses various forms of military architecture, including castles and fortresses. Castles serve as private residences for nobility or military orders with substantial fortification features, while fortresses are broader military constructions aimed at defending territories and establishing control. Both types share the common purpose of defense against invasion, though they serve different social and functional roles.\"\n      },\n      {\n        \"distance\": 1.292957924480073,\n        \"concept\": \"Military and Support Structures\",\n        \"description\": \"Military and support structures refer to various types of constructions designed for specific functions related to defense and utility. This concept encompasses fortified structures, such as castles and fortresses, built for protection and military purposes, as well as auxiliary structures that serve practical roles for main buildings, including storage, recreation, and additional facilities. Together, these structures enhance the safety, functionality, and usability of a property or territory.\"\n      }\n    ]\n  }\n]\n</code></pre></p>"},{"location":"operators/cluster/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: A unique name for the operation.</li> <li><code>type</code>: Must be set to \"cluster\".</li> <li><code>embedding_keys</code>: A list of keys to use for the embedding that is clustered on</li> <li><code>summary_prompt</code>: The prompt used to summarize a cluster based on its children. Access input variables with <code>left.keyname</code> or <code>right.keyname</code>.</li> <li><code>summary_schema</code>: The schema for the summary of each cluster. This is the output schema for the <code>summary_prompt</code> based llm call.</li> </ul>"},{"location":"operators/cluster/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>output_key</code> The name of the output key where the cluster path will be inserted in the items. \"clusters\" <code>model</code> The language model to use Falls back to <code>default_model</code> <code>embedding_model</code> The embedding model to use \"text-embedding-3-small\" <code>tools</code> List of tool definitions for LLM use None <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>validate</code> List of Python expressions to validate the output None <code>sample</code> Number of items to sample for this operation None"},{"location":"operators/equijoin/","title":"Equijoin Operation (Experimental)","text":"<p>The Equijoin operation in DocETL is an experimental feature designed for joining two datasets based on flexible, LLM-powered criteria. It leverages many of the same techniques as the Resolve operation, but applies them to the task of joining datasets rather than deduplicating within a single dataset.</p>"},{"location":"operators/equijoin/#motivation","title":"Motivation","text":"<p>While traditional database joins rely on exact matches, real-world data often requires more nuanced joining criteria. Equijoin allows for joins based on semantic similarity or complex conditions, making it ideal for scenarios where exact matches are impossible or undesirable.</p>"},{"location":"operators/equijoin/#example-matching-job-candidates-to-job-postings","title":"\ud83d\ude80 Example: Matching Job Candidates to Job Postings","text":"<p>Let's explore a practical example of using the Equijoin operation to match job candidates with suitable job postings based on skills and experience.</p> <pre><code>- name: match_candidates_to_jobs\n  type: equijoin\n  comparison_prompt: |\n    Compare the following job candidate and job posting:\n\n    Candidate Skills: {{ left.skills }}\n    Candidate Experience: {{ left.years_experience }}\n\n    Job Required Skills: {{ right.required_skills }}\n    Job Desired Experience: {{ right.desired_experience }}\n\n    Is this candidate a good match for the job? Consider both the overlap in skills and the candidate's experience level. Respond with \"True\" if it's a good match, or \"False\" if it's not a suitable match.\n  output:\n    schema:\n      match_score: float\n      match_rationale: string\n</code></pre> <p>This Equijoin operation matches job candidates to job postings:</p> <ol> <li>It uses the <code>comparison_prompt</code> to determine if a candidate is a good match for a job.</li> <li>The operation can be optimized to use efficient blocking rules, reducing the number of comparisons.</li> </ol> <p>Jinja2 Syntax with left and right</p> <p>The prompt template uses Jinja2 syntax, allowing you to reference input fields directly (e.g., <code>left.skills</code>). You can reference the left and right documents using <code>left</code> and <code>right</code> respectively.</p> <p>Performance Consideration</p> <p>For large datasets, running comparisons with an LLM can be time-consuming. It's recommended to optimize your pipeline using <code>docetl build pipeline.yaml</code> to generate efficient blocking rules for the operation.</p>"},{"location":"operators/equijoin/#blocking","title":"Blocking","text":"<p>Like the Resolve operation, Equijoin supports blocking techniques to improve efficiency. For details on how blocking works and how to implement it, please refer to the Blocking section in the Resolve operation documentation.</p>"},{"location":"operators/equijoin/#parameters","title":"Parameters","text":"<p>Equijoin shares many parameters with the Resolve operation. For a detailed list of required and optional parameters, please see the Parameters section in the Resolve operation documentation.</p> <p>Key differences for Equijoin include:</p> <ul> <li><code>resolution_prompt</code> is not used in Equijoin.</li> <li><code>limits</code> parameter is specific to Equijoin, allowing you to set maximum matches for each left and right item.</li> </ul>"},{"location":"operators/equijoin/#incorporating-into-a-pipeline","title":"Incorporating Into a Pipeline","text":"<p>Here's an example of how to incorporate the Equijoin operation into a pipeline using the job candidate matching scenario:</p> <pre><code>model: gpt-4o-mini\n\ndatasets:\n  candidates:\n    type: file\n    path: /path/to/candidates.json\n  job_postings:\n    type: file\n    path: /path/to/job_postings.json\n\noperations:\n  match_candidates_to_jobs:\n    type: equijoin\n    join_key:\n      left:\n        name: candidate_id\n      right:\n        name: job_id\n    comparison_prompt: |\n      Compare the following job candidate and job posting:\n\n      Candidate Skills: {{ left.skills }}\n      Candidate Experience: {{ left.years_experience }}\n\n      Job Required Skills: {{ right.required_skills }}\n      Job Desired Experience: {{ right.desired_experience }}\n\n      Is this candidate a good match for the job? Consider both the overlap in skills and the candidate's experience level. Respond with \"True\" if it's a good match, or \"False\" if it's not a suitable match.\n    output:\n      schema:\n        match_score: float\n        match_rationale: string\n\npipeline:\n  steps:\n    - name: match_candidates_to_jobs\n      operations:\n        - match_candidates_to_jobs:\n            left: candidates\n            right: job_postings\n\n  output:\n    type: file\n    path: \"/path/to/matched_candidates_jobs.json\"\n</code></pre> <p>This pipeline configuration demonstrates how to use the Equijoin operation to match job candidates with job postings. The pipeline reads candidate and job posting data from JSON files, performs the matching using the defined comparison prompt, and outputs the results to a new JSON file.</p>"},{"location":"operators/equijoin/#best-practices","title":"Best Practices","text":"<ol> <li>Leverage the Optimizer: Use <code>docetl build pipeline.yaml</code> to automatically generate efficient blocking rules for your Equijoin operation.</li> <li>Craft Thoughtful Comparison Prompts: Design prompts that effectively determine whether two records should be joined based on your specific use case.</li> <li>Balance Precision and Recall: When optimizing, consider the trade-off between catching all potential matches and reducing unnecessary comparisons.</li> <li>Mind Resource Constraints: Use <code>limit_comparisons</code> if you need to cap the total number of comparisons for large datasets.</li> <li>Iterate and Refine: Start with a small sample of your data to test and refine your join criteria before running on the full dataset.</li> </ol> <p>For additional best practices that apply to both Resolve and Equijoin operations, see the Best Practices section in the Resolve operation documentation.</p>"},{"location":"operators/filter/","title":"Filter Operation","text":"<p>The Filter operation in DocETL is used to selectively process data items based on specific conditions. It behaves similarly to the Map operation, but with a key difference: items that evaluate to false are filtered out of the dataset, allowing you to include or exclude data points from further processing in your pipeline.</p>"},{"location":"operators/filter/#motivation","title":"Motivation","text":"<p>Filtering is crucial when you need to:</p> <ul> <li>Focus on the most relevant data points</li> <li>Remove noise or irrelevant information from your dataset</li> <li>Create subsets of data for specialized analysis</li> <li>Optimize downstream processing by reducing data volume</li> </ul>"},{"location":"operators/filter/#example-filtering-high-impact-news-articles","title":"\ud83d\ude80 Example: Filtering High-Impact News Articles","text":"<p>Let's look at a practical example of using the Filter operation to identify high-impact news articles based on certain criteria.</p> <pre><code>- name: filter_high_impact_articles\n  type: filter\n  prompt: |\n    Analyze the following news article:\n    Title: \"{{ input.title }}\"\n    Content: \"{{ input.content }}\"\n\n    Determine if this article is high-impact based on the following criteria:\n    1. Covers a significant global or national event\n    2. Has potential long-term consequences\n    3. Affects a large number of people\n    4. Is from a reputable source\n\n    Respond with 'true' if the article meets at least 3 of these criteria, otherwise respond with 'false'.\n\n  output:\n    schema:\n      is_high_impact: boolean\n\n  model: gpt-4-turbo\n  validate:\n    - isinstance(output[\"is_high_impact\"], bool)\n</code></pre> <p>This Filter operation processes news articles and determines whether they are \"high-impact\" based on specific criteria. Unlike a Map operation, which would process all articles and add an \"is_high_impact\" field to each, this Filter operation will only pass through articles that meet the criteria, effectively removing low-impact articles from the dataset.</p> Sample Input and Output <p>Input: <pre><code>[\n  {\n    \"title\": \"Global Climate Summit Reaches Landmark Agreement\",\n    \"content\": \"In a historic move, world leaders at the Global Climate Summit have unanimously agreed to reduce carbon emissions by 50% by 2030. This unprecedented agreement involves all major economies and sets binding targets for renewable energy adoption, reforestation, and industrial emissions reduction. Experts hail this as a turning point in the fight against climate change, with potential far-reaching effects on global economies, energy systems, and everyday life for billions of people.\"\n  },\n  {\n    \"title\": \"Local Bakery Wins Best Croissant Award\",\n    \"content\": \"Downtown's favorite bakery, 'The Crusty Loaf', has been awarded the title of 'Best Croissant' in the annual City Food Festival. Owner Maria Garcia attributes the win to their use of imported French butter and a secret family recipe. Local food critics praise the bakery's commitment to traditional baking methods.\"\n  }\n]\n</code></pre></p> <p>Output: <pre><code>[\n  {\n    \"title\": \"Global Climate Summit Reaches Landmark Agreement\",\n    \"content\": \"In a historic move, world leaders at the Global Climate Summit have unanimously agreed to reduce carbon emissions by 50% by 2030. This unprecedented agreement involves all major economies and sets binding targets for renewable energy adoption, reforestation, and industrial emissions reduction. Experts hail this as a turning point in the fight against climate change, with potential far-reaching effects on global economies, energy systems, and everyday life for billions of people.\"\n  }\n]\n</code></pre></p> <p>This example demonstrates how the Filter operation distinguishes between high-impact news articles and those of more local or limited significance. The climate summit article is retained in the dataset due to its global significance, long-term consequences, and wide-ranging effects. The local bakery story, while interesting, doesn't meet the criteria for a high-impact article and is filtered out of the dataset.</p>"},{"location":"operators/filter/#configuration","title":"Configuration","text":""},{"location":"operators/filter/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: A unique name for the operation.</li> <li><code>type</code>: Must be set to \"filter\".</li> <li><code>prompt</code>: The prompt template to use for the filtering condition. Access input variables with <code>input.keyname</code>.</li> <li><code>output</code>: Schema definition for the output from the LLM. It must include only one field, a boolean field.</li> </ul>"},{"location":"operators/filter/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>model</code> The language model to use Falls back to <code>default_model</code> <code>optimize</code> Flag to enable operation optimization True <code>recursively_optimize</code> Flag to enable recursive optimization false <code>sample_size</code> Number of samples to use for the operation Processes all data <code>validate</code> List of Python expressions to validate the output None <code>num_retries_on_validate_failure</code> Number of retry attempts on validation failure 0 <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>sample</code> Number of samples to use for the operation None <p>Validation</p> <p>For more details on validation techniques and implementation, see operators.</p>"},{"location":"operators/filter/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Criteria: Define clear and specific criteria for filtering in your prompt.</li> <li>Boolean Output: Ensure your prompt guides the LLM to produce a clear boolean output.</li> <li>Data Flow Awareness: Remember that unlike Map, Filter will reduce the size of your dataset. Ensure this aligns with your pipeline's objectives.</li> </ol>"},{"location":"operators/gather/","title":"Gather Operation","text":"<p>The Gather operation in DocETL is designed to maintain context when processing divided documents. It complements the Split operation by adding contextual information from surrounding chunks to each segment.</p>"},{"location":"operators/gather/#motivation","title":"Motivation","text":"<p>When splitting long documents, such as complex legal contracts or court transcripts, individual chunks often lack sufficient context for accurate analysis or processing. This can lead to several challenges:</p> <ul> <li>Loss of reference information (e.g., terms defined in earlier sections)</li> <li>Incomplete representation of complex clauses that span multiple chunks</li> <li>Difficulty in understanding the broader document structure</li> <li>Missing important context from preambles or introductory sections</li> </ul> <p>Context Challenge in Legal Documents</p> <p>Imagine a lengthy merger agreement split into chunks. A single segment might contain clauses referencing \"the Company\" or \"Effective Date\" without clearly defining these terms. Without context from previous chunks, it becomes challenging to interpret the legal implications accurately.</p>"},{"location":"operators/gather/#how-gather-works","title":"How Gather Works","text":"<p>The Gather operation addresses these challenges by:</p> <ol> <li>Identifying relevant surrounding chunks (peripheral context)</li> <li>Adding this context to each chunk</li> <li>Preserving document structure information</li> </ol>"},{"location":"operators/gather/#peripheral-context","title":"Peripheral Context","text":"<p>Peripheral context refers to the surrounding text or information that helps provide a more complete understanding of a specific chunk of content. In legal documents, this can include:</p> <ul> <li>Preceding text that introduces key terms, parties, or conditions</li> <li>Following text that elaborates on clauses presented in the current chunk</li> <li>Document structure information, such as article or section headers</li> <li>Summarized versions of nearby chunks for efficient context provision</li> </ul>"},{"location":"operators/gather/#document-structure","title":"Document Structure","text":"<p>The Gather operation can maintain document structure through header hierarchies. This is particularly useful for preserving the overall structure of complex legal documents like contracts, agreements, or regulatory filings.</p>"},{"location":"operators/gather/#example-enhancing-context-in-legal-document-analysis","title":"\ud83d\ude80 Example: Enhancing Context in Legal Document Analysis","text":"<p>Let's walk through an example of using the Gather operation to process a long merger agreement.</p>"},{"location":"operators/gather/#step-1-extract-metadata-map-operation-before-splitting","title":"Step 1: Extract Metadata (Map operation before splitting)","text":"<p>First, we extract important metadata from the full document:</p> <pre><code>- name: extract_metadata\n  type: map\n  prompt: |\n    Extract the following metadata from the merger agreement:\n    1. Agreement Date\n    2. Parties involved\n    3. Total value of the merger (if specified)\n\n    Agreement text:\n    {{ input.agreement_text }}\n\n    Return the extracted information in a structured format.\n  output:\n    schema:\n      agreement_date: string\n      parties: list[string]\n      merger_value: string\n</code></pre>"},{"location":"operators/gather/#step-2-split-operation","title":"Step 2: Split Operation","text":"<p>Next, we split the document into manageable chunks:</p> <pre><code>- name: split_merger_agreement\n  type: split\n  split_key: agreement_text\n  method: token_count\n  method_kwargs:\n    token_count: 1000\n</code></pre>"},{"location":"operators/gather/#step-3-extract-headers-map-operation","title":"Step 3: Extract Headers (Map operation)","text":"<p>We extract headers from each chunk:</p> <pre><code>- name: extract_headers\n  type: map\n  input:\n    - agreement_text_chunk\n  prompt: |\n    Extract any section headers from the following merger agreement chunk:\n    {{ input.agreement_text_chunk }}\n    Return the headers as a list, preserving their hierarchy.\n  output:\n    schema:\n      headers: \"list[{header: string, level: integer}]\"\n</code></pre>"},{"location":"operators/gather/#step-4-gather-operation","title":"Step 4: Gather Operation","text":"<p>Now, we apply the Gather operation:</p> <pre><code>- name: context_gatherer\n  type: gather\n  content_key: agreement_text_chunk\n  doc_id_key: split_merger_agreement_id\n  order_key: split_merger_agreement_chunk_num\n  peripheral_chunks:\n    previous:\n      middle:\n        content_key: agreement_text_chunk_summary\n      tail:\n        content_key: agreement_text_chunk\n    next:\n      head:\n        count: 1\n        content_key: agreement_text_chunk\n  doc_header_key: headers\n</code></pre>"},{"location":"operators/gather/#step-5-analyze-chunks-map-operation-after-gather","title":"Step 5: Analyze Chunks (Map operation after Gather)","text":"<p>Finally, we analyze each chunk with its gathered context:</p> <pre><code>- name: analyze_chunks\n  type: map\n  input:\n    - agreement_text_chunk_rendered\n    - agreement_date\n    - parties\n    - merger_value\n  prompt: |\n    Analyze the following chunk of a merger agreement, considering the provided metadata:\n\n    Agreement Date: {{ input.agreement_date }}\n    Parties: {{ input.parties | join(', ') }}\n    Merger Value: {{ input.merger_value }}\n\n    Chunk content:\n    {{ input.agreement_text_chunk_rendered }}\n\n    Provide a summary of key points and any potential legal implications in this chunk.\n  output:\n    schema:\n      summary: string\n      legal_implications: list[string]\n</code></pre> <p>This configuration:</p> <ol> <li>Extracts important metadata from the full document before splitting</li> <li>Splits the document into manageable chunks</li> <li>Extracts headers from each chunk</li> <li>Gathers context for each chunk, including:</li> <li>Summaries of the chunks before the previous chunk</li> <li>The full content of the previous chunk</li> <li>The full content of the current chunk</li> <li>The full content of the next chunk</li> <li>Extracted headers for levels directly above headers in the current chunk, for structural context</li> <li>Analyzes each chunk with its gathered context and the extracted metadata</li> </ol>"},{"location":"operators/gather/#configuration","title":"Configuration","text":"<p>The Gather operation includes several key components:</p> <ul> <li><code>type</code>: Always set to \"gather\"</li> <li><code>doc_id_key</code>: Identifies chunks from the same original document</li> <li><code>order_key</code>: Specifies the sequence of chunks within a group</li> <li><code>content_key</code>: Indicates the field containing the chunk content</li> <li><code>peripheral_chunks</code>: Specifies how to include context from surrounding chunks</li> <li><code>doc_header_key</code> (optional): Denotes a field representing extracted headers for each chunk</li> <li><code>sample</code> (optional): Number of samples to use for the operation</li> </ul>"},{"location":"operators/gather/#peripheral-chunks-configuration","title":"Peripheral Chunks Configuration","text":"<p>The <code>peripheral_chunks</code> configuration in the Gather operation is highly flexible, allowing users to precisely control how context is added to each chunk. This configuration determines which surrounding chunks are included and how they are presented.</p>"},{"location":"operators/gather/#structure","title":"Structure","text":"<p>The <code>peripheral_chunks</code> configuration is divided into two main sections:</p> <ol> <li><code>previous</code>: Defines how chunks preceding the current chunk are included.</li> <li><code>next</code>: Defines how chunks following the current chunk are included.</li> </ol> <p>Each of these sections can contain up to three subsections:</p> <ul> <li><code>head</code>: The first chunk(s) in the section.</li> <li><code>middle</code>: Chunks between the <code>head</code> and <code>tail</code> sections.</li> <li><code>tail</code>: The last chunk(s) in the section.</li> </ul>"},{"location":"operators/gather/#configuration-options","title":"Configuration Options","text":"<p>For each subsection, you can specify:</p> <ul> <li><code>count</code>: The number of chunks to include (for <code>head</code> and <code>tail</code> only).</li> <li><code>content_key</code>: The key in the chunk data that contains the content to use.</li> </ul>"},{"location":"operators/gather/#example-configuration","title":"Example Configuration","text":"<pre><code>peripheral_chunks:\n  previous:\n    head:\n      count: 1\n      content_key: full_content\n    middle:\n      content_key: summary_content\n    tail:\n      count: 2\n      content_key: full_content\n  next:\n    head:\n      count: 1\n      content_key: full_content\n</code></pre> <p>This configuration would:</p> <ol> <li>Include the full content of the very first chunk.</li> <li>Include summaries of all chunks between the <code>head</code> and <code>tail</code> of the previous section.</li> <li>Include the full content of 2 chunks immediately before the current chunk.</li> <li>Include the full content of 1 chunk immediately after the current chunk.</li> </ol>"},{"location":"operators/gather/#behavior-details","title":"Behavior Details","text":"<ol> <li> <p>Content Selection:    If a <code>content_key</code> is specified that's different from the main content key, it's treated as a summary. This is useful for including condensed versions of chunks in the <code>middle</code> section to save space. If no <code>content_key</code> is specified, it defaults to the main content key of the operation.</p> </li> <li> <p>Chunk Ordering:    For the <code>previous</code> section, chunks are processed in reverse order (from the current chunk towards the beginning of the document). For the <code>next</code> section, chunks are processed in forward order.</p> </li> <li> <p>Skipped Content:    If there are gaps between included chunks, the operation inserts a note indicating how many characters were skipped. Example: <code>[... 5000 characters skipped ...]</code></p> </li> <li> <p>Chunk Labeling:    Each included chunk is labeled with its order number and whether it's a summary. Example: <code>[Chunk 5 (Summary)]</code> or <code>[Chunk 6]</code></p> </li> </ol>"},{"location":"operators/gather/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Balance Context and Conciseness: Use full content for immediate context (<code>head</code>) and summaries for <code>middle</code> sections to provide context without overwhelming the main content.</p> </li> <li> <p>Adapt to Document Structure: Adjust the <code>count</code> for <code>head</code> and <code>tail</code> based on the typical length of your document sections.</p> </li> <li> <p>Use Asymmetric Configurations: You might want more previous context than next context, or vice versa, depending on your specific use case.</p> </li> <li> <p>Consider Performance: Including too much context can increase processing time and token usage. Use summaries and selective inclusion to optimize performance.</p> </li> </ol> <p>By leveraging this flexible configuration, you can tailor the Gather operation to provide the most relevant context for your specific document processing needs, balancing completeness with efficiency.</p>"},{"location":"operators/gather/#output","title":"Output","text":"<p>The Gather operation adds a new field to each input document, named by appending \"_rendered\" to the <code>content_key</code>. This field contains:</p> <ol> <li>The reconstructed header hierarchy (if applicable)</li> <li>Previous context (if any)</li> <li>The main chunk, clearly marked</li> <li>Next context (if any)</li> <li>Indications of skipped content between contexts</li> </ol> <p>Sample Output for Merger Agreement</p> <pre><code>agreement_text_chunk_rendered: |\n  _Current Section:_ # Article 5: Representations and Warranties &gt; ## 5.1 Representations and Warranties of the Company\n\n  --- Previous Context ---\n  [Chunk 17] ... summary of earlier sections on definitions and parties ...\n  [... 500 characters skipped ...]\n  [Chunk 18] The Company hereby represents and warrants to Parent and Merger Sub as follows, except as set forth in the Company Disclosure Schedule:\n  --- End Previous Context ---\n\n  --- Begin Main Chunk ---\n  5.1.1 Organization and Qualification. The Company is duly organized, validly existing, and in good standing under the laws of its jurisdiction of organization and has all requisite corporate power and authority to own, lease, and operate its properties and to carry on its business as it is now being conducted...\n  --- End Main Chunk ---\n\n  --- Next Context ---\n  [Chunk 20] 5.1.2 Authority Relative to This Agreement. The Company has all necessary corporate power and authority to execute and deliver this Agreement, to perform its obligations hereunder, and to consummate the Merger...\n  [... 750 characters skipped ...]\n  --- End Next Context ---\n</code></pre>"},{"location":"operators/gather/#handling-document-structure","title":"Handling Document Structure","text":"<p>A key feature of the Gather operation is its ability to maintain document structure through header hierarchies. This is particularly useful for preserving the overall structure of complex documents like legal contracts, technical manuals, or research papers.</p>"},{"location":"operators/gather/#how-header-handling-works","title":"How Header Handling Works","text":"<ol> <li>Headers are typically extracted from each chunk using a Map operation after the Split but before the Gather.</li> <li>The Gather operation uses these extracted headers to reconstruct the relevant header hierarchy for each chunk.</li> <li>When rendering a chunk, the operation includes all the most recent headers from higher levels found in previous chunks.</li> <li>This ensures that each rendered chunk includes a complete \"path\" of headers leading to its content, preserving the document's overall structure and context.</li> </ol>"},{"location":"operators/gather/#example-header-handling-in-legal-contracts","title":"Example: Header Handling in Legal Contracts","text":"<p>Let's look at an example of how the Gather operation handles document headers in the context of legal contracts:</p> <p></p> <p>In this figure:</p> <ol> <li>We see two chunks (18 and 20) from a 74-page legal contract.</li> <li>Each chunk goes through a Map operation to extract headers.</li> <li>For Chunk 18, a level 1 header \"Grant of License\" is extracted.</li> <li>For Chunk 20, a level 3 header \"ctDNA Licenses\" is extracted.</li> <li>When rendering Chunk 20, the Gather operation includes:</li> <li>The level 1 header from Chunk 18 (\"Grant of License\")</li> <li>A level 2 header from Chunk 19 (not pictured, but included in the rendered output)</li> <li>The current level 3 header from Chunk 20 (\"ctDNA Licenses\")</li> </ol> <p>This hierarchical structure provides crucial context for understanding the content of Chunk 20, even though the higher-level headers are not directly present in that chunk.</p> <p>Importance of Header Hierarchy</p> <p>By maintaining the header hierarchy, the Gather operation ensures that each chunk is placed in its proper context within the overall document structure. This is especially crucial for complex documents where understanding the relationship between different sections is key to accurate analysis or processing.</p>"},{"location":"operators/gather/#best-practices_1","title":"Best Practices","text":"<ol> <li> <p>Extract Metadata Before Splitting: Run a map operation on the full document before splitting to extract any metadata that could be useful when processing any chunk (e.g., agreement dates, parties involved). Reference this metadata in subsequent map operations after the gather step.</p> </li> <li> <p>Balance Context and Efficiency: For ultra-long documents, consider using summarized versions of chunks in the \"middle\" sections to strike a balance between providing context and managing the overall size of the processed data.</p> </li> <li> <p>Preserve Document Structure: Utilize the <code>doc_header_key</code> parameter to include relevant structural information from the original document, which can be important for understanding the context of complex or structured content.</p> </li> <li> <p>Tailor Context to Your Task: Adjust the <code>peripheral_chunks</code> configuration based on the specific needs of your analysis. Some tasks may require more preceding context, while others might benefit from more following context.</p> </li> <li> <p>Combine with Other Operations: The Gather operation is most powerful when used in conjunction with Split, Map (for summarization or header extraction), and subsequent analysis operations to process long documents effectively.</p> </li> <li> <p>Consider Performance: Be mindful of the increased token count when adding context. Adjust your downstream operations accordingly to handle the larger input sizes, and use summarized context where appropriate to manage token usage.</p> </li> <li> <p>Use <code>next</code> Sparingly: The <code>next</code> parameter in the Gather operation should be used judiciously. It's primarily beneficial in specific scenarios:</p> <ul> <li>When dealing with structured data or tables where the next chunk provides essential context for understanding the current chunk.</li> <li>In cases where the end of a chunk might cut off a sentence or important piece of information that continues in the next chunk.</li> </ul> <p>For most text-based documents, focusing on the <code>prev</code> context is usually sufficient. Overuse of <code>next</code> can lead to unnecessary token consumption and potential redundancy in the gathered output.</p> <p>When to Use <code>next</code></p> <p>Consider using <code>next</code> when processing:</p> <pre><code>- Financial reports with multi-page tables\n- Technical documents where diagrams span multiple pages\n- Legal contracts where clauses might be split across chunk boundaries\n</code></pre> <p>By default, it's recommended to set <code>next=0</code> unless you have a specific need for forward context in your document processing pipeline.</p> </li> </ol>"},{"location":"operators/map/","title":"Map Operation","text":"<p>The Map operation in DocETL applies a specified transformation to each item in your input data, allowing for complex processing and insight extraction from large, unstructured documents.</p>"},{"location":"operators/map/#example-analyzing-long-form-news-articles","title":"\ud83d\ude80 Example: Analyzing Long-Form News Articles","text":"<p>Let's see a practical example of using the Map operation to analyze long-form news articles, extracting key information and generating insights.</p> <pre><code>- name: analyze_news_article\n  type: map\n  prompt: |\n    Analyze the following news article:\n    \"{{ input.article }}\"\n\n    Provide the following information:\n    1. Main topic (1-3 words)\n    2. Summary (2-3 sentences)\n    3. Key entities mentioned (list up to 5, with brief descriptions)\n    4. Sentiment towards the main topic (positive, negative, or neutral)\n    5. Potential biases or slants in reporting (if any)\n    6. Relevant categories (e.g., politics, technology, environment; list up to 3)\n    7. Credibility score (1-10, where 10 is highly credible)\n\n  output:\n    schema:\n      main_topic: string\n      summary: string\n      key_entities: list[object]\n      sentiment: string\n      biases: list[string]\n      categories: list[string]\n      credibility_score: integer\n\n  model: gpt-4o-mini\n  validate:\n    - len(output[\"main_topic\"].split()) &lt;= 3\n    - len(output[\"key_entities\"]) &lt;= 5\n    - output[\"sentiment\"] in [\"positive\", \"negative\", \"neutral\"]\n    - len(output[\"categories\"]) &lt;= 3\n    - 1 &lt;= output[\"credibility_score\"] &lt;= 10\n  num_retries_on_validate_failure: 2\n</code></pre> <p>This Map operation processes long-form news articles to extract valuable insights:</p> <ol> <li>Identifies the main topic of the article.</li> <li>Generates a concise summary.</li> <li>Extracts key entities (people, organizations, locations) mentioned in the article.</li> <li>Analyzes the overall sentiment towards the main topic.</li> <li>Identifies potential biases or slants in the reporting.</li> <li>Categorizes the article into relevant topics.</li> <li>Assigns a credibility score based on the content and sources.</li> </ol> <p>The operation includes validation to ensure the output meets our expectations and will retry up to 2 times if validation fails.</p> Sample Input and Output <p>Input: <pre><code>[\n  {\n    \"article\": \"In a groundbreaking move, the European Union announced yesterday a comprehensive plan to transition all member states to 100% renewable energy by 2050. The ambitious proposal, dubbed 'Green Europe 2050', aims to completely phase out fossil fuels and nuclear power across the continent.\n\n    European Commission President Ursula von der Leyen stated, 'This is not just about fighting climate change; it's about securing Europe's energy independence and economic future.' The plan includes massive investments in solar, wind, and hydroelectric power, as well as significant funding for research into new energy storage technologies.\n\n    However, the proposal has faced criticism from several quarters. Some Eastern European countries, particularly Poland and Hungary, argue that the timeline is too aggressive and could damage their economies, which are still heavily reliant on coal. Industry groups have also expressed concern about the potential for job losses in the fossil fuel sector.\n\n    Environmental groups have largely praised the initiative, with Greenpeace calling it 'a beacon of hope in the fight against climate change.' However, some activists argue that the 2050 target is not soon enough, given the urgency of the climate crisis.\n\n    The plan also includes provisions for a 'just transition,' with billions of euros allocated to retraining workers and supporting regions that will be most affected by the shift away from fossil fuels. Additionally, it proposes stricter energy efficiency standards for buildings and appliances, and significant investments in public transportation and electric vehicle infrastructure.\n\n    Experts are divided on the feasibility of the plan. Dr. Maria Schmidt, an energy policy researcher at the University of Berlin, says, 'While ambitious, this plan is achievable with the right political will and technological advancements.' However, Dr. John Smith from the London School of Economics warns, 'The costs and logistical challenges of such a rapid transition should not be underestimated.'\n\n    As the proposal moves forward for debate in the European Parliament, it's clear that 'Green Europe 2050' will be a defining issue for the continent in the coming years, with far-reaching implications for Europe's economy, environment, and global leadership in climate action.\"\n  }\n]\n</code></pre></p> <p>Output: <pre><code>[\n  {\n    \"main_topic\": \"EU Renewable Energy\",\n    \"summary\": \"The European Union has announced a plan called 'Green Europe 2050' to transition all member states to 100% renewable energy by 2050. The ambitious proposal aims to phase out fossil fuels and nuclear power, invest in renewable energy sources, and includes provisions for a 'just transition' to support affected workers and regions.\",\n    \"key_entities\": [\n      {\n        \"name\": \"European Union\",\n        \"description\": \"Political and economic union of 27 member states\"\n      },\n      {\n        \"name\": \"Ursula von der Leyen\",\n        \"description\": \"European Commission President\"\n      },\n      {\n        \"name\": \"Poland\",\n        \"description\": \"Eastern European country critical of the plan\"\n      },\n      {\n        \"name\": \"Hungary\",\n        \"description\": \"Eastern European country critical of the plan\"\n      },\n      {\n        \"name\": \"Greenpeace\",\n        \"description\": \"Environmental organization supporting the initiative\"\n      }\n    ],\n    \"sentiment\": \"positive\",\n    \"biases\": [\n      \"Slight bias towards environmental concerns over economic impacts\",\n      \"More emphasis on supportive voices than critical ones\"\n    ],\n    \"categories\": [\n      \"Environment\",\n      \"Politics\",\n      \"Economy\"\n    ],\n    \"credibility_score\": 8\n  }\n]\n</code></pre></p> <p>This example demonstrates how the Map operation can transform long, unstructured news articles into structured, actionable insights. These insights can be used for various purposes such as trend analysis, policy impact assessment, and public opinion monitoring.</p>"},{"location":"operators/map/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>name</code>: A unique name for the operation.</li> <li><code>type</code>: Must be set to \"map\".</li> </ul>"},{"location":"operators/map/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>prompt</code> The prompt template to use for the transformation. Access input variables with <code>input.keyname</code>. None <code>output</code> Schema definition for the output from the LLM. None <code>model</code> The language model to use Falls back to <code>default_model</code> <code>optimize</code> Flag to enable operation optimization <code>True</code> <code>recursively_optimize</code> Flag to enable recursive optimization of operators synthesized as part of rewrite rules <code>false</code> <code>sample</code> Number of samples to use for the operation Processes all data <code>tools</code> List of tool definitions for LLM use None <code>validate</code> List of Python expressions to validate the output None <code>num_retries_on_validate_failure</code> Number of retry attempts on validation failure 0 <code>gleaning</code> Configuration for advanced validation and LLM-based refinement None <code>drop_keys</code> List of keys to drop from the input before processing None <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>timeout</code> Timeout for each LLM call in seconds 120 <p>Note: If <code>drop_keys</code> is specified, <code>prompt</code> and <code>output</code> become optional parameters.</p> <p>Validation and Gleaning</p> <p>For more details on validation techniques and implementation, see operators.</p>"},{"location":"operators/map/#advanced-features","title":"Advanced Features","text":""},{"location":"operators/map/#tool-use","title":"Tool Use","text":"<p>Tools can extend the capabilities of the Map operation. Each tool is a Python function that can be called by the LLM during execution, and follows the OpenAI Function Calling API.</p> Tool Definition Example <pre><code>tools:\n- required: true\n    code: |\n    def count_words(text):\n        return {\"word_count\": len(text.split())}\n    function:\n    name: count_words\n    description: Count the number of words in a text string.\n    parameters:\n        type: object\n        properties:\n        text:\n            type: string\n        required:\n        - text\n</code></pre> <p>Warning</p> <p>Tool use and gleaning cannot be used simultaneously.</p>"},{"location":"operators/map/#input-truncation","title":"Input Truncation","text":"<p>If the input doesn't fit within the token limit, DocETL automatically truncates tokens from the middle of the input data, preserving the beginning and end which often contain more important context. A warning is displayed when truncation occurs.</p>"},{"location":"operators/map/#batching","title":"Batching","text":"<p>If you have a really large collection of documents and you don't want to run them through the Map operation at the same time, you can use the <code>batch_size</code> parameter to process data in smaller chunks. This can significantly reduce memory usage and improve performance.</p> <p>To enable batching in your map operations, you need to specify the <code>max_batch_size</code> parameter in your configuration.</p> <pre><code>- name: extract_summaries\n  type: map\n  max_batch_size: 5\n  clustering_method: random\n  prompt: |\n    Summarize this text: \"{{ input.text }}\"\n  output:\n    schema:\n      summary: string\n</code></pre> <p>In the above config, there will be no more than 5 API calls to the LLM at a time (i.e., 5 documents processed at a time, one per API call).</p>"},{"location":"operators/map/#dropping-keys","title":"Dropping Keys","text":"<p>You can use a map operation to act as an LLM no-op, and just drop any key-value pairs you don't want to save to the output file. To do this, you can use the <code>drop_keys</code> parameter.</p> <pre><code>- name: drop_keys_example\n  type: map\n  drop_keys:\n    - \"keyname1\"\n    - \"keyname2\"\n</code></pre>"},{"location":"operators/map/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Prompts: Write clear, specific prompts that guide the LLM to produce the desired output.</li> <li>Robust Validation: Use validation to ensure output quality and consistency.</li> <li>Appropriate Model Selection: Choose the right model for your task, balancing performance and cost.</li> <li>Optimize for Scale: For large datasets, consider using <code>sample</code> to test your operation before running on the full dataset.</li> <li>Use Tools Wisely: Leverage tools for complex calculations or operations that the LLM might struggle with. You can write any Python code in the tools, so you can even use tools to call other APIs or search the internet.</li> </ol>"},{"location":"operators/parallel-map/","title":"Parallel Map Operation","text":"<p>The Parallel Map operation in DocETL applies multiple independent transformations to each item in the input data concurrently, maintaining a 1:1 input-to-output ratio while generating multiple fields simultaneously.</p> <p>Similarity to Map Operation</p> <p>The Parallel Map operation is very similar to the standard Map operation. The key difference is that Parallel Map allows you to define multiple prompts that run concurrently (without having to explicitly create a DAG), whereas a standard Map operation typically involves a single transformation.</p>"},{"location":"operators/parallel-map/#configuration","title":"Configuration","text":"<p>Each prompt in the Parallel Map operation is responsible for generating specific fields of the output. The prompts are executed concurrently, improving efficiency when working with multiple transformations.</p> <p>The output schema should include all the fields generated by the individual prompts, ensuring that the results are combined into a single output item for each input.</p>"},{"location":"operators/parallel-map/#required-parameters","title":"Required Parameters","text":"Parameter Description <code>name</code> A unique name for the operation <code>type</code> Must be set to \"parallel_map\" <code>prompts</code> A list of prompt configurations (see below) <code>output</code> Schema definition for the combined output from all prompts <p>Each prompt configuration in the <code>prompts</code> list should contain:</p> <ul> <li><code>prompt</code>: The prompt template to use for the transformation</li> <li><code>output_keys</code>: List of keys that this prompt will generate</li> <li><code>model</code> (optional): The language model to use for this specific prompt</li> </ul>"},{"location":"operators/parallel-map/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>model</code> The default language model to use Falls back to <code>default_model</code> <code>optimize</code> Flag to enable operation optimization True <code>recursively_optimize</code> Flag to enable recursive optimization false <code>sample</code> Number of samples to use for the operation Processes all data <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 Why use Parallel Map instead of multiple Map operations? <p>While you could achieve similar results with multiple Map operations, Parallel Map offers several advantages:</p> <ol> <li>Concurrency: Prompts run in parallel, potentially reducing overall processing time.</li> <li>Simplified Configuration: You define multiple transformations in a single operation, reducing pipeline complexity.</li> <li>Unified Output: Results from all prompts are combined into a single output item, simplifying downstream operations.</li> </ol>"},{"location":"operators/parallel-map/#example-processing-job-applications","title":"\ud83d\ude80 Example: Processing Job Applications","text":"<p>Here's an example of a parallel map operation that processes job applications by extracting key information and evaluating candidates:</p> <pre><code>- name: process_job_application\n  type: parallel_map\n  prompts:\n    - name: extract_skills\n      prompt: \"Given the following resume: '{{ input.resume }}', list the top 5 relevant skills for a software engineering position.\"\n      output_keys:\n        - skills\n      model: gpt-4o-mini\n    - name: calculate_experience\n      prompt: \"Based on the work history in this resume: '{{ input.resume }}', calculate the total years of relevant experience for a software engineering role.\"\n      output_keys:\n        - years_experience\n      model: gpt-4o-mini\n    - name: evaluate_cultural_fit\n      prompt: \"Analyze the following cover letter: '{{ input.cover_letter }}'. Rate the candidate's potential cultural fit on a scale of 1-10, where 10 is the highest.\"\n      output_keys:\n        - cultural_fit_score\n      model: gpt-4o-mini\n  output:\n    schema:\n      skills: list[string]\n      years_experience: float\n      cultural_fit_score: integer\n</code></pre> <p>This Parallel Map operation processes job applications by concurrently extracting skills, calculating experience, and evaluating cultural fit.</p>"},{"location":"operators/parallel-map/#advantages","title":"Advantages","text":"<ol> <li>Concurrency: Multiple transformations are applied simultaneously, potentially reducing overall processing time.</li> <li>Simplicity: Users can define multiple transformations without needing to create explicit DAGs in the configuration.</li> <li>Flexibility: Different models can be used for different prompts within the same operation.</li> <li>Maintainability: Each transformation can be defined and updated independently, making it easier to manage complex operations.</li> </ol>"},{"location":"operators/parallel-map/#best-practices","title":"Best Practices","text":"<ol> <li>Independent Transformations: Ensure that the prompts in a Parallel Map operation are truly independent of each other to maximize the benefits of concurrent execution.</li> <li>Balanced Prompts: Try to design prompts that have similar complexity and execution times to optimize overall performance.</li> <li>Output Schema Alignment: Ensure that the output schema correctly captures all the fields generated by the individual prompts.</li> </ol>"},{"location":"operators/reduce/","title":"Reduce Operation","text":"<p>The Reduce operation in DocETL aggregates data based on a key. It supports both batch reduction and incremental folding for large datasets, making it versatile for various data processing tasks.</p>"},{"location":"operators/reduce/#motivation","title":"Motivation","text":"<p>Reduce operations are essential when you need to summarize or aggregate data across multiple records. For example, you might want to:</p> <ul> <li>Analyze sentiment trends across social media posts</li> <li>Consolidate patient medical records from multiple visits</li> <li>Synthesize key findings from a set of research papers on a specific topic</li> </ul>"},{"location":"operators/reduce/#example-summarizing-customer-feedback","title":"\ud83d\ude80 Example: Summarizing Customer Feedback","text":"<p>Let's look at a practical example of using the Reduce operation to summarize customer feedback by department:</p> <pre><code>- name: summarize_feedback\n  type: reduce\n  reduce_key: department\n  prompt: |\n    Summarize the customer feedback for the {{ inputs[0].department }} department:\n\n    {% for item in inputs %}\n    Feedback {{ loop.index }}: {{ item.feedback }}\n    {% endfor %}\n\n    Provide a concise summary of the main points and overall sentiment.\n  output:\n    schema:\n      summary: string\n      sentiment: string\n</code></pre> <p>This Reduce operation processes customer feedback grouped by department:</p> <ol> <li>Groups all feedback entries by the 'department' key.</li> <li>For each department, it applies the prompt to summarize the feedback and determine overall sentiment.</li> <li>Outputs a summary and sentiment for each department.</li> </ol>"},{"location":"operators/reduce/#configuration","title":"Configuration","text":""},{"location":"operators/reduce/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>type</code>: Must be set to \"reduce\".</li> <li><code>reduce_key</code>: The key (or list of keys) to use for grouping data. Use <code>_all</code> to group all data into one group.</li> <li><code>prompt</code>: The prompt template to use for the reduction operation.</li> <li><code>output</code>: Schema definition for the output from the LLM.</li> </ul>"},{"location":"operators/reduce/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>sample</code> Number of samples to use for the operation None <code>synthesize_resolve</code> If false, won't synthesize a resolve operation between map and reduce true <code>model</code> The language model to use Falls back to default_model <code>input</code> Specifies the schema or keys to subselect from each item All keys from input items <code>pass_through</code> If true, non-input keys from the first item in the group will be passed through false <code>associative</code> If true, the reduce operation is associative (i.e., order doesn't matter) true <code>fold_prompt</code> A prompt template for incremental folding None <code>fold_batch_size</code> Number of items to process in each fold operation None <code>value_sampling</code> A dictionary specifying the sampling strategy for large groups None <code>verbose</code> If true, enables detailed logging of the reduce operation false <code>persist_intermediates</code> If true, persists the intermediate results for each group to the key <code>_{operation_name}_intermediates</code> false <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2"},{"location":"operators/reduce/#advanced-features","title":"Advanced Features","text":""},{"location":"operators/reduce/#incremental-folding","title":"Incremental Folding","text":"<p>For large datasets, the Reduce operation supports incremental folding. This allows processing of large groups in smaller batches, which can be more efficient and reduce memory usage.</p> <p>To enable incremental folding, provide a <code>fold_prompt</code> and <code>fold_batch_size</code>:</p> <pre><code>- name: large_data_reduce\n  type: reduce\n  reduce_key: category\n  prompt: |\n    Summarize the data for category {{ inputs[0].category }}:\n    {% for item in inputs %}\n    Item {{ loop.index }}: {{ item.data }}\n    {% endfor %}\n  fold_prompt: |\n    Combine the following summaries for category {{ inputs[0].category }}:\n    Current summary: {{ output.summary }}\n    New data:\n    {% for item in inputs %}\n    Item {{ loop.index }}: {{ item.data }}\n    {% endfor %}\n  fold_batch_size: 100\n  output:\n    schema:\n      summary: string\n</code></pre>"},{"location":"operators/reduce/#example-rendered-prompt","title":"Example Rendered Prompt","text":"<p>Rendered Reduce Prompt</p> <p>Let's consider an example of how a reduce operation prompt might look when rendered with actual data. Assume we have a reduce operation that summarizes product reviews, and we're processing reviews for a product with ID \"PROD123\". Here's what the rendered prompt might look like:</p> <pre><code>Summarize the reviews for product PROD123:\n\nReview 1: This laptop is amazing! The battery life is incredible, lasting me a full day of work without needing to charge. The display is crisp and vibrant, perfect for both work and entertainment. The only minor drawback is that it can get a bit warm during intensive tasks.\n\nReview 2: I'm disappointed with this purchase. While the laptop looks sleek, its performance is subpar. It lags when running multiple applications, and the fan noise is quite noticeable. On the positive side, the keyboard is comfortable to type on.\n\nReview 3: Decent laptop for the price. It handles basic tasks well, but struggles with more demanding software. The build quality is solid, and I appreciate the variety of ports. Battery life is average, lasting about 6 hours with regular use.\n\nReview 4: Absolutely love this laptop! It's lightweight yet powerful, perfect for my needs as a student. The touchpad is responsive, and the speakers produce surprisingly good sound. My only wish is that it had a slightly larger screen.\n\nReview 5: Mixed feelings about this product. The speed and performance are great for everyday use and light gaming. However, the webcam quality is poor, which is a letdown for video calls. The design is sleek, but the glossy finish attracts fingerprints easily.\n</code></pre> <p>This example shows how the prompt template is filled with actual review data for a specific product. The language model would then process this prompt to generate a summary of the reviews for the product.</p>"},{"location":"operators/reduce/#scratchpad-technique","title":"Scratchpad Technique","text":"<p>When doing an incremental reduce, the task may require intermediate state that is not represented in the output. For example, if the task is to determine all features more than one person liked about the product, we need some intermediate state to keep track of the features that have been liked once, so if we see the same feature liked again, we can update the output. DocETL maintains an internal \"scratchpad\" to handle this.</p>"},{"location":"operators/reduce/#how-it-works","title":"How it works","text":"<ol> <li>The process starts with an empty accumulator and an internal scratchpad.</li> <li>It sequentially folds in batches of more than one element at a time.</li> <li>The internal scratchpad tracks additional state necessary for accurately solving tasks incrementally. The LLM decides what to write to the scratchpad.</li> <li>During each internal LLM call, the current scratchpad state is used along with the accumulated output and new inputs.</li> <li>The LLM updates both the accumulated output and the internal scratchpad, which are used in the next fold operation.</li> </ol> <p>The scratchpad technique is handled internally by DocETL, allowing users to define complex reduce operations without worrying about the complexities of state management across batches. Users provide their reduce and fold prompts focusing on the desired output, while DocETL uses the scratchpad technique behind the scenes to ensure accurate tracking of trends and efficient processing of large datasets.</p>"},{"location":"operators/reduce/#value-sampling","title":"Value Sampling","text":"<p>For very large groups, you can use value sampling to process a representative subset of the data. This can significantly reduce processing time and costs.</p> <p>The following table outlines the available value sampling methods:</p> Method Description random Randomly select a subset of values first_n Select the first N values cluster Use K-means clustering to select representative samples semantic_similarity Select samples based on semantic similarity to a query <p>To enable value sampling, add a <code>value_sampling</code> configuration to your reduce operation. The configuration should specify the method, sample size, and any additional parameters required by the chosen method.</p> <p>Value Sampling Configuration</p> <pre><code>- name: sampled_reduce\n  type: reduce\n  reduce_key: product_id\n  prompt: |\n    Summarize the reviews for product {{ inputs[0].product_id }}:\n    {% for item in inputs %}\n    Review {{ loop.index }}: {{ item.review }}\n    {% endfor %}\n  value_sampling:\n    enabled: true\n    method: cluster\n    sample_size: 50\n  output:\n    schema:\n      summary: string\n</code></pre> <p>In this example, the Reduce operation will use K-means clustering to select a representative sample of 50 reviews for each product_id.</p> <p>For semantic similarity sampling, you can use a query to select the most relevant samples. This is particularly useful when you want to focus on specific aspects of the data.</p> <p>Semantic Similarity Sampling</p> <pre><code>- name: sampled_reduce_sem_sim\n  type: reduce\n  reduce_key: product_id\n  prompt: |\n    Summarize the reviews for product {{ inputs[0].product_id }}, focusing on comments about battery life and performance:\n    {% for item in inputs %}\n    Review {{ loop.index }}: {{ item.review }}\n    {% endfor %}\n  value_sampling:\n    enabled: true\n    method: sem_sim\n    sample_size: 30\n    embedding_model: text-embedding-3-small\n    embedding_keys:\n      - review\n    query_text: \"Battery life and performance\"\n  output:\n    schema:\n      summary: string\n</code></pre> <p>In this example, the Reduce operation will use semantic similarity to select the 30 reviews most relevant to battery life and performance for each product_id. This allows you to focus the summarization on specific aspects of the product reviews.</p>"},{"location":"operators/reduce/#best-practices","title":"Best Practices","text":"<ol> <li>Choose Appropriate Keys: Select <code>reduce_key</code>(s) that logically group your data for the desired aggregation.</li> <li>Design Effective Prompts: Create prompts that clearly instruct the model on how to aggregate or summarize the grouped data.</li> <li>Consider Data Size: For large datasets, use incremental folding and value sampling to manage processing efficiently.</li> <li>Optimize Your Pipeline: Use <code>docetl build pipeline.yaml</code> to optimize your pipeline, which can introduce efficient merge operations and resolve steps if needed.</li> <li>Balance Precision and Efficiency: When dealing with very large groups, consider using value sampling to process a representative subset of the data.</li> </ol>"},{"location":"operators/resolve/","title":"Resolve Operation","text":"<p>The Resolve operation in DocETL identifies and canonicalizes duplicate entities in your data. It's particularly useful when dealing with inconsistencies that can arise from LLM-generated content, or data from multiple sources.</p>"},{"location":"operators/resolve/#motivation","title":"Motivation","text":"<p>Map operations executed by LLMs may sometimes yield inconsistent results, even when referring to the same entity. For example, when extracting patient names from medical transcripts, you might end up with variations like \"Mrs. Smith\" and \"Jane Smith\" for the same person. In such cases, a Resolve operation on the <code>patient_name</code> field can help standardize patient names before conducting further analysis.</p>"},{"location":"operators/resolve/#example-standardizing-patient-names","title":"\ud83d\ude80 Example: Standardizing Patient Names","text":"<p>Let's see a practical example of using the Resolve operation to standardize patient names extracted from medical transcripts.</p> <pre><code>- name: standardize_patient_names\n  type: resolve\n  optimize: true\n  comparison_prompt: |\n    Compare the following two patient name entries:\n\n    Patient 1: {{ input1.patient_name }}\n    Date of Birth 1: {{ input1.date_of_birth }}\n\n    Patient 2: {{ input2.patient_name }}\n    Date of Birth 2: {{ input2.date_of_birth }}\n\n    Are these entries likely referring to the same patient? Consider name similarity and date of birth. Respond with \"True\" if they are likely the same patient, or \"False\" if they are likely different patients.\n  resolution_prompt: |\n    Standardize the following patient name entries into a single, consistent format:\n\n    {% for entry in inputs %}\n    Patient Name {{ loop.index }}: {{ entry.patient_name }}\n    {% endfor %}\n\n    Provide a single, standardized patient name that represents all the matched entries. Use the format \"LastName, FirstName MiddleInitial\" if available.\n  output:\n    schema:\n      patient_name: string\n</code></pre> <p>This Resolve operation processes patient names to identify and standardize duplicates:</p> <ol> <li>Compares all pairs of patient names using the <code>comparison_prompt</code>. In the prompt, you can reference to the documents via <code>input1</code> and <code>input2</code>.</li> <li>For identified duplicates, it applies the <code>resolution_prompt</code> to generate a standardized name. You can reference all matched entries via the <code>inputs</code> variable.</li> </ol> <p>Note: The prompt templates use Jinja2 syntax, allowing you to reference input fields directly (e.g., <code>input1.patient_name</code>).</p> <p>Performance Consideration</p> <p>You should not run this operation as-is unless your dataset is small! Running O(n^2) comparisons with an LLM can be extremely time-consuming for large datasets. Instead, optimize your pipeline first using <code>docetl build pipeline.yaml</code> and run the optimized version, which will generate efficient blocking rules for the operation. Make sure you've set <code>optimize: true</code> in your resolve operation config.</p>"},{"location":"operators/resolve/#blocking","title":"Blocking","text":"<p>To improve efficiency, the Resolve operation supports \"blocking\" - a technique to reduce the number of comparisons by only comparing entries that are likely to be matches. DocETL supports two types of blocking:</p> <ol> <li>Embedding similarity: Compare embeddings of specified fields and only process pairs above a certain similarity threshold.</li> <li>Python conditions: Apply custom Python expressions to determine if a pair should be compared.</li> </ol> <p>Here's an example of a Resolve operation with blocking:</p> <pre><code>- name: standardize_patient_names\n  type: resolve\n  comparison_prompt: |\n    # (Same as previous example)\n  resolution_prompt: |\n    # (Same as previous example)\n  output:\n    schema:\n      patient_name: string\n  blocking_keys:\n    - last_name\n    - date_of_birth\n  blocking_threshold: 0.8\n  blocking_conditions:\n    - \"left['last_name'][:2].lower() == right['last_name'][:2].lower()\"\n    - \"left['first_name'][:2].lower() == right['first_name'][:2].lower()\"\n    - \"left['date_of_birth'] == right['date_of_birth']\"\n    - \"left['ssn'][-4:] == right['ssn'][-4:]\"\n</code></pre> <p>In this example, pairs will be considered for comparison if:</p> <ul> <li>The embedding similarity of their <code>last_name</code> and <code>date_of_birth</code> fields is above 0.8, OR</li> <li>The <code>last_name</code> fields start with the same two characters, OR</li> <li>The <code>first_name</code> fields start with the same two characters, OR</li> <li>The <code>date_of_birth</code> fields match exactly, OR</li> <li>The last four digits of the <code>ssn</code> fields match.</li> </ul>"},{"location":"operators/resolve/#how-the-comparison-algorithm-works","title":"How the Comparison Algorithm Works","text":"<p>After determining eligible pairs for comparison, the Resolve operation uses a Union-Find (Disjoint Set Union) algorithm to efficiently group similar items. Here's a breakdown of the process:</p> <ol> <li>Initialization: Each item starts in its own cluster.</li> <li>Pair Generation: All possible pairs of items are generated for comparison.</li> <li>Batch Processing: Pairs are processed in batches (controlled by <code>compare_batch_size</code>).</li> <li>Comparison: For each batch:    a. An LLM performs pairwise comparisons to determine if items match.    b. Matching pairs trigger a <code>merge_clusters</code> operation to combine their clusters.</li> <li>Iteration: Steps 3-4 repeat until all pairs are compared.</li> <li>Result Collection: All non-empty clusters are collected as the final result.</li> </ol> <p>Efficiency</p> <p>The batch processing of comparisons allows for efficient, incremental clustering as matches are found, without needing to rebuild the entire cluster structure after each match. This allows for parallelization of LLM calls, improving overall performance. However, this also limits parallelism to the batch size, so choose an appropriate value for <code>compare_batch_size</code> based on your dataset size and system capabilities.</p>"},{"location":"operators/resolve/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>type</code>: Must be set to \"resolve\".</li> <li><code>comparison_prompt</code>: The prompt template to use for comparing potential matches.</li> <li><code>resolution_prompt</code>: The prompt template to use for reducing matched entries.</li> <li><code>output</code>: Schema definition for the output from the LLM.</li> </ul>"},{"location":"operators/resolve/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default <code>embedding_model</code> The model to use for creating embeddings Falls back to <code>default_model</code> <code>resolution_model</code> The language model to use for reducing matched entries Falls back to <code>default_model</code> <code>comparison_model</code> The language model to use for comparing potential matches Falls back to <code>default_model</code> <code>blocking_keys</code> List of keys to use for initial blocking All keys in the input data <code>blocking_threshold</code> Embedding similarity threshold for considering entries as potential matches None <code>blocking_conditions</code> List of conditions for initial blocking [] <code>input</code> Specifies the schema or keys to subselect from each item to pass into the prompts All keys from input items <code>embedding_batch_size</code> The number of entries to send to the embedding model at a time 1000 <code>compare_batch_size</code> The number of entity pairs processed in each batch during the comparison phase 100 <code>limit_comparisons</code> Maximum number of comparisons to perform None <code>timeout</code> Timeout for each LLM call in seconds 120 <code>max_retries_per_timeout</code> Maximum number of retries per timeout 2 <code>sample</code> Number of samples to use for the operation None ## Best Practices <ol> <li>Anticipate Resolve Needs: If you anticipate needing a Resolve operation and want to control the prompts, create it in your pipeline and let the optimizer find the appropriate blocking rules and thresholds.</li> <li>Let the Optimizer Help: The optimizer can detect if you need a Resolve operation (e.g., because there's a downstream reduce operation you're optimizing) and can create a Resolve operation with suitable prompts and blocking rules.</li> <li>Effective Comparison Prompts: Design comparison prompts that consider all relevant factors for determining matches.</li> <li>Detailed Resolution Prompts: Create resolution prompts that effectively standardize or combine information from matched records.</li> <li>Appropriate Model Selection: Choose suitable models for embedding (if used) and language tasks.</li> <li>Optimize Batch Size: If you expect to compare a large number of pairs, consider increasing the <code>compare_batch_size</code>. This parameter effectively limits parallelism, so a larger value can improve performance for large datasets.</li> </ol> <p>Balancing Batch Size</p> <p>While increasing <code>compare_batch_size</code> can improve parallelism, be cautious not to set it too high. Extremely large batch sizes might overwhelm system memory or exceed API rate limits. Consider your system's capabilities and the characteristics of your dataset when adjusting this parameter.</p> <p>The Resolve operation is particularly useful for data cleaning, deduplication, and creating standardized records from multiple data sources. It can significantly improve data quality and consistency in your dataset.</p>"},{"location":"operators/split/","title":"Split Operation","text":"<p>The Split operation in DocETL is designed to divide long text content into smaller, manageable chunks. This is particularly useful when dealing with large documents that exceed the token limit of language models or when the LLM's performance degrades with increasing input size for complex tasks.</p>"},{"location":"operators/split/#motivation","title":"Motivation","text":"<p>Some common scenarios where the Split operation is valuable include:</p> <ul> <li>Processing long customer support transcripts to analyze specific sections</li> <li>Dividing extensive research papers or reports for detailed analysis</li> <li>Breaking down large legal documents to extract relevant clauses or sections</li> <li>Preparing long-form content for summarization or topic extraction</li> </ul>"},{"location":"operators/split/#operation-example-splitting-customer-support-transcripts","title":"\ud83d\ude80 Operation Example: Splitting Customer Support Transcripts","text":"<p>Here's an example of using the Split operation to divide customer support transcripts into manageable chunks:</p> <pre><code>- name: split_transcript\n  type: split\n  split_key: transcript\n  method: token_count\n  method_kwargs:\n    num_tokens: 500\n    model: gpt-4o-mini\n</code></pre> <p>This Split operation processes long customer support transcripts:</p> <ol> <li>Splits the 'transcript' field into chunks of approximately 500 tokens each.</li> <li>Uses the gpt-4o-mini model's tokenizer for accurate token counting.</li> <li>Generates multiple output items for each input item, one for each chunk.</li> </ol> <p>Note that chunks will not overlap in content.</p>"},{"location":"operators/split/#configuration","title":"Configuration","text":""},{"location":"operators/split/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>type</code>: Must be set to \"split\".</li> <li><code>split_key</code>: The key of the field containing the text to split.</li> <li><code>method</code>: The method to use for splitting. Options are \"delimiter\" and \"token_count\".</li> <li><code>method_kwargs</code>: A dictionary of keyword arguments for the splitting method.</li> <li>For \"delimiter\" method: <code>delimiter</code> (string) to use for splitting.</li> <li>For \"token_count\" method: <code>num_tokens</code> (integer) specifying the maximum number of tokens per chunk.</li> </ul>"},{"location":"operators/split/#optional-parameters-in-method_kwargs","title":"Optional Parameters in `method_kwargs","text":"Parameter Description Default <code>model</code> The language model's tokenizer to use Falls back to <code>default_model</code> <code>num_splits_to_group</code> Number of splits to group together into one chunk (only for \"delimiter\" method) 1 <code>sample</code> Number of samples to use for the operation None"},{"location":"operators/split/#splitting-methods","title":"Splitting Methods","text":""},{"location":"operators/split/#token-count-method","title":"Token Count Method","text":"<p>The token count method splits the text into chunks based on a specified number of tokens. This is useful when you need to ensure that each chunk fits within the token limit of your language model, or you know that smaller chunks lead to higher performance.</p>"},{"location":"operators/split/#delimiter-method","title":"Delimiter Method","text":"<p>The delimiter method splits the text based on a specified delimiter string. This is particularly useful when you want to split your text at logical boundaries, such as paragraphs or sections.</p> <p>Delimiter Method Example</p> <p>If you set the <code>delimiter</code> to <code>\"\\n\\n\"</code> (double newline) and <code>num_splits_to_group</code> to 3, each chunk will contain 3 paragraphs.</p> <pre><code>- name: split_by_paragraphs\n  type: split\n  split_key: document\n  method: delimiter\n  method_kwargs:\n    delimiter: \"\\n\\n\"\n  num_splits_to_group: 3\n</code></pre>"},{"location":"operators/split/#output","title":"Output","text":"<p>The Split operation generates multiple output items for each input item:</p> <ul> <li>All original key-value pairs from the input item.</li> <li><code>{split_key}_chunk</code>: The content of the split chunk.</li> <li><code>{op_name}_id</code>: A unique identifier for each original document.</li> <li><code>{op_name}_chunk_num</code>: The sequential number of the chunk within its original document.</li> </ul>"},{"location":"operators/split/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Analyzing Customer Frustration:    Split long support transcripts, then use a map operation to identify frustration indicators in each chunk, followed by a reduce operation to summarize frustration points across the chunks (per transcript).</p> </li> <li> <p>Document Summarization:    Split large documents, apply a map operation for section-wise summarization, then use a reduce operation to compile an overall summary.</p> </li> <li> <p>Topic Extraction from Research Papers:    Divide research papers into sections, use a map operation to extract key topics from each section, then apply a reduce operation to synthesize main themes across the entire paper.</p> </li> </ol>"},{"location":"operators/split/#end-to-end-pipeline-example-analyzing-customer-frustration","title":"\ud83d\ude80 End-to-End Pipeline Example: Analyzing Customer Frustration","text":"<p>Let's walk through a complete example of using Split, Map, and Reduce operations to analyze customer frustration in support transcripts.</p>"},{"location":"operators/split/#step-1-split-operation","title":"Step 1: Split Operation","text":"<pre><code>- name: split_transcript\n  type: split\n  split_key: transcript\n  method: token_count\n  method_kwargs:\n    num_tokens: 500\n    model: gpt-4o-mini\n</code></pre>"},{"location":"operators/split/#step-2-map-operation-identify-frustration-indicators","title":"Step 2: Map Operation (Identify Frustration Indicators)","text":"<pre><code>- name: identify_frustration\n  type: map\n  input:\n    - transcript_chunk\n  prompt: |\n    Analyze the following customer support transcript chunk for signs of customer frustration:\n\n    {{ input.transcript_chunk }}\n\n    Identify any indicators of frustration, such as:\n    1. Use of negative language\n    2. Repetition of issues\n    3. Expressions of dissatisfaction\n    4. Requests for escalation\n\n    Provide a list of frustration indicators found, if any.\n  output:\n    schema:\n      frustration_indicators: list[string]\n</code></pre>"},{"location":"operators/split/#step-3-reduce-operation-summarize-frustration-points","title":"Step 3: Reduce Operation (Summarize Frustration Points)","text":"<pre><code>- name: summarize_frustration\n  type: reduce\n  reduce_key: split_transcript_id\n  associative: false\n  prompt: |\n    Summarize the customer frustration points for this support transcript:\n\n    {% for item in inputs %}\n    Chunk {{ item.split_transcript_chunk_num }}:\n    {% for indicator in item.frustration_indicators %}\n    - {{ indicator }}\n    {% endfor %}\n    {% endfor %}\n\n    Provide a concise summary of the main frustration points and their frequency or intensity across the entire transcript.\n  output:\n    schema:\n      frustration_summary: string\n      primary_issues: list[string]\n      frustration_level: string # e.g., \"low\", \"medium\", \"high\"\n</code></pre> <p>Non-Associative Reduce Operation</p> <p>Note the <code>associative: false</code> parameter in the reduce operation. This is crucial when the order of the chunks matters for your analysis. It ensures that the reduce operation processes the chunks in the order they appear in the original transcript, which is often important for understanding the context and progression of customer frustration.</p>"},{"location":"operators/split/#explanation","title":"Explanation","text":"<ol> <li>The Split operation divides long transcripts into 500-token chunks.</li> <li>The Map operation analyzes each chunk for frustration indicators.</li> <li>The Reduce operation combines the frustration indicators from all chunks of a transcript, summarizing the overall frustration points, primary issues, and assessing the overall frustration level. The <code>associative: false</code> setting ensures that the chunks are processed in their original order.</li> </ol> <p>This pipeline allows for detailed analysis of customer frustration in long support transcripts, which would be challenging to process in a single pass due to token limitations or degraded LLM performance on very long inputs.</p>"},{"location":"operators/split/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Choose the Right Splitting Method: Use the token count method when working with models that have strict token limits. Use the delimiter method when you need to split at logical boundaries in your text.</p> </li> <li> <p>Balance Chunk Size: When using the token count method, choose a chunk size that balances between context preservation and model performance. Smaller chunks may lose context, while larger chunks may degrade model performance. The DocETL optimizer can find the chunk size that works best for your task, if you choose to use the optimizer.</p> </li> <li> <p>Consider Overlap: In some cases, you might want to implement overlap between chunks to maintain context. This isn't built into the Split operation, but you can achieve it by post-processing the split chunks.</p> </li> <li> <p>Use Appropriate Delimiters: When using the delimiter method, choose a delimiter that logically divides your text. Common choices include double newlines for paragraphs, or custom markers for document sections. When using the delimiter method, adjust the <code>num_splits_to_group</code> parameter to create chunks that contain an appropriate amount of context for your task.</p> </li> <li> <p>Mind the Order: If the order of chunks matters for your analysis, always set <code>associative: false</code> in your subsequent reduce operations.</p> </li> <li> <p>Optimize for Performance: For very large documents, consider using a combination of delimiter and token count methods. First split into large sections using delimiters, then apply token count splitting to ensure no chunk exceeds model limits.</p> </li> </ol> <p>By leveraging the Split operation effectively, you can process large documents efficiently and extract meaningful insights using subsequent map and reduce operations.</p>"},{"location":"operators/unnest/","title":"Unnest Operation","text":"<p>The Unnest operation in DocETL is designed to expand an array field or a dictionary in the input data into multiple items. This operation is particularly useful when you need to process or analyze individual elements of an array or specific fields of a nested dictionary separately.</p> <p>How Unnest Works</p> <p>The Unnest operation behaves differently depending on the type of data being unnested:</p> <ul> <li>For list-type unnesting: It replaces the original key with each individual element from the list.</li> <li>For dictionary-type unnesting: It adds new keys to the parent dictionary based on the <code>expand_fields</code> parameter.</li> </ul> <p>Unnest does not have an output schema. It modifies the structure of your data in place.</p>"},{"location":"operators/unnest/#motivation","title":"Motivation","text":"<p>The Unnest operation is valuable in scenarios where you need to:</p> <ul> <li>Process individual items from a list of products in an order</li> <li>Analyze separate entries in a list of comments or reviews</li> <li>Expand nested data structures for more granular processing</li> <li>Flatten complex data structures for easier analysis</li> </ul>"},{"location":"operators/unnest/#configuration","title":"Configuration","text":""},{"location":"operators/unnest/#required-parameters","title":"Required Parameters","text":"Parameter Description type Must be set to \"unnest\" name A unique name for the operation unnest_key The key of the array field to unnest"},{"location":"operators/unnest/#optional-parameters","title":"Optional Parameters","text":"Parameter Description Default keep_empty If true, empty arrays being exploded will be kept in the output (with value None) false expand_fields A list of fields to expand from the nested dictionary into the parent dictionary, if unnesting a dict [] recursive If true, the unnest operation will be applied recursively to nested arrays false depth The maximum depth for recursive unnesting (only applicable if recursive is true) inf sample Number of samples to use for the operation None"},{"location":"operators/unnest/#output","title":"Output","text":"<p>The Unnest operation modifies the structure of your data:</p> <ul> <li>For list-type unnesting: It generates multiple output items for each input item, replacing the original array in the <code>unnest_key</code> field with individual elements.</li> <li>For dictionary-type unnesting: It expands the specified fields into the parent dictionary.</li> </ul> <p>All other original key-value pairs from the input item are preserved in the output.</p> <p>Note</p> <p>When unnesting dictionaries, the original nested dictionary is preserved in the output, and the specified fields are expanded into the parent dictionary.</p>"},{"location":"operators/unnest/#use-cases","title":"Use Cases","text":"<ol> <li>Product Analysis in Orders: Unnest a list of products in each order, then use a map operation to analyze each product individually.</li> <li>Comment Sentiment Analysis: Unnest a list of comments for each post, enabling sentiment analysis on individual comments.</li> <li>Nested Data Structure Flattening: Unnest complex nested data structures to create a flattened dataset for easier analysis or processing.</li> <li>Processing Time Series Data: Unnest time series data stored in arrays to analyze individual time points.</li> </ol>"},{"location":"operators/unnest/#example-analyzing-product-reviews","title":"Example: Analyzing Product Reviews","text":"<p>Let's walk through an example of using the Unnest operation to prepare product reviews for detailed analysis.</p> <pre><code>- name: extract_salient_quotes\n  type: map\n  prompt: |\n    For the following product review, extract up to 3 salient quotes that best represent the reviewer's opinion:\n\n    {{ input.review_text }}\n\n    For each quote, provide the text and its sentiment (positive, negative, or neutral).\n  output:\n    schema:\n      salient_quotes: list[string]\n\n- name: unnest_quotes\n  type: unnest\n  unnest_key: salient_quotes\n\n- name: analyze_quote\n  type: map\n  prompt: |\n    Analyze the following quote from a product review:\n\n    Quote &amp; information: {{ input.salient_quotes }}\n    Review text: {{ input.review_text }}\n\n    Provide a detailed analysis of the quote, including:\n    1. The specific aspect of the product being discussed\n    2. The strength of the sentiment (-5 to 5, where -5 is extremely negative and 5 is extremely positive)\n    3. Any key terms or phrases that stand out\n\n  output:\n    schema:\n      product_aspect: string\n      sentiment_strength: number\n      key_terms: list[string]\n</code></pre> <p>This example demonstrates how the Unnest operation fits into a pipeline for analyzing product reviews:</p> <ol> <li>The first Map operation extracts salient quotes from each review.</li> <li>The Unnest operation expands the 'salient_quotes' array, creating individual items for each quote. Each quote can now be accessed via <code>input.salient_quotes</code>.</li> <li>The second Map operation performs a detailed analysis on each individual quote.</li> </ol> <p>By unnesting the quotes, we enable more granular analysis that wouldn't be possible if we processed the entire review as a single unit.</p>"},{"location":"operators/unnest/#advanced-features","title":"Advanced Features","text":""},{"location":"operators/unnest/#recursive-unnesting","title":"Recursive Unnesting","text":"<p>When dealing with deeply nested structures, you can use the <code>recursive</code> parameter to apply the unnest operation at multiple levels:</p> <pre><code>- name: recursive_unnest\n  type: unnest\n  unnest_key: nested_data\n  recursive: true\n  depth: 3 # Limit recursion to 3 levels deep\n</code></pre>"},{"location":"operators/unnest/#dictionary-expansion","title":"Dictionary Expansion","text":"<p>When unnesting dictionaries, you can use the <code>expand_fields</code> parameter to flatten specific fields into the parent structure:</p> <pre><code>- name: expand_user_data\n  type: unnest\n  unnest_key: user_info\n  expand_fields:\n    - name\n    - age\n    - location\n</code></pre> <p>In this case, <code>name</code>, <code>age</code>, and <code>location</code> would be added as new keys in the parent dictionary, alongside the original <code>user_info</code> key.</p>"},{"location":"operators/unnest/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Choose the Right Unnest Key: Ensure you're unnesting the correct field that contains the array or nested structure you want to expand.</p> </li> <li> <p>Consider Data Volume: Unnesting can significantly increase the number of items in your data stream. Be mindful of this when designing subsequent operations in your pipeline.</p> </li> <li> <p>Use Expand Fields Wisely: When unnesting dictionaries, use the <code>expand_fields</code> parameter to flatten your data structure if needed, but be cautious of potential key conflicts.</p> </li> <li> <p>Handle Empty Arrays: Decide whether empty arrays should be kept (using <code>keep_empty</code>) based on your specific use case and how subsequent operations should handle null values.</p> </li> <li> <p>Preserve Context: When unnesting, consider whether you need to carry forward any context from the parent item. The unnest operation preserves all other fields, which helps maintain context.</p> </li> </ol>"},{"location":"optimization/configuration/","title":"Advanced: Customizing Optimization","text":"<p>You can customize the optimization process for specific operations using the ``optimizer_config in your pipeline.</p>"},{"location":"optimization/configuration/#global-configuration","title":"Global Configuration","text":"<p>The following options can be applied globally to all operations in your pipeline during optimization:</p> <ul> <li> <p><code>num_retries</code>: The number of times to retry optimizing if the LLM agent fails. Default is 1.</p> </li> <li> <p><code>sample_sizes</code>: Override the default sample sizes for each operator type. Specify as a dictionary with operator types as keys and integer sample sizes as values.</p> </li> </ul> <p>Default sample sizes:</p> <pre><code>SAMPLE_SIZE_MAP = {\n    \"reduce\": 40,\n    \"map\": 5,\n    \"resolve\": 100,\n    \"equijoin\": 100,\n    \"filter\": 5,\n}\n</code></pre>"},{"location":"optimization/configuration/#equijoin-configuration","title":"Equijoin Configuration","text":"<ul> <li><code>target_recall</code>: Change the default target recall (default is 0.95).</li> </ul>"},{"location":"optimization/configuration/#resolve-configuration","title":"Resolve Configuration","text":"<ul> <li><code>target_recall</code>: Specify the target recall for the resolve operation.</li> </ul>"},{"location":"optimization/configuration/#reduce-configuration","title":"Reduce Configuration","text":"<ul> <li><code>synthesize_resolve</code>: Set to <code>False</code> if you definitely don't want a resolve operation synthesized or want to turn off this rewrite rule.</li> </ul>"},{"location":"optimization/configuration/#map-configuration","title":"Map Configuration","text":"<ul> <li><code>force_chunking_plan</code>: Set to <code>True</code> if you want the the optimizer to force plan that breaks up the input documents into chunks.</li> </ul>"},{"location":"optimization/configuration/#example-configuration","title":"Example Configuration","text":"<p>Here's an example of how to use the <code>optimizer_config</code> in your pipeline:</p> <pre><code>optimizer_config:\n  num_retries: 2\n  sample_sizes:\n    map: 10\n    reduce: 50\n  reduce:\n    synthesize_resolve: false\n  map:\n    force_chunking_plan: true\n\noperations:\n  - name: extract_medications\n    type: map\n    optimize: true\n    # ... other configuration ...\n\n  - name: summarize_prescriptions\n    type: reduce\n    optimize: true\n    # ... other configuration ...\n# ... rest of the pipeline configuration ...\n</code></pre> <p>This configuration will:</p> <ol> <li>Retry optimization up to 2 times for each operation if the LLM agent fails.</li> <li>Use custom sample sizes for map (10) and reduce (50) operations.</li> <li>Prevent the synthesis of resolve operations for reduce operations.</li> <li>Force a chunking plan for map operations.</li> </ol>"},{"location":"optimization/example/","title":"Running the Optimizer","text":"<p>Optimizer Stability</p> <p>The optimization process can be unstable, as well as resource-intensive (we've seen it take up to 10 minutes to optimize a single operation, spending up to ~$50 in API costs for end-to-end pipelines). We recommend optimizing one operation at a time and retrying if necessary, as results may vary between runs. This approach also allows you to confidently verify that each optimized operation is performing as expected before moving on to the next.</p> <p>See the API for more details on how to resume the optimizer from a failed run, by rerunning <code>docetl build pipeline.yaml --resume</code> (with the <code>--resume</code> flag).</p> <p>Also, you can use gpt-4o-mini for cheaper optimizations (rather than the default gpt-4o), which you can do via <code>docetl build pipeline.yaml --model=gpt-4o-mini</code>.</p> <p>To optimize your pipeline, start with your initial configuration and follow these steps:</p> <ol> <li> <p>Set <code>optimize: True</code> for the operation you want to optimize (start with the first operation, if you're not sure which one).</p> </li> <li> <p>Run the optimizer using the command <code>docetl build pipeline.yaml</code>. This will generate an optimized version in <code>pipeline_opt.yaml</code>.</p> </li> <li> <p>Review the optimized operation in <code>pipeline_opt.yaml</code>. If you're satisfied with the changes, copy the optimized operation back into your original <code>pipeline.yaml</code>.</p> </li> <li> <p>Move on to the next LLM-powered operation and repeat steps 1-3.</p> </li> <li> <p>Once all operations are optimized, your <code>pipeline.yaml</code> will contain the fully optimized pipeline.</p> </li> </ol> <p>When optimizing a resolve operation, the optimizer will also set blocking configurations and thresholds, saving you from manual configuration.</p> <p>Feeling Ambitious?</p> <p>You can run the optimizer on your entire pipeline by setting <code>optimize: True</code> for each operation you want to optimize. But sometimes the agent fails to find a better plan, and you'll need to manually intervene. We are exploring human-in-the-loop optimization, where the optimizer can ask for human feedback to improve its plans.</p>"},{"location":"optimization/example/#example-optimizing-a-medical-transcripts-pipeline","title":"Example: Optimizing a Medical Transcripts Pipeline","text":"<p>Let's walk through optimizing a pipeline for extracting medication information from medical transcripts. We'll start with an initial pipeline and optimize it step by step.</p>"},{"location":"optimization/example/#initial-pipeline","title":"Initial Pipeline","text":"<pre><code>datasets:\n  transcripts:\n    path: medical_transcripts.json\n    type: file\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Analyze the transcript: {{ input.src }}\n      List all medications mentioned.\n\n  - name: unnest_medications\n    type: unnest\n    unnest_key: medication\n\n  - name: summarize_prescriptions\n    type: reduce\n    reduce_key:\n      - medication\n    output:\n      schema:\n        side_effects: str\n        uses: str\n    prompt: |\n      Summarize side effects and uses of {{ reduce_key }} from:\n      {% for value in inputs %}\n      Transcript {{ loop.index }}: {{ value.src }}\n      {% endfor %}\n\npipeline:\n  output:\n    path: medication_summaries.json\n    type: file\n  steps:\n    - input: transcripts\n      name: medical_info_extraction\n      operations:\n        - extract_medications\n        - unnest_medications\n        - summarize_prescriptions\n</code></pre>"},{"location":"optimization/example/#optimization-steps","title":"Optimization Steps","text":"<p>First, we'll optimize the <code>extract_medications</code> operation. Set <code>optimize: True</code> for this operation and run the optimizer. Review the changes and integrate them into your pipeline.</p> <p>Then, optimize the <code>summarize_prescriptions</code> operation by setting <code>optimize: True</code> and running <code>docetl build pipeline.yaml</code> again. The optimizer may suggest adding a resolve operation at this point, and will automatically configure blocking and thresholds. After completing all steps, your optimized pipeline might look like this:</p>"},{"location":"optimization/example/#optimized-pipeline","title":"Optimized Pipeline","text":"<pre><code>datasets:\n  transcripts:\n    path: medical_transcripts.json\n    type: file\n\ndefault_model: gpt-4o-mini\n\noperations:\n  - name: extract_medications\n    type: map\n    output:\n      schema:\n        medication: list[str]\n    prompt: |\n      Analyze the transcript: {{ input.src }}\n      List all medications mentioned.\n    gleaning:\n      num_rounds: 1\n      validation_prompt: |\n        Evaluate the extraction for completeness and accuracy:\n        1. Are all medications, dosages, and symptoms from the transcript included?\n        2. Is the extracted information correct and relevant?\n\n  - name: unnest_medications\n    type: unnest\n    unnest_key: medication\n\n  - name: resolve_medications\n    type: resolve\n    blocking_keys:\n      - medication\n    blocking_threshold: 0.7\n    comparison_prompt: |\n      Compare medications:\n      1: {{ input1.medication }}\n      2: {{ input2.medication }}\n      Are these the same or closely related?\n    resolution_prompt: |\n      Standardize the name for:\n      {% for entry in inputs %}\n      - {{ entry.medication }}\n      {% endfor %}\n\n  - name: summarize_prescriptions\n    type: reduce\n    reduce_key:\n      - medication\n    output:\n      schema:\n        side_effects: str\n        uses: str\n    prompt: |\n      Summarize side effects and uses of {{ reduce_key }} from:\n      {% for value in inputs %}\n      Transcript {{ loop.index }}: {{ value.src }}\n      {% endfor %}\n    fold_batch_size: 10\n    fold_prompt: |\n      Update the existing summary of side effects and uses for {{ reduce_key }} based on the following additional transcripts:\n      {% for value in inputs %}\n      Transcript {{ loop.index }}: {{ value.src }}\n      {% endfor %}\n\n      Existing summary:\n      Side effects: {{ output.side_effects }}\n      Uses: {{ output.uses }}\n\n      Provide an updated and comprehensive summary, incorporating both the existing information and any new insights from the additional transcripts.\n\npipeline:\n  output:\n    path: medication_summaries.json\n    type: file\n  steps:\n    - input: transcripts\n      name: medical_info_extraction\n      operations:\n        - extract_medications\n        - unnest_medications\n        - resolve_medications\n        - summarize_prescriptions\n</code></pre> <p>This optimized pipeline now includes improved prompts, a resolve operation, and additional output fields for more comprehensive medication information extraction.</p> <p>Feedback Welcome</p> <p>We're continually improving the optimizer. Your feedback on its performance and usability is invaluable. Please share your experiences and suggestions!</p>"},{"location":"optimization/example/#optimizer-api","title":"Optimizer API","text":"<p>Build and optimize the configuration specified in the YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>Path</code> <p>Path to the YAML file containing the pipeline configuration.</p> <code>Argument(..., help='Path to the YAML file containing the pipeline configuration')</code> <code>max_threads</code> <code>Optional[int]</code> <p>Maximum number of threads to use for running operations.</p> <code>Option(None, help='Maximum number of threads to use for running operations')</code> <code>model</code> <code>str</code> <p>Model to use for optimization. Defaults to \"gpt-4o\".</p> <code>Option('gpt-4o', help='Model to use for optimization')</code> <code>resume</code> <code>bool</code> <p>Whether to resume optimization from a previous run. Defaults to False.</p> <code>Option(False, help='Resume optimization from a previous build that may have failed')</code> <code>timeout</code> <code>int</code> <p>Timeout for optimization operations in seconds. Defaults to 60.</p> <code>Option(60, help='Timeout for optimization operations in seconds')</code> Source code in <code>docetl/cli.py</code> <pre><code>@app.command()\ndef build(\n    yaml_file: Path = typer.Argument(\n        ..., help=\"Path to the YAML file containing the pipeline configuration\"\n    ),\n    max_threads: Optional[int] = typer.Option(\n        None, help=\"Maximum number of threads to use for running operations\"\n    ),\n    model: str = typer.Option(\"gpt-4o\", help=\"Model to use for optimization\"),\n    resume: bool = typer.Option(\n        False, help=\"Resume optimization from a previous build that may have failed\"\n    ),\n    timeout: int = typer.Option(\n        60, help=\"Timeout for optimization operations in seconds\"\n    ),\n):\n    \"\"\"\n    Build and optimize the configuration specified in the YAML file.\n\n    Args:\n        yaml_file (Path): Path to the YAML file containing the pipeline configuration.\n        max_threads (Optional[int]): Maximum number of threads to use for running operations.\n        model (str): Model to use for optimization. Defaults to \"gpt-4o\".\n        resume (bool): Whether to resume optimization from a previous run. Defaults to False.\n        timeout (int): Timeout for optimization operations in seconds. Defaults to 60.\n    \"\"\"\n    # Get the current working directory (where the user called the command)\n    cwd = os.getcwd()\n\n    # Load .env file from the current working directory\n    env_file = os.path.join(cwd, \".env\")\n    if os.path.exists(env_file):\n        load_dotenv(env_file)\n\n    optimizer = Optimizer.from_yaml(\n        str(yaml_file),\n        max_threads=max_threads,\n        model=model,\n        timeout=timeout,\n        resume=resume,\n    )\n    optimizer.optimize()\n    optimizer.save_optimized_config()\n</code></pre> <p>handler: python options: members: - build show_root_full_path: true show_root_toc_entry: true show_root_heading: true show_source: false show_name: true</p>"},{"location":"optimization/overview/","title":"DocETL Optimizer","text":"<p>The DocETL optimizer finds a plan that improves the accuracy of your document processing pipelines. It works by analyzing and potentially rewriting operations marked for optimization, finding optimal plans for execution.</p>"},{"location":"optimization/overview/#key-features","title":"Key Features","text":"<ul> <li>Automatically decomposes complex operations into more efficient sub-pipelines</li> <li>Inserts resolve operations before reduce operations when beneficial</li> <li>Optimizes for large documents that exceed context limits</li> <li>Improves accuracy in high-volume reduce operations with incremental reduce</li> </ul>"},{"location":"optimization/overview/#how-it-works","title":"How It Works","text":"<p>The optimizer employs AI agents to generate and validate potential optimizations:</p> <ol> <li>Generation Agents: Create alternative plans for operations, potentially breaking them down into multiple steps.</li> <li>Validation Agents: Evaluate and compare the outputs of different plans to determine the most effective approach.</li> </ol> <pre><code>graph TB\n    A[User-Defined Operation] --&gt; B[Validation Agent]\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    B --&gt;|Synthesize| C[Validator Prompt]\n    C --&gt; D[Evaluate on Sample Data]\n    D --&gt; E{Needs Optimization?}\n    E --&gt;|Yes| F[Generation Agent]\n    E --&gt;|No| J[Optimized Operation]\n    style F fill:#bbf,stroke:#333,stroke-width:2px\n    F --&gt;|Create| G[Candidate Plans]\n    G --&gt; H[Validation Agent]\n    style H fill:#f9f,stroke:#333,stroke-width:2px\n    H --&gt;|Rank/Compare| I[Select Best Plan]\n    I --&gt; J</code></pre>"},{"location":"optimization/overview/#should-i-use-the-optimizer","title":"Should I Use the Optimizer?","text":"<p>While any pipeline can potentially benefit from optimization, there are specific scenarios where using the optimizer can significantly improve your pipeline's performance and accuracy. When should you use the optimizer?</p> <p>Large Documents</p> <p>If you have documents that approach or exceed context limits and a map operation that transforms these documents using an LLM, the optimizer can help:</p> <ul> <li>Improve accuracy</li> <li>Enable processing of entire documents</li> <li>Optimize for large-scale data handling</li> </ul> <p>Entity Resolution</p> <p>The optimizer is particularly useful when:</p> <pre><code>- You need a resolve operation before your reduce operation\n- You've defined a resolve operation but want to optimize it for speed using blocking\n</code></pre> <p>High-Volume Reduce Operations</p> <p>Consider using the optimizer when:</p> <pre><code>- You have many documents feeding into a reduce operation for a given key\n- You're concerned about the accuracy of the reduce operation due to high volume\n- You want to optimize for better accuracy in complex reductions\n</code></pre> <p>Even if your pipeline doesn't fall into these specific categories, optimization can still be beneficial. For example, the optimizer can enhance your operations by adding gleaning to an operation, which uses an LLM-powered validator to ensure operation correctness. Learn more about gleaning.</p>"},{"location":"optimization/overview/#example-optimizing-legal-contract-analysis","title":"Example: Optimizing Legal Contract Analysis","text":"<p>Let's consider a pipeline for analyzing legal contracts, extracting clauses, and summarizing them by type. Initially, you might have a single map operation to extract and tag clauses, followed by a reduce operation to summarize them. However, this approach might not be accurate enough for long contracts.</p>"},{"location":"optimization/overview/#initial-pipeline","title":"Initial Pipeline","text":"<p>In the initial pipeline, you might have a single map operation that attempts to extract all clauses and tag them with their types in one go. This is followed by a reduce operation that summarizes the clauses by type. Maybe the reduce operation accurately summarizes the clauses in a single LLM call per clause type, but the map operation might not be able to accurately extract and tag the clauses in a single LLM call.</p>"},{"location":"optimization/overview/#optimized-pipeline","title":"Optimized Pipeline","text":"<p>After applying the optimizer, your pipeline could be transformed into a more efficient and accurate sub-pipeline:</p> <ol> <li>Split Operation: Breaks down each long contract into manageable chunks.</li> <li>Map Operation: Processes each chunk to extract and tag clauses.</li> <li>Reduce Operation: For each contract, combine the extracted and tagged clauses from each chunk.</li> </ol> <p>The goal of the DocETL optimizer is to try many ways of rewriting your pipeline and then select the best one. This may take some time (20-30 minutes for very complex tasks and large documents). But the optimizer's ability to break down complex tasks into more manageable sub-steps can lead to more accurate and reliable results.</p>"},{"location":"optimization/python-api/","title":"Optimizing Pipelines with the Python API","text":"<p>You may have your pipelines defined in Python instead of YAML and want to optimize them. Here's an example of how to use the Python API to define, optimize, and run a document processing pipeline similar to the medical transcripts example we saw earlier.</p> <pre><code>from docetl.api import Pipeline, Dataset, MapOp, UnnestOp, ResolveOp, ReduceOp, PipelineStep, PipelineOutput\n\n# Define datasets\ndatasets = {\n    \"transcripts\": Dataset(type=\"file\", path=\"medical_transcripts.json\"),\n}\n\n# Define operations\noperations = [\n    MapOp(\n        name=\"extract_medications\",\n        type=\"map\",\n        optimize=True,  # This operation will be optimized\n        output={\"schema\": {\"medication\": \"list[str]\"}},\n        prompt=\"Analyze the transcript: {{ input.src }}\\nList all medications mentioned.\",\n    ),\n    UnnestOp(\n        name=\"unnest_medications\",\n        type=\"unnest\",\n        unnest_key=\"medication\"\n    ),\n    ResolveOp(\n        name=\"resolve_medications\",\n        type=\"resolve\",\n        blocking_keys=[\"medication\"],\n        optimize=True,  # This operation will be optimized\n        comparison_prompt=\"Compare medications:\\n1: {{ input1.medication }}\\n2: {{ input2.medication }}\\nAre these the same or closely related?\",\n        resolution_prompt=\"Standardize the name for:\\n{% for entry in inputs %}\\n- {{ entry.medication }}\\n{% endfor %}\"\n    ),\n    ReduceOp(\n        name=\"summarize_prescriptions\",\n        type=\"reduce\",\n        reduce_key=[\"medication\"],\n        output={\"schema\": {\"side_effects\": \"str\", \"uses\": \"str\"}},\n        prompt=\"Summarize side effects and uses of {{ reduce_key }} from:\\n{% for value in inputs %}\\nTranscript {{ loop.index }}: {{ value.src }}\\n{% endfor %}\",\n        optimize=True,  # This operation will be optimized\n    )\n]\n\n# Define pipeline steps\nsteps = [\n    PipelineStep(name=\"medical_info_extraction\", input=\"transcripts\", operations=[\"extract_medications\", \"unnest_medications\", \"resolve_medications\", \"summarize_prescriptions\"])\n]\n\n# Define pipeline output\noutput = PipelineOutput(type=\"file\", path=\"medication_summaries.json\")\n\n# Create the pipeline\npipeline = Pipeline(\n    name=\"medical_transcripts_pipeline\",\n    datasets=datasets,\n    operations=operations,\n    steps=steps,\n    output=output,\n    default_model=\"gpt-4o-mini\"\n)\n\n# Optimize the pipeline\noptimized_pipeline = pipeline.optimize(model=\"gpt-4o-mini\")\n\n# Run the optimized pipeline\nresult = optimized_pipeline.run()\n\nprint(f\"Pipeline execution completed. Total cost: ${result:.2f}\")\n</code></pre> <p>This example demonstrates how to create a pipeline that processes medical transcripts, extracts medication information, resolves similar medications, and summarizes prescription details.</p> <p>Optimization</p> <p>Notice that some operations have <code>optimize=True</code> set. DocETL will only optimize operations with this flag set to <code>True</code>. In this example, the <code>extract_medications</code>, <code>resolve_medications</code>, and <code>summarize_prescriptions</code> operations will be optimized.</p> <p>Optimization Model</p> <p>We use <code>pipeline.optimize(model=\"gpt-4o-mini\")</code> to optimize the pipeline using the GPT-4o-mini model for the agents. This allows you to specify which model to use for optimization, which can be particularly useful when you want to balance between performance and cost.</p> <p>The pipeline is optimized before execution to improve performance and accuracy. By setting <code>optimize=True</code> for specific operations, you have fine-grained control over which parts of your pipeline undergo optimization.</p>"}]}